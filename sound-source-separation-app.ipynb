{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa, librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "n_fft = 1024\n",
    "hop_length = 8\n",
    "sr = 16000\n",
    "\n",
    "# cate bucati PF iau in considerare pentru AF -> cu overlapping => 15 frame-uri \n",
    "# ultimul frame din AF fiind PF-ul curent\n",
    "nb_of_frames_in_AF = 4\n",
    "# exp_w_avg_beta = 0.98\n",
    "marja_eroare = 1e-2\n",
    "\n",
    "# frame_length_ms = 5\n",
    "# total_length_ms = 5000.0\n",
    "# nb_of_samples = 80000\n",
    "samples_per_frame_10ms = 160 #(int)(160000 * 10 / 10000.0)\n",
    "samples_per_frame_5ms = 80 #(int)(80000 * 5 / 5000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliz(mix):\n",
    "    mu = np.mean(mix, axis=0)\n",
    "    var = np.var(mix, axis=0)\n",
    "\n",
    "    mix_norm = (mix - mu) / np.sqrt(var + 1e-8)\n",
    "    \n",
    "    return mix_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(stft_1, stft_2):\n",
    "    # small epsilon to avoid dividing by zero\n",
    "    eps = np.finfo(np.float).eps\n",
    "\n",
    "    # compute model as the sum of spectrograms\n",
    "    mix = eps + np.abs(stft_1) + np.abs(stft_2)    \n",
    "    mask = np.divide(np.abs(stft_1), mix)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STFT_AF_frames(AF_array, PF_size):\n",
    "    frames = []\n",
    "    half = (int)(PF_size/2)\n",
    "    frames_added = 0\n",
    "    \n",
    "    i = AF_array.shape[0]-1\n",
    "    \n",
    "    # use AF as 4 times bigger than PF's size -> PF = 10 ms => AF = 40 ms\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 1. add PF[i]\n",
    "    frame = librosa.stft(librosa.to_mono(AF_array[i]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "    frame = normaliz(frame)\n",
    "    \n",
    "    tensor = []\n",
    "    tensor.append(frame)        \n",
    "    frames.append(tensor)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 2. add PF[i-1, i] -> overlap\n",
    "    overlap_frame = AF_array[i-1][half:]\n",
    "    overlap_frame = np.concatenate((overlap_frame, AF_array[i][:half]))\n",
    "    \n",
    "    frame = librosa.stft(librosa.to_mono(overlap_frame), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "    frame = normaliz(frame)\n",
    "\n",
    "    tensor = []\n",
    "    tensor.append(frame)\n",
    "    frames.append(tensor)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 3. add PF[i - 1]\n",
    "    frame = librosa.stft(librosa.to_mono(AF_array[i - 1]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "    frame = normaliz(frame)\n",
    "    \n",
    "    tensor = []\n",
    "    tensor.append(frame)        \n",
    "    frames.append(tensor)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 4. add PF[i-1, i] -> overlap\n",
    "    overlap_frame = AF_array[i-1][half:]\n",
    "    overlap_frame = np.concatenate((overlap_frame, AF_array[i][:half]))\n",
    "    \n",
    "    frame = librosa.stft(librosa.to_mono(overlap_frame), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "    frame = normaliz(frame)\n",
    "\n",
    "    tensor = []\n",
    "    tensor.append(frame)\n",
    "    frames.append(tensor)\n",
    "        \n",
    "    \n",
    "    return np.asarray(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_set_for_mix(mix, voice_1, voice_2, samples_per_frame):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    mix_frames = np.array([mix[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    voice_1_frames = np.array([voice_1[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    voice_2_frames = np.array([voice_2[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    \n",
    "    for index in range(mix_frames.shape[0]):\n",
    "        \n",
    "        if index < 2:\n",
    "            continue\n",
    "        \n",
    "        # 1. create train set input for AF with current PF and previous frames\n",
    "        AF_frames = np.array([mix_frames[i] for i in range(index - 2, index)])      \n",
    "        AF_STFT_frames = get_STFT_AF_frames(AF_array=AF_frames, PF_size= samples_per_frame)\n",
    "        inputs.append(AF_STFT_frames)\n",
    "        \n",
    "        # 2. create train set target for that AF, containing the mask for the current PF\n",
    "        stft_voice_1 = librosa.stft(librosa.to_mono(voice_1_frames[index]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        stft_voice_2 = librosa.stft(librosa.to_mono(voice_2_frames[index]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        mask = compute_mask(stft_voice_1, stft_voice_2)\n",
    "        \n",
    "        targets.append(mask)\n",
    "    \n",
    "    # train set for only one audio\n",
    "    train_set_input = torch.from_numpy(np.array(inputs))\n",
    "    \n",
    "    # target contains the calculated masks for each PF from mix\n",
    "    train_set_target = torch.from_numpy(np.array(targets))\n",
    "    return np.array(inputs), np.array(targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    height = 0\n",
    "    width = 0\n",
    "    \n",
    "    # default values for height and width are given for a processing frame of 10 ms\n",
    "    # 5 ms  = [513, 11]\n",
    "    def __init__(self, height = 513, width = 21):\n",
    "        super(Network, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # layer 1\n",
    "        self.fc1 = nn.Linear(1 * height * width, 300)\n",
    "        self.fc_batch = nn.BatchNorm1d(300)\n",
    "        \n",
    "        # layer 2\n",
    "        self.fc2 = nn.Linear(300, 300)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # layer 3\n",
    "        self.fc3 = nn.Linear(300, height * width)\n",
    "        self.fc3_batch = nn.BatchNorm1d(height * width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # layer 1\n",
    "        # firstly, transform the matrix into an array for the FC\n",
    "        x = x.view(-1, self.height * self.width)        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        # layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "#          # layer 2\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc_batch(x)\n",
    "#         x = torch.relu(x)\n",
    "# #         x = self.drop(x)\n",
    "        \n",
    "#                  # layer 2\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc_batch(x)\n",
    "#         x = torch.relu(x)\n",
    "# #         x = self.drop(x)\n",
    "        \n",
    "        # layer 3\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc3_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = x.view(nb_of_frames_in_AF, self.height, self.width) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkConv(nn.Module):\n",
    "    height = 0\n",
    "    width = 0\n",
    "    \n",
    "    # default values for height and width are given for a processing frame of 10 ms\n",
    "    # 5 ms  = [513, 11]\n",
    "    def __init__(self, height = 513, width = 21):\n",
    "        super(NetworkConv, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, padding=2)\n",
    "        self.conv1_batch = nn.BatchNorm2d(20)\n",
    "        \n",
    "        # layer 2\n",
    "        self.conv2 = nn.Conv2d(20, 20, kernel_size=5, padding=2)\n",
    "        self.conv2_batch = nn.BatchNorm2d(20)\n",
    "    \n",
    "        # layer 3\n",
    "        self.fc1 = nn.Linear(20 * height * width, 250)\n",
    "        self.fc1_batch = nn.BatchNorm1d(250)\n",
    "        \n",
    "        # layer 4\n",
    "        self.fc2 = nn.Linear(250, height * width)\n",
    "        self.fc2_batch = nn.BatchNorm1d(height * width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # firstly, transform the matrix into an array for the FC\n",
    "        \n",
    "        \n",
    "        # layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv1_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 3\n",
    "        x = x.view(-1, self.height * self.width * 20) \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 4\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = x.view(nb_of_frames_in_AF, self.height, self.width) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mix_of_10_seconds(voice):\n",
    "    nb_sample_for_10_seconds = 160000\n",
    "    \n",
    "    if len(voice) < nb_sample_for_10_seconds:\n",
    "        voice = np.pad(voice, (0,(nb_sample_for_10_seconds - len(voice) )), 'constant', constant_values=(0))\n",
    "    elif len(voice) > nb_sample_for_10_seconds:\n",
    "        voice = voice[0:nb_sample_for_10_seconds]\n",
    "    \n",
    "    return voice        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mix_of_5_seconds(voice):\n",
    "    nb_sample_for_5_seconds = 80000\n",
    "    \n",
    "    if len(voice) < nb_sample_for_5_seconds:\n",
    "        voice = np.pad(voice, (0,(nb_sample_for_5_seconds - len(voice) )), 'constant', constant_values=(0))\n",
    "    elif len(voice) > nb_sample_for_5_seconds:\n",
    "        voice = voice[0:nb_sample_for_5_seconds]\n",
    "    \n",
    "    return voice   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_voice_interchanged(voice, is_valid):\n",
    "    voices = []\n",
    "    pos_half = (int)(len(voice) / 2)\n",
    "    pos_1quart = (int)(pos_half / 2)\n",
    "    pos_3quart = (int)(pos_half + (pos_half / 2))\n",
    "    pos_1third = 26000\n",
    "    \n",
    "    if is_valid == True:\n",
    "        voice_perm = voice[pos_1third * 2 :len(voice)]\n",
    "        voice_perm = np.append(voice_perm, voice[0:pos_1third])\n",
    "        voice_perm = np.append(voice_perm, voice[pos_1third:pos_1third * 2])\n",
    "        voices.append(voice_perm)\n",
    "    else:\n",
    "        voices.append(voice)\n",
    "\n",
    "        voice_perm = voice[pos_3quart:len(voice)]\n",
    "        voice_perm = np.append(voice_perm, voice[0:pos_1quart])\n",
    "        voice_perm = np.append(voice_perm, voice[pos_1quart:pos_half])\n",
    "        voice_perm = np.append(voice_perm, voice[pos_half:pos_3quart])\n",
    "        voices.append(voice_perm)\n",
    "        \n",
    "        voice_perm = voice[pos_half:pos_3quart]\n",
    "        voice_perm = np.append(voice_perm, voice[pos_3quart:len(voice)])\n",
    "        voice_perm = np.append(voice_perm, voice[pos_1quart:pos_half])\n",
    "        voice_perm = np.append(voice_perm, voice[0:pos_1quart])\n",
    "        voices.append(voice_perm)\n",
    "        \n",
    "        voice_perm = voice[pos_1quart:pos_half]\n",
    "        voice_perm = np.append(voice_perm, voice[pos_half:pos_3quart])\n",
    "        voice_perm = np.append(voice_perm, voice[pos_3quart:len(voice)])\n",
    "        voice_perm = np.append(voice_perm, voice[0:pos_1quart])\n",
    "        voices.append(voice_perm)\n",
    "        \n",
    "        voice_perm = voice[pos_3quart:len(voice)]\n",
    "        voice_perm = np.append(voice_perm, voice[0:pos_1quart])\n",
    "        voice_perm = np.append(voice_perm, voice[pos_half:pos_3quart])\n",
    "        voice_perm = np.append(voice_perm, voice[pos_1quart:pos_half])\n",
    "        voices.append(voice_perm)\n",
    "\n",
    "\n",
    "        voice_perm = voice[pos_1quart:pos_half]\n",
    "        voice_perm = np.append(voice_perm, voice[pos_half:len(voice)])\n",
    "        voice_perm = np.append(voice_perm, voice[0:pos_1quart])\n",
    "        voices.append(voice_perm)\n",
    "    \n",
    "    return np.array(voices)\n",
    "    \n",
    "    # split voice into smaller batches/chunks\n",
    "    # np.array_split takes as args the data input and the number of batches!!! not the number of elems in a batch\n",
    "#     if is_valid == False:\n",
    "#         voice_batches = np.array_split(np.array(voice),5)\n",
    "#     else:\n",
    "#         voice_batches = np.array_split(np.array(voice),6)\n",
    "        \n",
    "#     np.random.shuffle(voice_batches)\n",
    "\n",
    "#     voice_perm = []\n",
    "#     for v in voice_batches:\n",
    "#         voice_perm.extend(v)\n",
    "    \n",
    "#     return np.array(voice_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_from_voices(voice1_file_name, voice2_file_name, \n",
    "                                 samples_per_frame, mix_5_seconds):\n",
    "    inputs_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "            \n",
    "    voice1, sr = librosa.load(voice1_file_name, sr=16000) \n",
    "    voice1 = np.append(voice1, voice1)\n",
    "    \n",
    "    voice2, sr = librosa.load(voice2_file_name, sr=16000)\n",
    "    voice2 = np.append(voice2, voice2)\n",
    "\n",
    "    # pad smaller array with zeros if it's the case or delete last entries\n",
    "    if mix_5_seconds == True:\n",
    "        voice1 = make_mix_of_5_seconds(voice1)\n",
    "        voice2 = make_mix_of_5_seconds(voice2)\n",
    "    else:\n",
    "        voice1 = make_mix_of_10_seconds(voice1)\n",
    "        voice2 = make_mix_of_10_seconds(voice2)\n",
    "\n",
    "    # load the mixed audio \n",
    "    mix = voice1 + voice2\n",
    "\n",
    "    voice1 = np.array(voice1)\n",
    "    voice2 = np.array(voice2)\n",
    "    mix = np.array(mix)\n",
    "\n",
    "    inputs, targets = get_train_set_for_mix(mix, voice1, voice2, samples_per_frame)\n",
    "    inputs_list.extend(inputs)\n",
    "    targets_list.extend(targets)\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    \n",
    "    print(\"create_set_from_voices dims: \", inputs_list.shape, targets_list.shape)\n",
    "    \n",
    "    network_set = dict(zip(inputs_list, targets_list))\n",
    "    \n",
    "    return network_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interchanged_set_from_voices(voice1_file_name, voice2_file_name, \n",
    "                                 samples_per_frame, mix_5_seconds, nb_of_permutations, is_validation):\n",
    "    inputs_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    noise_filename = 'recordings/noises/piano5sec.wav'\n",
    "    \n",
    "    voice1, sr = librosa.load(voice1_file_name, sr=16000) \n",
    "    voice1 = np.append(voice1, voice1)\n",
    "    \n",
    "    voice2, sr = librosa.load(voice2_file_name, sr=16000)\n",
    "    voice2 = np.append(voice2, voice2)\n",
    "    \n",
    "    noise, sr = librosa.load(noise_filename, sr=16000)\n",
    "    noise = np.append(noise, noise)\n",
    "\n",
    "    # pad smaller array with zeros if it's the case or delete last entries\n",
    "    if mix_5_seconds == True:\n",
    "        voice1 = make_mix_of_5_seconds(voice1)\n",
    "        voice2 = make_mix_of_5_seconds(voice2)\n",
    "        noise = make_mix_of_5_seconds(noise)\n",
    "    else:\n",
    "        voice1 = make_mix_of_10_seconds(voice1)\n",
    "        voice2 = make_mix_of_10_seconds(voice2)\n",
    "        noise = make_mix_of_10_seconds(noise)\n",
    "\n",
    "    voices1 = get_voice_interchanged(voice1, is_validation)\n",
    "    voices2 = get_voice_interchanged(voice2, is_validation)\n",
    "        \n",
    "    for i in range(len(voices1)):\n",
    "            mix = voices1[i] + voices2[i]\n",
    "            mix = np.array(mix)\n",
    "\n",
    "            inputs, targets = get_train_set_for_mix(mix, voices1[i], voices2[i], samples_per_frame)\n",
    "            print(inputs.shape)\n",
    "            inputs_list.extend(inputs)\n",
    "            targets_list.extend(targets)\n",
    "    \n",
    "    # add combinations : voice1+noise and voice2+noise only for TRAIN SET\n",
    "#     if is_validation == False:\n",
    "#         noise = noise / 5\n",
    "#         mix = voice1 + noise\n",
    "#         mix = np.array(mix)\n",
    "\n",
    "#         inputs, targets = get_train_set_for_mix(mix, voice1, noise, samples_per_frame)\n",
    "#         print(inputs.shape)\n",
    "#         inputs_list.extend(inputs)\n",
    "#         targets_list.extend(targets)\n",
    "\n",
    "#         mix = voice2 + noise\n",
    "#         mix = np.array(mix)\n",
    "\n",
    "#         inputs, targets = get_train_set_for_mix(mix, voice2, noise, samples_per_frame)\n",
    "#         print(inputs.shape)\n",
    "#         inputs_list.extend(inputs)\n",
    "#         targets_list.extend(targets)\n",
    "    #------------------------------------------------------------\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    \n",
    "    print(\"create_interchanged_set_from_voices dims: \", inputs_list.shape, targets_list.shape)\n",
    "    \n",
    "    network_set = dict(zip(inputs_list, targets_list))\n",
    "    \n",
    "    return network_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_with_noise_from_voices(voice1_file_name, voice2_file_name, \n",
    "                                            samples_per_frame, mix_5_seconds, noise_filename):\n",
    "    inputs_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "            \n",
    "    voice1, sr = librosa.load(voice1_file_name, sr=16000) \n",
    "    voice1 = np.append(voice1, voice1)\n",
    "    \n",
    "    voice2, sr = librosa.load(voice2_file_name, sr=16000)\n",
    "    voice2 = np.append(voice2, voice2)\n",
    "\n",
    "    noise, sr = librosa.load(noise_filename, sr=16000) \n",
    "    noise = np.append(noise, noise)\n",
    "    noise = noise / 5\n",
    "\n",
    "    # pad smaller array with zeros if it's the case or delete last entries\n",
    "    if mix_5_seconds == True:\n",
    "        voice1 = make_mix_of_5_seconds(voice1)\n",
    "        voice2 = make_mix_of_5_seconds(voice2)\n",
    "        noise = make_mix_of_5_seconds(noise)\n",
    "    else:\n",
    "        voice1 = make_mix_of_10_seconds(voice1)\n",
    "        voice2 = make_mix_of_10_seconds(voice2)\n",
    "        noise = make_mix_of_10_seconds(noise)\n",
    "\n",
    "    # load the mixed audio \n",
    "    mix = voice1 + voice2 + noise\n",
    "\n",
    "    voice1 = np.array(voice1)\n",
    "    voice2 = np.array(voice2)\n",
    "    noise = np.array(noise)\n",
    "    mix = np.array(mix)\n",
    "\n",
    "    inputs, targets = get_train_set_for_mix(mix, voice1, voice2, samples_per_frame)\n",
    "    inputs_list.extend(inputs)\n",
    "    targets_list.extend(targets)\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    \n",
    "    print(\"create_set_with_noise_from_voices dims: \", inputs_list.shape, targets_list.shape)\n",
    "    \n",
    "    network_set = dict(zip(inputs_list, targets_list))\n",
    "    \n",
    "    return network_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_from_voices_80_10_10(voice1_file_name, voice2_file_name, \n",
    "                                          samples_per_frame, mix_5_seconds):\n",
    "            \n",
    "    voice1, sr = librosa.load(voice1_file_name, sr=16000) \n",
    "    voice1 = np.append(voice1, voice1)\n",
    "    \n",
    "    voice2, sr = librosa.load(voice2_file_name, sr=16000) \n",
    "    voice2 = np.append(voice2, voice2)\n",
    "\n",
    "    # pad smaller array with zeros if it's the case or delete last entries\n",
    "    if mix_5_seconds == True:\n",
    "        voice1 = make_mix_of_5_seconds(voice1)\n",
    "        voice2 = make_mix_of_5_seconds(voice2)\n",
    "    else:\n",
    "        voice1 = make_mix_of_10_seconds(voice1)\n",
    "        voice2 = make_mix_of_10_seconds(voice2)\n",
    "\n",
    "    train_len = (int)(80/100 * len(voice1))\n",
    "    diff = (int)( (len(voice1) - train_len) / 2)\n",
    "    \n",
    "    # --------------------------------------split voices into train/valid/test sets\n",
    "    voice1_train = voice1[:train_len]\n",
    "    voice2_train = voice2[:train_len]\n",
    "    print(len(voice1_train))\n",
    "    \n",
    "    voice1_valid = voice1[train_len:(train_len + diff)]\n",
    "    voice2_valid = voice2[train_len:(train_len + diff)]\n",
    "    print(len(voice1_valid))\n",
    "    \n",
    "    voice1_test = voice1[(train_len + diff):len(voice1)]\n",
    "    voice2_test = voice2[(train_len + diff):len(voice2)]\n",
    "    print(len(voice1_test))\n",
    "\n",
    "    voice1 = np.array(voice1)\n",
    "    voice2 = np.array(voice2)\n",
    "\n",
    "    # -------------------------------------compute train set [with permutations or without]\n",
    "    inputs_list, targets_list = [], []\n",
    "        \n",
    "#     mix = voice1_train + voice2_train\n",
    "#     mix = np.array(mix)\n",
    "\n",
    "#     inputs, targets = get_train_set_for_mix(mix, voice1_train, voice2_train, samples_per_frame)\n",
    "#     inputs_list.extend(inputs)\n",
    "#     targets_list.extend(targets)\n",
    "    \n",
    "            \n",
    "    voices1 = get_voice_interchanged(voice1_train, False)\n",
    "    voices2 = get_voice_interchanged(voice2_train, False)\n",
    "        \n",
    "    for i in range(len(voices1)):\n",
    "            mix = voices1[i] + voices2[i]\n",
    "            mix = np.array(mix)\n",
    "\n",
    "            inputs, targets = get_train_set_for_mix(mix, voices1[i], voices2[i], samples_per_frame)\n",
    "            inputs_list.extend(inputs)\n",
    "            targets_list.extend(targets)\n",
    "\n",
    "#   for i in range(nb_of_permutations):\n",
    "\n",
    "#       voice1_perm = get_voice_interchanged(voice1_train)\n",
    "#       voice2_perm = get_voice_interchanged(voice2_train)\n",
    "\n",
    "#       mix = voice1_perm + voice2_perm\n",
    "#       mix = np.array(mix)\n",
    "\n",
    "#       inputs, targets = get_train_set_for_mix(mix, voice1, voice2, samples_per_frame)\n",
    "#       inputs_list.extend(inputs)\n",
    "#       targets_list.extend(targets)\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    train_set = dict(zip(inputs_list, targets_list))\n",
    "    \n",
    "    # ------------------------------------------------------compute valid set\n",
    "    inputs_list, targets_list = [], []\n",
    "    \n",
    "    mix = voice1_valid + voice2_valid\n",
    "    inputs, targets = get_train_set_for_mix(mix, voice1_valid, voice2_valid, samples_per_frame)\n",
    "    inputs_list.extend(inputs)\n",
    "    targets_list.extend(targets)\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    valid_set = dict(zip(inputs_list, targets_list))\n",
    "\n",
    "    # ---------------------------------------------------------compute test set\n",
    "    inputs_list, targets_list = [], []\n",
    "    \n",
    "    mix = voice1_test + voice2_test\n",
    "    inputs, targets = get_train_set_for_mix(mix, voice1_test, voice2_test, samples_per_frame)\n",
    "    inputs_list.extend(inputs)\n",
    "    targets_list.extend(targets)\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    test_set = dict(zip(inputs_list, targets_list))\n",
    "    \n",
    "    print(\"create_set_from_voices_80_10_10 sets dim:\", len(train_set), len(valid_set), len(test_set))\n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000\n",
      "8000\n",
      "8000\n",
      "create_set_from_voices_80_10_10 sets dim: 2388 48 48\n",
      "create_set_from_voices dims:  torch.Size([498, 4, 1, 513, 21]) torch.Size([498, 513, 21])\n",
      "(498, 4, 1, 513, 21)\n",
      "(498, 4, 1, 513, 21)\n",
      "(498, 4, 1, 513, 21)\n",
      "(498, 4, 1, 513, 21)\n",
      "(498, 4, 1, 513, 21)\n",
      "(498, 4, 1, 513, 21)\n",
      "create_interchanged_set_from_voices dims:  torch.Size([2988, 4, 1, 513, 21]) torch.Size([2988, 513, 21])\n",
      "(498, 4, 1, 513, 21)\n",
      "create_interchanged_set_from_voices dims:  torch.Size([498, 4, 1, 513, 21]) torch.Size([498, 513, 21])\n",
      "create_set_from_voices dims:  torch.Size([498, 4, 1, 513, 21]) torch.Size([498, 513, 21])\n",
      "create_set_with_noise_from_voices dims:  torch.Size([498, 4, 1, 513, 21]) torch.Size([498, 513, 21])\n"
     ]
    }
   ],
   "source": [
    "noise_filename = 'recordings/noises/piano5sec.wav'\n",
    "noise2_filename = 'recordings/noises/engine.wav'\n",
    "noise3_filename = 'recordings/noises/beeps.wav'\n",
    "\n",
    "voice1_file_name = 'recordings/voice1/arctic_a0007.wav'\n",
    "voice2_file_name = 'recordings/voice2/arctic_a0032.wav'\n",
    "\n",
    "voice1_file_names_list2 = ['recordings/voice1/arctic_a0407.wav']\n",
    "voice2_file_names_list2 = ['recordings/voice2/arctic_a0032.wav']\n",
    "\n",
    "noises_list = ['recordings/noises/engine.wav', 'recordings/noises/applauses.wav', 'recordings/noises/beeps.wav']\n",
    "\n",
    "\n",
    "train_set_80, valid_set_10, test_set_10 = create_set_from_voices_80_10_10(voice1_file_name, voice2_file_name,\n",
    "                                                                        samples_per_frame=samples_per_frame_10ms,\n",
    "                                                                        mix_5_seconds=True)\n",
    "# un singur mix, intreg, simplu\n",
    "train_set = create_set_from_voices(voice1_file_name, voice2_file_name, \n",
    "                                         samples_per_frame = samples_per_frame_10ms,\n",
    "                                        mix_5_seconds= True)\n",
    "\n",
    "# un singur mix, intreg, 3 inregistrari, din care 2 permutate\n",
    "train_set_interchanged = create_interchanged_set_from_voices(voice1_file_name, voice2_file_name, \n",
    "                                         samples_per_frame = samples_per_frame_10ms,\n",
    "                                        mix_5_seconds= True, nb_of_permutations = 3, is_validation=False)\n",
    "\n",
    "# un singur mix, intreg, permutat in alt mod fata de permutarile din train\n",
    "# are si un noise de applause in functie, doar trebuie comentat / decomentat cand e adaugat in mix\n",
    "valid_set = create_interchanged_set_from_voices(voice1_file_name, voice2_file_name,\n",
    "                                                    samples_per_frame = samples_per_frame_10ms,\n",
    "                                                    mix_5_seconds= True, nb_of_permutations = 1, is_validation=True)\n",
    "\n",
    "# un singur mix, intreg, contine doar cele doua voci pentru care s-a facut trainul\n",
    "test_set = create_set_from_voices(voice1_file_name, voice2_file_name,\n",
    "                                                    samples_per_frame=samples_per_frame_10ms,\n",
    "                                                    mix_5_seconds= True)\n",
    "\n",
    "# un singur mix, intreg, cele doua voci + zgomot de pian\n",
    "test_set_with_noise = create_set_with_noise_from_voices(voice1_file_name, voice2_file_name,\n",
    "                                                    samples_per_frame=samples_per_frame_10ms,\n",
    "                                                    mix_5_seconds= True, \n",
    "                                                    noise_filename=noise_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_network(nr_epochs, train_set, valid_set, test_set, model_name, optimizer, network):\n",
    "\n",
    "    # plot train/valid loss contains losses on each epoch, so we can see after each epoch what happens with the error\n",
    "    plot_train_losses = []\n",
    "    plot_valid_losses = []\n",
    "    plot_train_accuracy = []\n",
    "    plot_valid_accuracy = []\n",
    "\n",
    "\n",
    "    epoch = 1\n",
    "    while epoch <= nr_epochs:\n",
    "        \n",
    "        correct_train = 0\n",
    "        correct_test = 0\n",
    "        train_losses, valid_losses = [], []\n",
    "        loss = 0\n",
    "\n",
    "        # training part \n",
    "        network.train()\n",
    "        for index, (input, target) in enumerate(train_set.items()):\n",
    "\n",
    "            # if cuda is available, send (input, target) to gpu\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1. forward propagation\n",
    "            output = network(input)\n",
    "\n",
    "\n",
    "            # 2. loss calculation\n",
    "            loss = loss_function(target, output[len(output)-1])  \n",
    "\n",
    "\n",
    "            # 3. backward propagation\n",
    "            loss.backward() \n",
    "\n",
    "\n",
    "            # 4. weight optimization\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # 5. save the loss for this PF\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            \n",
    "            # 6. check how many items where predicted correctly\n",
    "            correct_train += 1 if loss.item() <= marja_eroare else 0 \n",
    "            \n",
    "\n",
    "        # add the mean loss for this training epoch for ploting\n",
    "        train_mean_loss_for_epoch = np.mean(train_losses)\n",
    "        plot_train_losses.append(train_mean_loss_for_epoch)\n",
    "        \n",
    "        current_accuracy = (int)(100. * correct_train / len(train_set))\n",
    "        plot_train_accuracy.append(current_accuracy)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(\"--------------------------------------Epoch\", str(epoch) ,\"------------------------------\")\n",
    "            print(\"Train set -> \", \"loss: \", loss.item(), \"accuracy: \", current_accuracy, \"%\", \"[\" +\n",
    "                  str(correct_train) + \"/\" + str(len(train_set)) + \"]\")\n",
    "        \n",
    "        #-----------------------------------------------------------------------\n",
    "        # evaluation part \n",
    "#         network.eval()\n",
    "        if epoch % 1 == 0:\n",
    "#             \n",
    "            with torch.no_grad():\n",
    "                for index, (input, target) in enumerate(valid_set.items()):\n",
    "\n",
    "                    # if cuda is available, send (input, target) to gpu\n",
    "                    input, target = input.cuda(), target.cuda()\n",
    "\n",
    "                    # 1. forward propagation\n",
    "                    output = network.forward(input)\n",
    "\n",
    "                    # 2. loss calculation\n",
    "                    loss = loss_function(target, output[len(output)-1]).detach().item()\n",
    "                    \n",
    "                    # 3. save loss for current PF\n",
    "                    valid_losses.append(loss)\n",
    "\n",
    "                    # 4. check how many items where predicted correctly\n",
    "                    correct_test += 1 if loss <= marja_eroare else 0\n",
    "\n",
    "                 # add the mean loss for this training epoch for ploting\n",
    "                valid_mean_loss_for_epoch = np.mean(valid_losses)\n",
    "                plot_valid_losses.append(valid_mean_loss_for_epoch)\n",
    "                \n",
    "                current_accuracy = (int)(100. * correct_test / len(valid_set))\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1 :\n",
    "                    print(\"Valid set -> \", \"loss: \", loss, \"accuracy: \", current_accuracy, \"%\", \"[\" +\n",
    "                          str(correct_test) + \"/\" + str(len(valid_set)) + \"]\")\n",
    "                \n",
    "                if len(plot_valid_accuracy) >= 1:\n",
    "                    if current_accuracy > max(plot_valid_accuracy):\n",
    "                        print(\"--------------------------------------Epoch\", str(epoch) ,\"------------------------------\")\n",
    "                        torch.save(network, str(current_accuracy) + \"-valid-\"+ model_name)\n",
    "                        network_test = torch.load(str(current_accuracy) + \"-valid-\" + model_name)\n",
    "                        eval(network=network_test, test_set=test_set)\n",
    "            \n",
    "            plot_valid_accuracy.append(current_accuracy)\n",
    "            \n",
    "            if len(plot_valid_losses) >= 5:\n",
    "                valid_l = plot_valid_losses[len(plot_valid_losses) - 5 : len(plot_valid_losses)]\n",
    "                train_l = plot_train_losses[len(plot_train_losses) - 5 : len(plot_train_losses)]\n",
    "                if valid_l == sorted(valid_l) and train_l == sorted(train_l, reverse=True):\n",
    "                    \n",
    "                    print(\"Stop training due to validation loss increasing [training loss decreasing] for 5 consecutive epochs.\")\n",
    "                    break\n",
    "                if all(x >= 98 for x in plot_valid_accuracy[len(plot_valid_accuracy) - 10: len(plot_valid_accuracy)]):\n",
    "                    print(\"Stop training due to validation accuracy greater than 98% for 10 epochs.\")\n",
    "                    break\n",
    "                    \n",
    "        epoch += 1\n",
    "\n",
    "    torch.save(network, model_name)\n",
    "    \n",
    "    plt.plot(plot_train_losses, label='Training loss')\n",
    "    plt.plot(plot_valid_losses, label='Validation loss')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(plot_train_accuracy, label='Training accuracy')\n",
    "    plt.plot(plot_valid_accuracy, label='Validation accuracy')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECONSTRUIRE SUNET PE FRAME-URI, FOLOSIND TARGETURILE GENERATE PT SETUL DE ANTRENARE\n",
    "def split_audios_epoch(name, mask, mix, samples_per_frame):\n",
    "    \n",
    "    frame_pos = (int)(nb_of_frames_in_AF/2)\n",
    "\n",
    "    mask_stack = torch.stack(mask)\n",
    "    cpu_mask = mask_stack.cpu()\n",
    "    n_mask = cpu_mask.detach().numpy()\n",
    "\n",
    "    mix_frames = np.array([mix[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    sound1 = np.empty([0,])\n",
    "    sound2 = np.empty([0,])\n",
    "\n",
    "\n",
    "    for i in range (0, len(mix_frames)):\n",
    "        stft_mix = librosa.stft(librosa.to_mono(mix_frames[i]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        # reteaua nu a invatat mastile pentru primele frame_pos bucati [PF] din mix\n",
    "        # deci las matricea STFT asa cum e in mix\n",
    "        if i < frame_pos :\n",
    "            y_frame_1_stft_with_mask = stft_mix\n",
    "            y_frame_2_stft_with_mask = stft_mix\n",
    "\n",
    "            inverse_sound1_stft = librosa.istft(y_frame_1_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "            inverse_sound2_stft = librosa.istft(y_frame_2_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "\n",
    "            sound1 = np.concatenate((sound1, inverse_sound1_stft))\n",
    "            sound2 = np.concatenate((sound2, inverse_sound2_stft))\n",
    "            \n",
    "        else:\n",
    "            # n_mask[i-7] pentru ca reteaua invata pt AF-uri facute pe cate 8 PF-uri -> 15 items pt AF\n",
    "            # abia de cand ajunge la primul AF care s a putut compune, folosesc masca invata de retea\n",
    "            \n",
    "            y_frame_1_stft_with_mask = np.multiply(n_mask[i-frame_pos], stft_mix)\n",
    "            y_frame_2_stft_with_mask = np.multiply((1 -  n_mask[i-frame_pos]), stft_mix)\n",
    "\n",
    "            inverse_sound1_stft = librosa.istft(y_frame_1_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "            inverse_sound2_stft = librosa.istft(y_frame_2_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "\n",
    "            sound1 = np.concatenate((sound1, inverse_sound1_stft))\n",
    "            sound2 = np.concatenate((sound2, inverse_sound2_stft))\n",
    "\n",
    "\n",
    "    librosa.output.write_wav(\"recordings/voice1-\"+ str(name) + \".wav\", sound1, sr = 16000)\n",
    "    librosa.output.write_wav(\"recordings/voice2-\"+ str(name) + \".wav\", sound2, sr = 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(network, test_set):\n",
    "\n",
    "#     network.eval()\n",
    "                \n",
    "    test_loss, test_mask, test_accuracy = [], [], []\n",
    "    correct_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, (input, target) in enumerate(test_set.items()):\n",
    "\n",
    "            # if cuda is available, send (input, target) to gpu\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # 1. forward propagation\n",
    "            output = network.forward(input)\n",
    "\n",
    "            # 2. loss calculation\n",
    "            loss = loss_function(target, output[len(output)-1]).detach().item()\n",
    "\n",
    "            # 3. save the mask for the current PF, meaning the last entry in the output\n",
    "            current_mask = output[len(output)-1]\n",
    "            test_mask.append(current_mask)\n",
    "\n",
    "            test_loss.append(loss)\n",
    "\n",
    "            # 4. check how many items where predicted correctly\n",
    "            correct_test += 1 if loss <= marja_eroare else 0\n",
    "\n",
    "\n",
    "        current_accuracy = 100. * correct_test / len(test_set)\n",
    "        print(\"Test set -> \", \"test loss: \", np.mean(test_loss), \"accuracy: \", (int)(current_accuracy), \"%\", \"[\" +\n",
    "                          str(correct_test) + \"/\" + str(len(test_set)) + \"]\")\n",
    "\n",
    "        return test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAdam(nr_epochs, train_set, valid_set, test_set, mixnb):\n",
    "\n",
    "    print(\"__________________________________________ADAM FC_________________________________\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model_name = str(nr_epochs) + \"epoci-\"+ mixnb + \".pth\"\n",
    "    \n",
    "    network = Network()\n",
    "    \n",
    "    print(\"network sent to CUDA\")\n",
    "    network.cuda()\n",
    "    \n",
    "    # set optimizer -> article : Adam, lr = 0.001, b1 = 0.9, b2 = 0.999\n",
    "    optimizer = optim.Adam(network.parameters(), lr = 0.001, betas = (0.9, 0.999))\n",
    "        \n",
    "    start_time = time.time()\n",
    "    train_network(nr_epochs=nr_epochs, \n",
    "                train_set=train_set, valid_set=valid_set, test_set = test_set,\n",
    "                model_name=model_name,\n",
    "                optimizer=optimizer,\n",
    "                network=network)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"Elapsed time for \" + model_name + \" : {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    \n",
    "    return model_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAdamConv(nr_epochs, train_set, valid_set, test_set, mixnb):\n",
    "    \n",
    "    print(\"__________________________________________ADAM CONV + FC_________________________________\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model_name = str(nr_epochs) + \"epoci-\"+ mixnb + \".pth\"\n",
    "    \n",
    "    networkConv = NetworkConv()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"networkConv sent to CUDA\")\n",
    "        networkConv.cuda()\n",
    "        \n",
    "    # set optimizer -> article : Adam, lr = 0.001, b1 = 0.9, b2 = 0.999\n",
    "    optimizer = optim.Adam(networkConv.parameters(), lr = 0.001, betas = (0.9, 0.999))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_network(nr_epochs=nr_epochs, \n",
    "                train_set=train_set, valid_set=valid_set, test_set=test_set,\n",
    "                model_name=model_name,\n",
    "                optimizer=optimizer,\n",
    "                network=networkConv)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"Elapsed time for \" + model_name + \" : {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    \n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mix_from_voices(voice1_filename, voice2_filename, mix_5_sec):\n",
    "    \n",
    "    voice1, sr = librosa.load(voice1_filename, sr=16000) \n",
    "    voice1 = np.append(voice1, voice1)\n",
    "    \n",
    "    voice2, sr = librosa.load(voice2_filename, sr=16000)\n",
    "    voice2 = np.append(voice2, voice2)\n",
    "    \n",
    "    if mix_5_sec == True:\n",
    "        voice1 = make_mix_of_5_seconds(voice1)\n",
    "        voice2 = make_mix_of_5_seconds(voice2)\n",
    "    else:\n",
    "        voice1 = make_mix_of_10_seconds(voice1)\n",
    "        voice2 = make_mix_of_10_seconds(voice2)\n",
    "        \n",
    "    mix = voice1 + voice2\n",
    "    \n",
    "    librosa.output.write_wav(\"recordings/mix-withOUT-noise.wav\", mix, 16000)\n",
    "    \n",
    "    return mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mix_from_voices_with_noise(voice1_filename, voice2_filename, noise_filename, mix_5_sec):\n",
    "    \n",
    "    voice1, sr = librosa.load(voice1_filename, sr=16000) \n",
    "    voice1 = np.append(voice1, voice1)\n",
    "    \n",
    "    voice2, sr = librosa.load(voice2_filename, sr=16000)\n",
    "    voice2 = np.append(voice2, voice2)\n",
    "    \n",
    "    noise, sr = librosa.load(noise_filename, sr=16000)\n",
    "    noise = np.append(noise, noise)\n",
    "    noise = noise / 5\n",
    "    \n",
    "    if mix_5_sec == True:\n",
    "        voice1 = make_mix_of_5_seconds(voice1)\n",
    "        voice2 = make_mix_of_5_seconds(voice2)\n",
    "        noise = make_mix_of_5_seconds(noise)\n",
    "    else:\n",
    "        voice1 = make_mix_of_10_seconds(voice1)\n",
    "        voice2 = make_mix_of_10_seconds(voice2)\n",
    "        noise = make_mix_of_10_seconds(noise)\n",
    "    \n",
    "    mix = voice1 + voice2 + noise\n",
    "    \n",
    "    librosa.output.write_wav(\"recordings/mix-with-noise.wav\", mix, 16000)\n",
    "    return mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________ADAM FC_________________________________\n",
      "network sent to CUDA\n",
      "--------------------------------------Epoch 1 ------------------------------\n",
      "Train set ->  loss:  0.12880367040634155 accuracy:  0 % [7/2988]\n",
      "Valid set ->  loss:  0.062467802315950394 accuracy:  0 % [0/498]\n",
      "--------------------------------------Epoch 5 ------------------------------\n",
      "Test set ->  test loss:  0.09270383174583526 accuracy:  0 % [4/498]\n",
      "--------------------------------------Epoch 10 ------------------------------\n",
      "Train set ->  loss:  0.07739967107772827 accuracy:  2 % [65/2988]\n",
      "Valid set ->  loss:  0.02678583562374115 accuracy:  2 % [10/498]\n",
      "--------------------------------------Epoch 10 ------------------------------\n",
      "Test set ->  test loss:  0.08944254846173716 accuracy:  0 % [3/498]\n",
      "--------------------------------------Epoch 16 ------------------------------\n",
      "Test set ->  test loss:  0.07771188665899527 accuracy:  0 % [4/498]\n",
      "--------------------------------------Epoch 20 ------------------------------\n",
      "Train set ->  loss:  0.04121655598282814 accuracy:  2 % [88/2988]\n",
      "Valid set ->  loss:  0.01443832740187645 accuracy:  2 % [14/498]\n",
      "--------------------------------------Epoch 30 ------------------------------\n",
      "Train set ->  loss:  0.05043374374508858 accuracy:  3 % [115/2988]\n",
      "Valid set ->  loss:  0.027429906651377678 accuracy:  2 % [12/498]\n",
      "--------------------------------------Epoch 31 ------------------------------\n",
      "Test set ->  test loss:  0.07555006408080249 accuracy:  1 % [6/498]\n",
      "--------------------------------------Epoch 40 ------------------------------\n",
      "Train set ->  loss:  0.04079382121562958 accuracy:  3 % [110/2988]\n",
      "Valid set ->  loss:  0.01164955087006092 accuracy:  3 % [18/498]\n",
      "--------------------------------------Epoch 43 ------------------------------\n",
      "Test set ->  test loss:  0.07194993036320381 accuracy:  0 % [3/498]\n",
      "--------------------------------------Epoch 50 ------------------------------\n",
      "Train set ->  loss:  0.02153889462351799 accuracy:  4 % [137/2988]\n",
      "Valid set ->  loss:  0.011492045596241951 accuracy:  4 % [22/498]\n",
      "--------------------------------------Epoch 60 ------------------------------\n",
      "Train set ->  loss:  0.02620280347764492 accuracy:  5 % [166/2988]\n",
      "Valid set ->  loss:  0.009652598761022091 accuracy:  5 % [28/498]\n",
      "--------------------------------------Epoch 65 ------------------------------\n",
      "Test set ->  test loss:  0.0763103227775327 accuracy:  1 % [7/498]\n",
      "--------------------------------------Epoch 70 ------------------------------\n",
      "Train set ->  loss:  0.01490157563239336 accuracy:  5 % [179/2988]\n",
      "Valid set ->  loss:  0.011939392425119877 accuracy:  5 % [27/498]\n",
      "--------------------------------------Epoch 80 ------------------------------\n",
      "Train set ->  loss:  0.019765257835388184 accuracy:  6 % [195/2988]\n",
      "Valid set ->  loss:  0.008992297574877739 accuracy:  8 % [41/498]\n",
      "--------------------------------------Epoch 80 ------------------------------\n",
      "Test set ->  test loss:  0.07427829387130208 accuracy:  2 % [10/498]\n",
      "--------------------------------------Epoch 90 ------------------------------\n",
      "Train set ->  loss:  0.017935317009687424 accuracy:  7 % [218/2988]\n",
      "Valid set ->  loss:  0.01125185377895832 accuracy:  6 % [30/498]\n",
      "--------------------------------------Epoch 100 ------------------------------\n",
      "Train set ->  loss:  0.01225828193128109 accuracy:  7 % [226/2988]\n",
      "Valid set ->  loss:  0.008593343198299408 accuracy:  7 % [39/498]\n",
      "--------------------------------------Epoch 104 ------------------------------\n",
      "Test set ->  test loss:  0.07355363900028918 accuracy:  2 % [11/498]\n"
     ]
    }
   ],
   "source": [
    "import winsound\n",
    "import os\n",
    "\n",
    "voice1 = 'recordings/voice1/arctic_a0007.wav'\n",
    "voice2 = 'recordings/voice2/arctic_a0032.wav'\n",
    "    \n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model_name = runAdam(300, train_set, valid_set, test_set_with_noise, \"FC-mix-intreg-v1(7)-v2(32)\")\n",
    "\n",
    "# # test first what it learned from train\n",
    "# mix = get_mix_from_voices(voice1, voice2, mix_5_sec=True)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set)\n",
    "# split_audios_epoch(\"FC-mix-intreg-v1(7)-v2(32)-test\", test_mask, mix, samples_per_frame=samples_per_frame_10ms)\n",
    "\n",
    "# # test with noise\n",
    "# mix = get_mix_from_voices_with_noise(voice1, voice2, 'recordings/noises/piano5sec.wav', mix_5_sec = True)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set_with_noise)\n",
    "# split_audios_epoch(\"FC-mix-intreg-v1(7)-v2(32)-test-noise\", test_mask, mix, samples_per_frame=samples_per_frame_10ms)\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model_name = runAdamConv(200, train_set, valid_set, test_set_with_noise, \"CONV-mix-intreg-v1(7)-v2(32)\")\n",
    "\n",
    "# # test first what it learned from train\n",
    "# mix = get_mix_from_voices(voice1, voice2)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set)\n",
    "# split_audios_epoch(\"CONV-mix-intreg-v1(7)-v2(32)-test\", test_mask, mix)\n",
    "\n",
    "# # test with noise\n",
    "# mix = get_mix_from_voices_with_noise(voice1, voice2, 'recordings/noises/piano5sec.wav')\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set_with_noise)\n",
    "# split_audios_epoch(\"CONV-mix-intreg-v1(7)-v2(32)-test-noise\", test_mask, mix)\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model_name = runAdam(200, train_set_80, valid_set_10, test_set_10, \"FC-mix-80-10-10-v1(7)-v2(32)\")\n",
    "\n",
    "# # test first what it learned from train\n",
    "# mix = get_mix_from_voices(voice1, voice2, mix_5_sec=True)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set)\n",
    "# split_audios_epoch(\"FC-mix-80-10-10-v1(7)-v2(32)-test\", test_mask, mix, samples_per_frame_10ms)\n",
    "\n",
    "# # test with noise\n",
    "# mix = get_mix_from_voices_with_noise(voice1, voice2, 'recordings/noises/piano5sec.wav', mix_5_sec=True)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set_with_noise)\n",
    "# split_audios_epoch(\"FC-mix-80-10-10-v1(7)-v2(32)-test-noise\", test_mask, mix, samples_per_frame_10ms)\n",
    "\n",
    "# #-----------------------------------------------------------------------------------\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model_name = runAdamConv(200, train_set_80, valid_set_10, test_set_10, \"CONV-mix-80-10-10-v1(7)-v2(32)\")\n",
    "\n",
    "# # test first what it learned from train\n",
    "# mix = get_mix_from_voices(voice1, voice2)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set)\n",
    "# split_audios_epoch(\"CONV-mix-80-10-10-v1(7)-v2(32)-test\", test_mask, mix)\n",
    "\n",
    "# # test with noise\n",
    "# mix = get_mix_from_voices_with_noise(voice1, voice2, 'recordings/noises/piano5sec.wav')\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set_with_noise)\n",
    "# split_audios_epoch(\"CONV-mix-80-10-10-v1(7)-v2(32)-test-noise\", test_mask, mix)\n",
    "\n",
    "# #-----------------------------------------------------------------------------------\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model_name = runAdam(200, train_set_interchanged, valid_set, test_set_with_noise, \"FC-mix-intreg-permutat-v1(7)-v2(32)\")\n",
    "\n",
    "# test first what it learned from train\n",
    "mix = get_mix_from_voices(voice1, voice2, mix_5_sec=True)\n",
    "network = torch.load(model_name)\n",
    "test_mask = eval(network=network, test_set=test_set)\n",
    "split_audios_epoch(\"FC-mix-intreg-permutat-v1(7)-v2(32)-test\", test_mask, mix, samples_per_frame=samples_per_frame_10ms)\n",
    "\n",
    "# test with noise\n",
    "mix = get_mix_from_voices_with_noise(voice1, voice2, 'recordings/noises/piano5sec.wav', mix_5_sec = True)\n",
    "network = torch.load(model_name)\n",
    "test_mask = eval(network=network, test_set=test_set_with_noise)\n",
    "split_audios_epoch(\"FC-mix-intreg-permutat-v1(7)-v2(32)-test-noise\", test_mask, mix, samples_per_frame=samples_per_frame_10ms)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "# mai trebuie convolutionala aici\n",
    "# ------------------------------------------------------------------------------------\n",
    "# torch.cuda.empty_cache()\n",
    "# model_name = runAdam(200, train_set_interchanged_and_noisy, valid_set, test_set_with_noise, \"FC-mix-intreg-permutat-si2zgomote-v1(7)-v2(32)\")\n",
    "\n",
    "# # test first what it learned from train\n",
    "# mix = get_mix_from_voices(voice1, voice2, mix_5_sec=True)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set)\n",
    "# split_audios_epoch(\"FC-mix-intreg-permutat-si2zgomote-v1(7)-v2(32)-test\", test_mask, mix, samples_per_frame=samples_per_frame_10ms)\n",
    "\n",
    "# # test with noise\n",
    "# mix = get_mix_from_voices_with_noise(voice1, voice2, 'recordings/noises/piano5sec.wav', mix_5_sec = True)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set_with_noise)\n",
    "# split_audios_epoch(\"FC-mix-intreg-permutat-si2zgomote-v1(7)-v2(32)-test-noise\", test_mask, mix, samples_per_frame=samples_per_frame_10ms)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model_name = runAdam(200, train_set_80, valid_set_10, test_set_10, \"FC-mix-80-10-10-permutat-v1(7)-v2(32)\")\n",
    "\n",
    "# # test first what it learned from train\n",
    "# mix = get_mix_from_voices(voice1, voice2, mix_5_sec=True)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set)\n",
    "# split_audios_epoch(\"FC-mix-80-10-10-permutat-v1(7)-v2(32)-test\", test_mask, mix, samples_per_frame_10ms)\n",
    "\n",
    "# # test with noise\n",
    "# mix = get_mix_from_voices_with_noise(voice1, voice2, 'recordings/noises/piano5sec.wav', mix_5_sec=True)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set_with_noise)\n",
    "# split_audios_epoch(\"FC-mix-80-10-10-permutat-v1(7)-v2(32)-test-noise\", test_mask, mix, samples_per_frame_10ms)\n",
    "\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
