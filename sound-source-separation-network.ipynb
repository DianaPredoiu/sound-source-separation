{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\envs\\machine-learning\\lib\\site-packages\\pydub\\utils.py:165: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import librosa, librosa.display\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import my_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import time\n",
    "\n",
    "n_fft = 1024\n",
    "hop_length = 8\n",
    "sr = 16000\n",
    "\n",
    "# cate bucati PF iau in considerare pentru AF -> cu overlapping => 15 frame-uri \n",
    "# ultimul frame din AF fiind PF-ul curent\n",
    "# nb_of_PFs_per_AF = 4\n",
    "nb_of_frames_in_AF = 4\n",
    "exp_w_avg_beta = 0.98\n",
    "marja_eroare = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliz(mix):\n",
    "    mu = np.mean(mix, axis=0)\n",
    "    var = np.var(mix, axis=0)\n",
    "\n",
    "    mix_norm = (mix - mu) / np.sqrt(var + 1e-8)\n",
    "    \n",
    "    return mix_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STFT_AF_frames(AF_array, PF_size):\n",
    "    frames = []\n",
    "    half = (int)(PF_size/2)\n",
    "    frames_added = 0\n",
    "    \n",
    "    i = AF_array.shape[0]-1\n",
    "    \n",
    "    # use AF as 4 times bigger than PF's size -> PF = 10 ms => AF = 40 ms\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 1. add PF[i]\n",
    "    frame = librosa.stft(librosa.to_mono(AF_array[i]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "#     frame = normaliz(frame)\n",
    "    \n",
    "    tensor = []\n",
    "    tensor.append(frame)        \n",
    "    frames.append(tensor)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 2. add PF[i-1, i] -> overlap\n",
    "    overlap_frame = AF_array[i-1][half:]\n",
    "    overlap_frame = np.concatenate((overlap_frame, AF_array[i][:half]))\n",
    "    \n",
    "    frame = librosa.stft(librosa.to_mono(overlap_frame), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "#     frame = normaliz(frame)\n",
    "\n",
    "    tensor = []\n",
    "    tensor.append(frame)\n",
    "    frames.append(tensor)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 3. add PF[i - 1]\n",
    "    frame = librosa.stft(librosa.to_mono(AF_array[i - 1]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "#     frame = normaliz(frame)\n",
    "    \n",
    "    tensor = []\n",
    "    tensor.append(frame)        \n",
    "    frames.append(tensor)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 4. add PF[i-1, i] -> overlap\n",
    "    overlap_frame = AF_array[i-1][half:]\n",
    "    overlap_frame = np.concatenate((overlap_frame, AF_array[i][:half]))\n",
    "    \n",
    "    frame = librosa.stft(librosa.to_mono(overlap_frame), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "#     frame = normaliz(frame)\n",
    "\n",
    "    tensor = []\n",
    "    tensor.append(frame)\n",
    "    frames.append(tensor)\n",
    "        \n",
    "    \n",
    "    return np.asarray(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_set_for_mix(mix, voice_1, voice_2, samples_per_frame):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    mix_frames = np.array([mix[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    voice_1_frames = np.array([voice_1[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    voice_2_frames = np.array([voice_2[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    \n",
    "    for index in range(mix_frames.shape[0]):\n",
    "        \n",
    "        if index < 2:\n",
    "            continue\n",
    "        \n",
    "        # 1. create train set input for AF with current PF and previous frames\n",
    "        AF_frames = np.array([mix_frames[i] for i in range(index - 2, index)])      \n",
    "        AF_STFT_frames = get_STFT_AF_frames(AF_array=AF_frames, PF_size= samples_per_frame)\n",
    "        inputs.append(AF_STFT_frames)\n",
    "        \n",
    "        # 2. create train set target for that AF, containing the mask for the current PF\n",
    "        stft_voice_1 = librosa.stft(librosa.to_mono(voice_1_frames[index]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        stft_voice_2 = librosa.stft(librosa.to_mono(voice_2_frames[index]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        mask = my_utils.compute_mask(stft_voice_1, stft_voice_2)\n",
    "        \n",
    "        targets.append(mask)\n",
    "    \n",
    "    # train set for only one audio\n",
    "    train_set_input = torch.from_numpy(np.array(inputs))\n",
    "    \n",
    "    # target contains the calculated masks for each PF from mix\n",
    "    train_set_target = torch.from_numpy(np.array(targets))\n",
    "    return np.array(inputs), np.array(targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_set_for_mix_noise(mix, voice_1, voice_2, samples_per_frame):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    mix_frames = np.array([mix[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    voice_1_frames = np.array([voice_1[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    voice_2_frames = np.array([voice_2[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    \n",
    "    for index in range(mix_frames.shape[0]):\n",
    "        \n",
    "        if index < 2:\n",
    "            continue\n",
    "        \n",
    "        # 1. create train set input for AF with current PF and previous frames\n",
    "        AF_frames = np.array([mix_frames[i] for i in range(index - 2, index)])      \n",
    "        AF_STFT_frames = get_STFT_AF_frames(AF_array=AF_frames, PF_size= samples_per_frame)\n",
    "        inputs.append(AF_STFT_frames)\n",
    "        \n",
    "        # 2. create train set target for that AF, containing the mask for the current PF\n",
    "        stft_voice_1 = librosa.stft(librosa.to_mono(voice_1_frames[index]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        stft_voice_2 = librosa.stft(librosa.to_mono(voice_2_frames[index]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        mask = my_utils.compute_mask(stft_voice_1, stft_voice_2)\n",
    "        \n",
    "        targets.append(mask)\n",
    "    \n",
    "    # train set for only one audio\n",
    "    train_set_input = torch.from_numpy(np.array(inputs))\n",
    "    \n",
    "    # target contains the calculated masks for each PF from mix\n",
    "    train_set_target = torch.from_numpy(np.array(targets))\n",
    "\n",
    "    return np.array(inputs), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    height = 0\n",
    "    width = 0\n",
    "    \n",
    "    # default values for height and width are given for a processing frame of 10 ms\n",
    "    # 5 ms  = [513, 11]\n",
    "    def __init__(self, height = 513, width = 21):\n",
    "        super(Network, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # layer 1\n",
    "        self.fc1 = nn.Linear(1 * height * width, 250)\n",
    "        self.fc1_batch = nn.BatchNorm1d(250)\n",
    "        \n",
    "        # layer 2\n",
    "        self.fc2 = nn.Linear(250, 250)\n",
    "        self.fc2_batch = nn.BatchNorm1d(250)\n",
    "        \n",
    "        # layer 3\n",
    "        self.fc3 = nn.Linear(250, height * width)\n",
    "        self.fc3_batch = nn.BatchNorm1d(height * width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # layer 1\n",
    "        # firstly, transform the matrix into an array for the FC\n",
    "        x = x.view(-1, self.height * self.width)        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 3\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc3_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = x.view(nb_of_frames_in_AF, self.height, self.width) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkConv(nn.Module):\n",
    "    height = 0\n",
    "    width = 0\n",
    "    \n",
    "    # default values for height and width are given for a processing frame of 10 ms\n",
    "    # 5 ms  = [513, 11]\n",
    "    def __init__(self, height = 513, width = 21):\n",
    "        super(NetworkConv, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, padding=2)\n",
    "        self.conv1_batch = nn.BatchNorm2d(20)\n",
    "        \n",
    "        # layer 2\n",
    "        self.conv2 = nn.Conv2d(20, 20, kernel_size=5, padding=2)\n",
    "        self.conv2_batch = nn.BatchNorm2d(20)\n",
    "    \n",
    "        # layer 3\n",
    "        self.fc1 = nn.Linear(20 * height * width, 250)\n",
    "        self.fc1_batch = nn.BatchNorm1d(250)\n",
    "        \n",
    "        # layer 4\n",
    "        self.fc2 = nn.Linear(250, height * width)\n",
    "        self.fc2_batch = nn.BatchNorm1d(height * width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # firstly, transform the matrix into an array for the FC\n",
    "        \n",
    "        \n",
    "        # layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv1_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 3\n",
    "        x = x.view(-1, self.height * self.width * 20) \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 4\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = x.view(nb_of_frames_in_AF, self.height, self.width) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mix_of_10_seconds(voice):\n",
    "    nb_sample_for_10_seconds = 160000\n",
    "    \n",
    "    if len(voice) < nb_sample_for_10_seconds:\n",
    "        voice = np.pad(voice, (0,(nb_sample_for_10_seconds - len(voice) )), 'constant', constant_values=(0))\n",
    "    elif len(voice) > nb_sample_for_10_seconds:\n",
    "        voice = voice[0:nb_sample_for_10_seconds]\n",
    "    \n",
    "    return voice        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_from_voices_lists(voice1_file_names_list, voice2_file_names_list):\n",
    "    inputs_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    frame_length_ms = 10\n",
    "    total_length_ms = 10000.0\n",
    "    nb_of_samples = 160000\n",
    "    samples_per_frame = (int)(nb_of_samples * frame_length_ms / total_length_ms)\n",
    "    \n",
    "    for voice1_file_name in voice1_file_names_list:\n",
    "        for voice2_file_name in voice2_file_names_list:\n",
    "            \n",
    "            voice1, sr = librosa.load(voice1_file_name, sr=16000) \n",
    "            voice1 = np.append(voice1, voice1)\n",
    "            \n",
    "            voice2, sr = librosa.load(voice2_file_name, sr=16000)\n",
    "            voice2 = np.append(voice2, voice2)\n",
    "\n",
    "            # pad smaller array with zeros if it's the case or delete last entries\n",
    "            voice1 = make_mix_of_10_seconds(voice1)\n",
    "            voice2 = make_mix_of_10_seconds(voice2)\n",
    "\n",
    "            # load the mixed audio \n",
    "            mix = voice1 + voice2\n",
    "\n",
    "            voice1 = np.array(voice1)\n",
    "            voice2 = np.array(voice2)\n",
    "            mix = np.array(mix)\n",
    "\n",
    "            inputs, targets = get_train_set_for_mix(mix, voice1, voice2, samples_per_frame)\n",
    "            inputs_list.extend(inputs)\n",
    "            targets_list.extend(targets)\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    \n",
    "    print(\"dims for train set: \", inputs_list.shape, targets_list.shape)\n",
    "    \n",
    "    network_set = dict(zip(inputs_list, targets_list))\n",
    "    \n",
    "    return network_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_valid_set_from_voices_lists(voice1_file_names_list, voice2_file_names_list, noise_filename):\n",
    "    inputs_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    frame_length_ms = 10\n",
    "    total_length_ms = 5000.0\n",
    "    nb_of_samples = 80000\n",
    "    samples_per_frame = (int)(nb_of_samples * frame_length_ms / total_length_ms)\n",
    "    \n",
    "    i = 1\n",
    "    for voice1_file_name in voice1_file_names_list:\n",
    "        for voice2_file_name in voice2_file_names_list:\n",
    "            \n",
    "            voice1, sr = librosa.load(voice1_file_name, sr=16000) \n",
    "            voice2, sr = librosa.load(voice2_file_name, sr=16000) \n",
    "            noise, sr = librosa.load(noise_filename, sr=16000) \n",
    "\n",
    "            # pad smaller array with zeros if it's the case or delete last entries\n",
    "            voice1 = make_mix_of_10_seconds(voice1)\n",
    "            voice2 = make_mix_of_10_seconds(voice2)\n",
    "\n",
    "            # load the mixed audio \n",
    "            mix = voice1 + voice2\n",
    "            \n",
    "            if i % 2 == 1:\n",
    "                noise = make_mix_of_10_seconds(noise)\n",
    "                noise = noise / 5\n",
    "                mix += noise\n",
    "\n",
    "            voice1 = np.array(voice1)\n",
    "            voice2 = np.array(voice2)\n",
    "            mix = np.array(mix)\n",
    "\n",
    "            inputs, targets = get_train_set_for_mix(mix, voice1, voice2, samples_per_frame)\n",
    "            inputs_list.extend(inputs)\n",
    "            targets_list.extend(targets)\n",
    "            i += 1\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    \n",
    "    print(\"dims for valid set: \", inputs_list.shape, targets_list.shape)\n",
    "    \n",
    "    network_set = dict(zip(inputs_list, targets_list))\n",
    "    \n",
    "    return network_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_set_from_voices_lists(voice1_file_names_list, voice2_file_names_list, noise_filename):\n",
    "    inputs_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    frame_length_ms = 10\n",
    "    total_length_ms = 5000.0\n",
    "    nb_of_samples = 80000\n",
    "    samples_per_frame = (int)(nb_of_samples * frame_length_ms / total_length_ms)\n",
    "    \n",
    "    for voice1_file_name in voice1_file_names_list:\n",
    "        for voice2_file_name in voice2_file_names_list:\n",
    "            \n",
    "            voice1, sr = librosa.load(voice1_file_name, sr=16000) \n",
    "            voice2, sr = librosa.load(voice2_file_name, sr=16000) \n",
    "            noise, sr = librosa.load(noise_filename, sr = 16000)\n",
    "\n",
    "            # pad smaller array with zeros if it's the case or delete last entries\n",
    "            voice1 = make_mix_of_10_seconds(voice1)\n",
    "            voice2 = make_mix_of_10_seconds(voice2)\n",
    "            noise = make_mix_of_10_seconds(noise)\n",
    "\n",
    "            # load the mixed audio \n",
    "            noise = noise / 5\n",
    "            mix = voice1 + voice2 + noise\n",
    "\n",
    "            voice1 = np.array(voice1)\n",
    "            voice2 = np.array(voice2)\n",
    "            mix = np.array(mix)\n",
    "\n",
    "            inputs, targets = get_train_set_for_mix(mix, voice1, voice2, samples_per_frame)\n",
    "            inputs_list.extend(inputs)\n",
    "            targets_list.extend(targets)\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    \n",
    "    print(\"dims for test set: \", inputs_list.shape, targets_list.shape)\n",
    "    \n",
    "    network_set = dict(zip(inputs_list, targets_list))\n",
    "    \n",
    "    return network_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_from_voices_lists_80_10_10(voice1_file_names_list, voice2_file_names_list):\n",
    "    inputs_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    frame_length_ms = 10\n",
    "    total_length_ms = 10000.0\n",
    "    nb_of_samples = 160000\n",
    "    samples_per_frame = (int)(nb_of_samples * frame_length_ms / total_length_ms)\n",
    "    \n",
    "    for voice1_file_name in voice1_file_names_list:\n",
    "        for voice2_file_name in voice2_file_names_list:\n",
    "            \n",
    "            voice1, sr = librosa.load(voice1_file_name, sr=16000) \n",
    "            voice1 = np.append(voice1, voice1)\n",
    "            \n",
    "            voice2, sr = librosa.load(voice2_file_name, sr=16000) \n",
    "            voice2 = np.append(voice2, voice2)\n",
    "\n",
    "            # pad smaller array with zeros if it's the case or delete last entries\n",
    "            voice1 = make_mix_of_10_seconds(voice1)\n",
    "            voice2 = make_mix_of_10_seconds(voice2)\n",
    "\n",
    "            # load the mixed audio \n",
    "            mix = voice1 + voice2\n",
    "\n",
    "            voice1 = np.array(voice1)\n",
    "            voice2 = np.array(voice2)\n",
    "            mix = np.array(mix)\n",
    "\n",
    "            inputs, targets = get_train_set_for_mix(mix, voice1, voice2, samples_per_frame)\n",
    "            inputs_list.extend(inputs)\n",
    "            targets_list.extend(targets)\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    \n",
    "    print(\"dims for 80-10-10 set: \", inputs_list.shape, targets_list.shape)\n",
    "    \n",
    "    train_len = (int)(80/100 * len(inputs_list))\n",
    "    m = (int)( (len(inputs_list) - train_len) / 2)\n",
    "\n",
    "    train_set = dict(zip(inputs_list[:train_len, :, :], targets_list[:train_len, :, :]))\n",
    "    valid_set = dict(zip(inputs_list[train_len:(train_len + m), :, :], targets_list[train_len:(train_len + m), :, :]))\n",
    "    test_set = dict(zip(inputs_list[(train_len + m):len(inputs_list), :, :], targets_list[(train_len + m):len(inputs_list), :, :]))\n",
    "\n",
    "    print(\"lens\", len(train_set), len(valid_set), len(test_set))\n",
    "    return train_set, train_set, train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims for 80-10-10 set:  torch.Size([998, 4, 1, 513, 21]) torch.Size([998, 513, 21])\n",
      "lens 798 100 100\n",
      "dims for 80-10-10 set:  torch.Size([998, 4, 1, 513, 21]) torch.Size([998, 513, 21])\n",
      "lens 798 100 100\n",
      "dims for train set:  torch.Size([998, 4, 1, 513, 21]) torch.Size([998, 513, 21])\n",
      "dims for train set:  torch.Size([998, 4, 1, 513, 21]) torch.Size([998, 513, 21])\n"
     ]
    }
   ],
   "source": [
    "# noise_filename = 'recordings/noises/piano5sec.wav'\n",
    "# noise2_filename = 'recordings/noises/engine.wav'\n",
    "# noise3_filename = 'recordings/noises/beeps.wav'\n",
    "\n",
    "voice1_file_names_list = ['recordings/voice1/arctic_a0007.wav']#, 'recordings/noises/piano5sec.wav']#, 'recordings/voice1/arctic_a0003.wav']\n",
    "voice2_file_names_list = ['recordings/voice2/arctic_a0001.wav']#, 'recordings/voice2/arctic_a0002.wav', 'recordings/voice1/arctic_a0003.wav']#, 'recordings/noises/piano5sec.wav']#, 'recordings/voice1/arctic_a0003.wav']\n",
    "\n",
    "voice1_file_names_list_valid = ['recordings/voice1/arctic_a0407.wav']#, 'recordings/voice1/arctic_a0003.wav']\n",
    "voice2_file_names_list_valid = ['recordings/voice2/arctic_a0032.wav']#, 'recordings/voice2/arctic_a0002.wav']\n",
    "\n",
    "# voice1_file_names_list_test1 = ['recordings/voice1/arctic_a0007.wav']#, 'recordings/voice1/arctic_a0002.wav']\n",
    "# voice2_file_names_list_test1 = ['recordings/voice2/arctic_a0001.wav']#, 'recordings/voice2/arctic_a0001.wav']\n",
    "\n",
    "# voice1_file_names_list_test2 = ['recordings/voice1/arctic_a0007.wav']#, 'recordings/voice1/arctic_a0002.wav']\n",
    "# voice2_file_names_list_test2 = [noise_filename]#, 'recordings/voice2/arctic_a0001.wav']\n",
    "\n",
    "# voice1_file_names_list_test3 = ['recordings/voice1/arctic_a0007.wav']#, 'recordings/voice1/arctic_a0002.wav']\n",
    "# voice2_file_names_list_test3 = [noise2_filename]#, 'recordings/voice2/arctic_a0001.wav']\n",
    "\n",
    "# voice1_file_names_list_test4 = ['recordings/voice1/arctic_a0007.wav']#, 'recordings/voice1/arctic_a0002.wav']\n",
    "# voice2_file_names_list_test4 = [noise3_filename]#, 'recordings/voice2/arctic_a0001.wav']\n",
    "\n",
    "\n",
    "train_set_80_1, valid_set_10_1, test_set_10_1 = create_set_from_voices_lists_80_10_10(voice1_file_names_list, voice2_file_names_list)\n",
    "train_set_80_2, valid_set_10_2, test_set_10_2 = create_set_from_voices_lists_80_10_10(voice1_file_names_list_valid, voice2_file_names_list_valid)\n",
    "\n",
    "# train_set = create_set_from_voices_lists(voice1_file_names_list, voice2_file_names_list)\n",
    "# valid_set = create_valid_set_from_voices_lists(voice1_file_names_list_valid, voice2_file_names_list_valid, noise3_filename)\n",
    "test_set1 = create_set_from_voices_lists(voice1_file_names_list, voice2_file_names_list)\n",
    "test_set2 = create_set_from_voices_lists(voice1_file_names_list_valid, voice2_file_names_list_valid)\n",
    "# test_set3 = create_set_from_voices_lists(voice1_file_names_list_test3, voice2_file_names_list_test3)\n",
    "# test_set4 = create_set_from_voices_lists(voice1_file_names_list_test4, voice2_file_names_list_test4)\n",
    "\n",
    "\n",
    "# test_set1_noise = create_test_set_from_voices_lists(voice1_file_names_list_test1, voice2_file_names_list_test1, noise_filename)\n",
    "# test_set2_noise = create_test_set_from_voices_lists(voice1_file_names_list_test2, voice2_file_names_list_test2, noise2_filename)\n",
    "# test_set3_noise = create_test_set_from_voices_lists(voice1_file_names_list_test3, voice2_file_names_list_test3, noise_filename)\n",
    "# test_set4_noise = create_test_set_from_voices_lists(voice1_file_names_list_test4, voice2_file_names_list_test4, noise2_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_network(nr_epochs, train_set, valid_set, model_name, optimizer, network):\n",
    "\n",
    "    last_valid_loss = 0\n",
    "    nb_of_epochs_for_valid_loss = 0\n",
    "\n",
    "    # plot train/valid loss contains losses on each epoch, so we can see after each epoch what happens with the error\n",
    "    plot_train_losses = []\n",
    "    plot_valid_losses = []\n",
    "    plot_train_accuracy = []\n",
    "    plot_valid_accuracy = []\n",
    "\n",
    "\n",
    "    epoch = 1\n",
    "    while epoch <= nr_epochs:\n",
    "        \n",
    "        mask = []\n",
    "        mask_test = []\n",
    "        \n",
    "        correct_train = 0\n",
    "        correct_test = 0\n",
    "\n",
    "        train_losses, valid_losses = [], []\n",
    "        loss = 0\n",
    "\n",
    "        ## training part \n",
    "        network.train()\n",
    "        for index, (input, target) in enumerate(train_set.items()):\n",
    "\n",
    "            # target is transformed from a matrix to an array, so the error can be calculated easier\n",
    "            # since the network returns an array\n",
    "\n",
    "            # if cuda is available, send (input, target) to gpu\n",
    "            if torch.cuda.is_available():\n",
    "                input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1. forward propagation\n",
    "            output = network(input)\n",
    "\n",
    "\n",
    "            # 2. loss calculation\n",
    "            loss = loss_function(target, output[len(output)-1])  \n",
    "\n",
    "\n",
    "            # 3. backward propagation\n",
    "            loss.backward() \n",
    "\n",
    "\n",
    "            # 4. weight optimization\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # 5. save the loss for this PF\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            \n",
    "            # 6. check how many items where predicted correctly\n",
    "            correct_train += 1 if loss.item() <= marja_eroare else 0 \n",
    "            \n",
    "\n",
    "        # add the mean loss for this training epoch for ploting\n",
    "        train_mean_loss_for_epoch = np.mean(train_losses)\n",
    "        plot_train_losses.append(train_mean_loss_for_epoch)\n",
    "        \n",
    "        current_accuracy = 100. * correct_train / len(train_set)\n",
    "        plot_train_accuracy.append(current_accuracy)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(\"--------------------------------------Epoch\", str(epoch) ,\"------------------------------\")\n",
    "            print(\"Train set accuracy: {}/{} ({:.0f}%)\".format(correct_train, len(train_set), current_accuracy))\n",
    "        \n",
    "        #-----------------------------------------------------------------------\n",
    "        ## evaluation part \n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "#             network.eval()\n",
    "            with torch.no_grad():\n",
    "                for index, (input, target) in enumerate(valid_set.items()):\n",
    "\n",
    "                    # if cuda is available, send (input, target) to gpu\n",
    "                    if torch.cuda.is_available():\n",
    "                         input, target = input.cuda(), target.cuda()\n",
    "\n",
    "                     # 1. forward propagation\n",
    "                    output = network.forward(input)\n",
    "\n",
    "                     # 2. loss calculation\n",
    "                    loss = loss_function(target, output[len(output)-1]).detach().item()\n",
    "\n",
    "                     # 3. check how many items where predicted correctly\n",
    "                    correct_test += 1 if loss <= marja_eroare else 0\n",
    "\n",
    "                    if loss != last_valid_loss:\n",
    "                        last_valid_loss = loss\n",
    "                    else:\n",
    "                        nb_of_epochs_for_valid_loss += 1\n",
    "\n",
    "                    valid_losses.append(loss)\n",
    "\n",
    "                 # add the mean loss for this training epoch for ploting\n",
    "                valid_mean_loss_for_epoch = np.mean(valid_losses)\n",
    "                plot_valid_losses.append(valid_mean_loss_for_epoch)\n",
    "                \n",
    "                current_accuracy = 100. * correct_test / len(valid_set)\n",
    "                plot_valid_accuracy.append(current_accuracy)\n",
    "\n",
    "                    \n",
    "#                 print (\"\\nValid loss: \", valid_mean_loss_for_epoch)\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(\"Valid set accuracy: {}/{} ({:.0f}%)\".format(correct_test, len(valid_set),current_accuracy))\n",
    "            \n",
    "        epoch += 1\n",
    "\n",
    "    torch.save(network, model_name)\n",
    "    \n",
    "    plt.plot(plot_train_losses, label='Training loss')\n",
    "    plt.plot(plot_valid_losses, label='Validation loss')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(plot_train_accuracy, label='Training accuracy')\n",
    "    plt.plot(plot_valid_accuracy, label='Validation accuracy')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECONSTRUIRE SUNET PE FRAME-URI, FOLOSIND TARGETURILE GENERATE PT SETUL DE ANTRENARE\n",
    "def split_audios_epoch(name, mask, mix):\n",
    "    \n",
    "    frame_length_ms = 10\n",
    "    total_length_ms = 5000.0\n",
    "    nb_of_samples = 80000\n",
    "    samples_per_frame = (int)(nb_of_samples * frame_length_ms / total_length_ms)\n",
    "    \n",
    "    frame_pos = (int)(nb_of_frames_in_AF/2)\n",
    "\n",
    "    mask_stack = torch.stack(mask)\n",
    "    cpu_mask = mask_stack.cpu()\n",
    "    n_mask = cpu_mask.detach().numpy()\n",
    "\n",
    "    mix_frames = np.array([mix[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    sound1 = np.empty([0,])\n",
    "    sound2 = np.empty([0,])\n",
    "\n",
    "\n",
    "    for i in range (0, len(mix_frames)):\n",
    "        stft_mix = librosa.stft(librosa.to_mono(mix_frames[i]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        # reteaua nu a invatat mastile pentru primele frame_pos bucati [PF] din mix\n",
    "        # deci las matricea STFT asa cum e in mix\n",
    "        if i < frame_pos :\n",
    "            y_frame_1_stft_with_mask = stft_mix\n",
    "            y_frame_2_stft_with_mask = stft_mix\n",
    "\n",
    "            inverse_sound1_stft = librosa.istft(y_frame_1_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "            inverse_sound2_stft = librosa.istft(y_frame_2_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "\n",
    "            sound1 = np.concatenate((sound1, inverse_sound1_stft))\n",
    "            sound2 = np.concatenate((sound2, inverse_sound2_stft))\n",
    "            \n",
    "        else:\n",
    "            # n_mask[i-7] pentru ca reteaua invata pt AF-uri facute pe cate 8 PF-uri -> 15 items pt AF\n",
    "            # abia de cand ajunge la primul AF care s a putut compune, folosesc masca invata de retea\n",
    "            \n",
    "            y_frame_1_stft_with_mask = np.multiply(n_mask[i-frame_pos], stft_mix)\n",
    "            y_frame_2_stft_with_mask = np.multiply((1 -  n_mask[i-frame_pos]), stft_mix)\n",
    "\n",
    "            inverse_sound1_stft = librosa.istft(y_frame_1_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "            inverse_sound2_stft = librosa.istft(y_frame_2_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "\n",
    "            sound1 = np.concatenate((sound1, inverse_sound1_stft))\n",
    "            sound2 = np.concatenate((sound2, inverse_sound2_stft))\n",
    "\n",
    "\n",
    "    librosa.output.write_wav(\"recordings/voice1-\"+ str(name) + \".wav\", sound1, sr = 16000)\n",
    "    librosa.output.write_wav(\"recordings/voice2-\"+ str(name) + \".wav\", sound2, sr = 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(network, test_set):\n",
    "\n",
    "#     network.eval()\n",
    "                \n",
    "    test_loss, test_mask, test_accuracy = [], [], []\n",
    "    target_height, target_width = 0, 0\n",
    "    correct_test = 0\n",
    "\n",
    "    for index, (input, target) in enumerate(test_set.items()):\n",
    "\n",
    "        # if cuda is available, send (input, target) to gpu\n",
    "        if torch.cuda.is_available():\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # 1. forward propagation\n",
    "        output = network.forward(input)\n",
    "\n",
    "        # 2. loss calculation\n",
    "        loss = loss_function(target, output[len(output)-1]).detach().item()\n",
    "\n",
    "        # 3. save the mask for the current PF, meaning the last entry in the output\n",
    "        current_mask = output[len(output)-1]#.view(target_height, target_width)\n",
    "        test_mask.append(current_mask)\n",
    "\n",
    "        test_loss.append(loss)\n",
    "        \n",
    "        # 4. check how many items where predicted correctly\n",
    "        correct_test += 1 if loss <= marja_eroare else 0\n",
    "\n",
    "\n",
    "    current_accuracy = 100. * correct_test / len(test_set)\n",
    "    print(\"Test mix loss:\", np.mean(test_loss))\n",
    "    print(\"Mix accuracy: \", (int)(current_accuracy), '%')\n",
    "    \n",
    "    plt.plot(test_loss, label='Test loss')\n",
    "    plt.plot(test_accuracy, label='Test accuracy')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.show()\n",
    "    \n",
    "    return test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAdam(nr_epochs, train_set, valid_set, test_set, mixnb):\n",
    "\n",
    "    print(\"__________________________________________ADAM FC_________________________________\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model_name = \"./200epoci-80-10-10-\" + mixnb + \".pth\"\n",
    "    network = Network()\n",
    "    \n",
    "    # if cuda is available, send network's params to gpu\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"network sent to CUDA\")\n",
    "        network.cuda()\n",
    "    \n",
    "    # set optimizer -> article : Adam, lr = 0.001, b1 = 0.9, b2 = 0.999\n",
    "    optimizer = optim.Adam(network.parameters(), lr = 0.001, betas = (0.9, 0.999))\n",
    "        \n",
    "    start_time = time.time()\n",
    "    train_network(nr_epochs=nr_epochs, \n",
    "                train_set=train_set, valid_set=valid_set, \n",
    "                model_name=model_name,\n",
    "                optimizer=optimizer,\n",
    "                network=network)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"Elapsed time for \" + model_name + \" : {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    eval(network=network, test_set=test_set)\n",
    "    \n",
    "#     test_network(model_name, is_with_noise = False)\n",
    "    return model_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAdamConv(nr_epochs, train_set, valid_set, test_set, mixnb):\n",
    "    \n",
    "    print(\"__________________________________________ADAM CONV + FC_________________________________\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model_name = \"./200epoci-80-10-10-CONV-\"+ mixnb + \".pth\"\n",
    "    \n",
    "    networkConv = NetworkConv()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"networkConv sent to CUDA\")\n",
    "        networkConv.cuda()\n",
    "        \n",
    "    # set optimizer -> article : Adam, lr = 0.001, b1 = 0.9, b2 = 0.999\n",
    "    optimizer = optim.Adam(networkConv.parameters(), lr = 0.001, betas = (0.9, 0.999))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_network(nr_epochs=nr_epochs, \n",
    "                train_set=train_set, valid_set=valid_set, \n",
    "                model_name=model_name,\n",
    "                optimizer=optimizer,\n",
    "                network=networkConv)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"Elapsed time for \" + model_name + \" : {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    eval(network=network, test_set=test_set)\n",
    "    \n",
    "#     test_network(model_name, is_with_noise = False)\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mix_from_voices(voice1_filename, voice2_filename):\n",
    "    voice1, sr = librosa.load(voice1_filename, sr=16000) \n",
    "    voice1 = np.append(voice1, voice1)\n",
    "    \n",
    "    voice2, sr = librosa.load(voice2_filename, sr=16000)\n",
    "    voice2 = np.append(voice2, voice2)\n",
    "    # pad smaller array with zeros if it's the case or delete last entries\n",
    "    voice1 = make_mix_of_10_seconds(voice1)\n",
    "    voice2 = make_mix_of_10_seconds(voice2)\n",
    "    mix = voice1 + voice2\n",
    "    \n",
    "    return mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mix_from_voices_with_noise(voice1_filename, voice2_filename, noise_filename):\n",
    "    voice1, sr = librosa.load(voice1_filename, sr=16000) \n",
    "    voice2, sr = librosa.load(voice2_filename, sr=16000) \n",
    "    noise, sr = librosa.load(noise_filename, sr=16000)\n",
    "    # pad smaller array with zeros if it's the case or delete last entries\n",
    "    voice1 = make_mix_of_10_seconds(voice1)\n",
    "    voice2 = make_mix_of_10_seconds(voice2)\n",
    "    noise = make_mix_of_10_seconds(noise)\n",
    "    noise = noise / 5\n",
    "    \n",
    "    mix = voice1 + voice2 + noise\n",
    "    \n",
    "    return mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(model_name, is_with_noise):\n",
    "    voice1 = 'recordings/voice1/arctic_a0007.wav'\n",
    "    \n",
    "    \n",
    "    voice2 = 'recordings/voice2/arctic_a0001.wav'\n",
    "#     voice3 = 'recordings/voice2/arctic_a0002.wav'\n",
    "#     voice4 = 'recordings/voice2/arctic_a0003.wav'\n",
    "#     voice5 = 'recordings/noises/piano5sec.wav'\n",
    "    \n",
    "\n",
    "\n",
    "#     piano = 'recordings/noises/piano5sec.wav'\n",
    "#     engine = 'recordings/noises/engine.wav'\n",
    "\n",
    "    mix1 = 0\n",
    "    mix2 = 0\n",
    "    mix3 = 0\n",
    "    mix4 = 0\n",
    "    if is_with_noise == True:\n",
    "        mix1 = get_mix_from_voices_with_noise(voice1, voice2, piano)\n",
    "        mix2 = get_mix_from_voices_with_noise(voice1, voice3, engine)\n",
    "        mix3 = get_mix_from_voices_with_noise(voice1, voice4, piano)\n",
    "        mix4 = get_mix_from_voices_with_noise(voice1, voice5, engine)\n",
    "        \n",
    "        network = torch.load(model_name)\n",
    "        test_mask = eval(network=network, test_set=test_set1_noise)\n",
    "        split_audios_epoch(\"-test1-noise-piano\" + str(is_with_noise), test_mask, mix1)\n",
    "    #     split_audios_epoch(model_name[2:] + \"-test1-noise\" + str(is_with_noise), test_mask, mix1)\n",
    "\n",
    "#         network = torch.load(model_name)\n",
    "#         test_mask = eval(network=network, test_set=test_set2_noise)\n",
    "#         split_audios_epoch(\"-test2-noise-engine\" + str(is_with_noise), test_mask, mix2)\n",
    "\n",
    "#         network = torch.load(model_name)\n",
    "#         test_mask = eval(network=network, test_set=test_set3_noise)\n",
    "#         split_audios_epoch(\"-test3-noise-piano\" + str(is_with_noise), test_mask, mix3)\n",
    "        \n",
    "#         network = torch.load(model_name)\n",
    "#         test_mask = eval(network=network, test_set=test_set4_noise)\n",
    "#         split_audios_epoch(\"-test4-noise-engine\" + str(is_with_noise), test_mask, mix4)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        mix1 = get_mix_from_voices(voice1, voice2)\n",
    "#         mix2 = get_mix_from_voices(voice1, voice3)\n",
    "#         mix3 = get_mix_from_voices(voice1, voice4)\n",
    "#         mix4 = get_mix_from_voices(voice1, voice5)\n",
    "        \n",
    "        network = torch.load(model_name)\n",
    "        test_mask = eval(network=network, test_set=test_set1)\n",
    "        split_audios_epoch(\"-test1-\" + str(is_with_noise), test_mask, mix1)\n",
    "    #     split_audios_epoch(model_name[2:] + \"-test1-noise\" + str(is_with_noise), test_mask, mix1)\n",
    "\n",
    "#         network = torch.load(model_name)\n",
    "#         test_mask = eval(network=network, test_set=test_set2)\n",
    "#         split_audios_epoch(\"-test2-\" + str(is_with_noise), test_mask, mix2)\n",
    "\n",
    "#         network = torch.load(model_name)\n",
    "#         test_mask = eval(network=network, test_set=test_set3)\n",
    "#         split_audios_epoch(\"-test3-\" + str(is_with_noise), test_mask, mix3)\n",
    "\n",
    "#         network = torch.load(model_name)\n",
    "#         test_mask = eval(network=network, test_set=test_set4)\n",
    "#         split_audios_epoch(\"-test4-\" + str(is_with_noise), test_mask, mix4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.38 MiB (GPU 0; 4.00 GiB total capacity; 3.01 GiB already allocated; 2.42 MiB free; 618.00 KiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-4c36bf4505f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mmix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mix_from_voices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvoice1_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoice2_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./200epoci-80-10-10-CONV-v1(7)-v2(1).pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mtest_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_set1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0msplit_audios_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"adam-CONV-v1(7)-v2(1)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-8e70348d9ae8>\u001b[0m in \u001b[0;36meval\u001b[1;34m(network, test_set)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# 1. forward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# 2. loss calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-1300e572926b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# layer 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.38 MiB (GPU 0; 4.00 GiB total capacity; 3.01 GiB already allocated; 2.42 MiB free; 618.00 KiB cached)"
     ]
    }
   ],
   "source": [
    "import winsound\n",
    "import os\n",
    "\n",
    "voice1_1 = 'recordings/voice1/arctic_a0007.wav'\n",
    "voice2_1 = 'recordings/voice2/arctic_a0001.wav'\n",
    "\n",
    "voice1_2 = 'recordings/voice1/arctic_a0407.wav'\n",
    "voice2_2 = 'recordings/voice2/arctic_a0032.wav'\n",
    "    \n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model_name = runAdam(200, train_set_80_1, valid_set_10_1, test_set_10_1, \"v1(7)-v2(1)\")\n",
    "\n",
    "# mix = get_mix_from_voices(voice1_1, voice2_1)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set1)\n",
    "# split_audios_epoch(\"adam-FC-v1(7)-v2(1)\", test_mask, mix)\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# model_name = runAdamConv(200, train_set_80_1, valid_set_10_1, test_set_10_1, \"v1(7)-v2(1)\")\n",
    "\n",
    "mix = get_mix_from_voices(voice1_1, voice2_1)\n",
    "network = torch.load(\"./200epoci-80-10-10-CONV-v1(7)-v2(1).pth\")\n",
    "test_mask = eval(network=network, test_set=test_set1)\n",
    "split_audios_epoch(\"adam-CONV-v1(7)-v2(1)\", test_mask, mix)\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model_name = runAdam(200, train_set_80_2, valid_set_10_2, test_set_10_2, \"v1(407)-v2(32)\")\n",
    "\n",
    "# mix = get_mix_from_voices(voice1_2, voice2_2)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set1)\n",
    "# split_audios_epoch(\"adam-FC-v1(407)-v2(32)\", test_mask, mix)\n",
    "\n",
    "# #-----------------------------------------------------------------------------------\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model_name = runAdamConv(200, train_set_80_2, valid_set_10_2, test_set_10_2, \"v1(407)-v2(32)\")\n",
    "\n",
    "# mix = get_mix_from_voices(voice1_2, voice2_2)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set1)\n",
    "# split_audios_epoch(\"adam-CONV-v1(407)-v2(32)\", test_mask, mix)\n",
    "\n",
    "# #-----------------------------------------------------------------------------------\n",
    "\n",
    "# duration = 1000  # milliseconds\n",
    "# freq = 440  # Hz\n",
    "# winsound.Beep(freq, duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
