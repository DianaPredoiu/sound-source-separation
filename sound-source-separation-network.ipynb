{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa, librosa.display\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import my_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "n_fft = 1024\n",
    "hop_length = 8\n",
    "sr = 16000\n",
    "\n",
    "# cate bucati PF iau in considerare pentru AF -> cu overlapping => 15 frame-uri \n",
    "# ultimul frame din AF fiind PF-ul curent\n",
    "nb_of_PFs_per_AF = 4\n",
    "nb_of_frames_in_AF = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STFT_AF_frames(AF_array, PF_size):\n",
    "    frames = []\n",
    "    half = (int)(PF_size/2)\n",
    "    frames_added = 0\n",
    "    \n",
    "    i = AF_array.shape[0]-1\n",
    "    \n",
    "    # use AF as 4 times bigger than PF's size -> PF = 10 ms => AF = 40 ms\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 1. add PF[i]\n",
    "    frame = librosa.stft(librosa.to_mono(AF_array[i]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "    \n",
    "    tensor = []\n",
    "    tensor.append(frame)        \n",
    "    frames.append(tensor)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 2. add PF[i-1, i] -> overlap\n",
    "    overlap_frame = AF_array[i-1][half:]\n",
    "    overlap_frame = np.concatenate((overlap_frame, AF_array[i][:half]))\n",
    "    \n",
    "    frame = librosa.stft(librosa.to_mono(overlap_frame), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "\n",
    "    tensor = []\n",
    "    tensor.append(frame)\n",
    "    frames.append(tensor)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 3. add PF[i - 1]\n",
    "    frame = librosa.stft(librosa.to_mono(AF_array[i - 1]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "    \n",
    "    tensor = []\n",
    "    tensor.append(frame)        \n",
    "    frames.append(tensor)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 4. add PF[i-1, i] -> overlap\n",
    "    overlap_frame = AF_array[i-1][half:]\n",
    "    overlap_frame = np.concatenate((overlap_frame, AF_array[i][:half]))\n",
    "    \n",
    "    frame = librosa.stft(librosa.to_mono(overlap_frame), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "\n",
    "    tensor = []\n",
    "    tensor.append(frame)\n",
    "    frames.append(tensor)\n",
    "                      \n",
    "#     for i in range (0, AF_array.shape[0]):\n",
    "\n",
    "#         # 1. add current frame from AF\n",
    "#         frame = librosa.stft(librosa.to_mono(AF_array[i]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "#         frame = np.abs(frame)\n",
    "        \n",
    "#         tensor = []\n",
    "#         tensor.append(frame)        \n",
    "#         frames.append(tensor)\n",
    "        \n",
    "#         # 2. add overlapped frame if the current frame is not the last one\n",
    "#         if i < AF_array.shape[0]-1:\n",
    "#             overlap_frame = AF_array[i][half:]\n",
    "#             overlap_frame = np.concatenate((overlap_frame, AF_array[i+1][:half]))\n",
    "            \n",
    "#             frame = librosa.stft(librosa.to_mono(overlap_frame), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "#             frame = np.abs(frame)\n",
    "\n",
    "#             tensor = []\n",
    "#             tensor.append(frame)\n",
    "#             frames.append(tensor)\n",
    "        \n",
    "    \n",
    "    return np.asarray(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_set_for_mix(mix, voice_1, voice_2, samples_per_frame):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    mix_frames = np.array([mix[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    voice_1_frames = np.array([voice_1[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    voice_2_frames = np.array([voice_2[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    \n",
    "    for index in range(mix_frames.shape[0]):\n",
    "        \n",
    "        if index < 2:\n",
    "            continue\n",
    "        \n",
    "        # 1. create train set input for AF with current PF and previous frames\n",
    "        AF_frames = np.array([mix_frames[i] for i in range(index - 2, index)])      \n",
    "        AF_STFT_frames = get_STFT_AF_frames(AF_array=AF_frames, PF_size= samples_per_frame)\n",
    "        inputs.append(AF_STFT_frames)\n",
    "        \n",
    "        # 2. create train set target for that AF, containing the mask for the current PF\n",
    "        stft_voice_1 = librosa.stft(librosa.to_mono(voice_1_frames[index]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        stft_voice_2 = librosa.stft(librosa.to_mono(voice_2_frames[index]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        mask = my_utils.compute_mask(stft_voice_1, stft_voice_2)\n",
    "        \n",
    "        targets.append(mask)\n",
    "    \n",
    "    # train set for only one audio\n",
    "    train_set_input = torch.from_numpy(np.array(inputs))\n",
    "    print(train_set_input.shape)\n",
    "    \n",
    "    # target contains the calculated masks for each PF from mix\n",
    "    train_set_target = torch.from_numpy(np.array(targets))\n",
    "\n",
    "    print(train_set_input.shape)\n",
    "#     assert(train_set_input.shape == (996, 4, 1, 513, 11))\n",
    "#     assert(train_set_target.shape == (996, 513, 11))\n",
    "    train_set = dict(zip(train_set_input, train_set_target))\n",
    "    return train_set, train_set_target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    height = 0\n",
    "    width = 0\n",
    "    \n",
    "    # default values for height and width are given for a processing frame of 5 ms\n",
    "    def __init__(self, height = 513, width = 21):\n",
    "        super(Network, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # layer 1\n",
    "        self.fc1 = nn.Linear(1 * height * width, 250)\n",
    "        self.fc1_batch = nn.BatchNorm1d(250)\n",
    "        \n",
    "        # layer 2\n",
    "        self.fc2 = nn.Linear(250, 250)\n",
    "        self.fc2_batch = nn.BatchNorm1d(250)\n",
    "        \n",
    "        # layer 3\n",
    "        self.fc3 = nn.Linear(250, height * width)\n",
    "        self.fc3_batch = nn.BatchNorm1d(height * width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # layer 1\n",
    "        # firstly, transform the matrix into an array for the FC\n",
    "        x = x.view(-1, self.height * self.width)        \n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc1_batch(x)\n",
    "#         print(\"1 shapes: \", x.shape)\n",
    "        \n",
    "        # layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2_batch(x)\n",
    "#         print(\"2 shapes: \", x.shape)\n",
    "        \n",
    "        # layer 3\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc3_batch(x)\n",
    "        \n",
    "#         x = x.view(nb_of_PFs_per_AF * 2 - 1, self.height, self.width) \n",
    "        x = x.view(nb_of_frames_in_AF, self.height, self.width) \n",
    "#         print(\"3 shapes: \", x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network sent to CUDA\n"
     ]
    }
   ],
   "source": [
    "# create network\n",
    "# no params => height = 513, width = 11 -> for a processing frame of 5 ms\n",
    "network = Network()\n",
    "\n",
    "# if cuda is available, send network's params to gpu\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"network sent to CUDA\")\n",
    "    network.cuda()\n",
    "    \n",
    "# set optimizer -> article : Adam, lr = 0.001, b1 = 0.9, b2 = 0.999\n",
    "optimizer = optim.Adam(network.parameters(), lr = 0.001, betas = (0.9, 0.999))\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init arrays for train/test errors\n",
    "# train_losses = []\n",
    "# train_counter = []\n",
    "\n",
    "# test_losses = []\n",
    "# test_counter = [i*len(train_set) for i in range(n_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mix total length:  80000\n",
      "mix total length (ms) :  5000.0\n",
      "nb of samples for 10 ms frame = 160 samples/array\n",
      "---------------------------------------------------------\n",
      "torch.Size([498, 4, 1, 513, 21])\n",
      "torch.Size([498, 4, 1, 513, 21])\n",
      "torch.Size([498, 4, 1, 513, 21])\n",
      "torch.Size([498, 4, 1, 513, 21])\n",
      "AF entries in train_set:  498\n"
     ]
    }
   ],
   "source": [
    "male_filename = 'recordings/voice2/arctic_a0002.wav'\n",
    "female_filename = 'recordings/voice1/arctic_a0001.wav'\n",
    "\n",
    "male, sr = librosa.load(male_filename, sr=16000) \n",
    "female, sr = librosa.load(female_filename, sr=16000) \n",
    "\n",
    "# pad smaller array with zeros, so both audio files have the same length\n",
    "female, male = my_utils.make_wav_files_same_size(female, male)\n",
    "\n",
    "# load the mixed audio \n",
    "mix = female + male\n",
    "\n",
    "male = np.array(male)\n",
    "female = np.array(female)\n",
    "mix = np.array(mix)\n",
    "\n",
    "frame_length_ms = 10\n",
    "mix_length_ms = len(mix) / sr * 1000\n",
    "samples_per_frame = (int)(len(mix) * frame_length_ms / mix_length_ms)\n",
    "\n",
    "print(\"mix total length: \",len(mix))\n",
    "print(\"mix total length (ms) : \", mix_length_ms)\n",
    "print(\"nb of samples for\", frame_length_ms,\"ms frame =\", samples_per_frame, \"samples/array\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "\n",
    "train_set, targets = get_train_set_for_mix(mix, female, male, samples_per_frame)\n",
    "\n",
    "# make test set the same as train set so I can test the overfitting\n",
    "test_set, targets = get_train_set_for_mix(mix, female, male, samples_per_frame) \n",
    "print(\"AF entries in train_set: \",len(train_set))\n",
    "\n",
    "# print(targets.shape)\n",
    "# split_audios_epoch(\"TEST-mask\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_network(nr_epochs, train_set, test_set):\n",
    "\n",
    "    mask = []\n",
    "    mask_test = []\n",
    "\n",
    "    last_valid_loss = 0\n",
    "    nb_of_epochs_for_valid_loss = 0\n",
    "\n",
    "    # plot train/valid loss contains losses on each epoch, so we can see after each epoch what happens with the error\n",
    "    plot_train_loss = []\n",
    "    plot_valid_loss = []\n",
    "    \n",
    "    \n",
    "    target_height = 0\n",
    "    target_width = 0\n",
    "\n",
    "    for epoch in range(1, nr_epochs+1):\n",
    "        mask = []\n",
    "        mask_test = []\n",
    "\n",
    "        train_loss, valid_loss = [], []\n",
    "        loss = 0\n",
    "\n",
    "        print(\"--------------------------------------Epoch\", str(epoch) ,\"------------------------------\")\n",
    "        ## training part \n",
    "        network.train()\n",
    "        for index, (input, target) in enumerate(train_set.items()):\n",
    "\n",
    "            # target is transformed from a matrix to an array, so the error can be calculated easier\n",
    "            # since the network returns an array\n",
    "            if index == 0:\n",
    "                target_height = target.shape[0]\n",
    "                target_width = target.shape[1]\n",
    "            \n",
    "#             target_view = target.view(-1, target.shape[0] * target.shape[1])\n",
    "\n",
    "            # if cuda is available, send (input, target) to gpu\n",
    "            if torch.cuda.is_available():\n",
    "                input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1. forward propagation\n",
    "            output = network(input)\n",
    "\n",
    "\n",
    "            # 2. loss calculation\n",
    "            loss = loss_function(target, output[len(output)-1])  \n",
    "\n",
    "\n",
    "            # 3. backward propagation\n",
    "            loss.backward() \n",
    "\n",
    "\n",
    "            # 4. weight optimization\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # 5. save the loss for this PF\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "            # 6. save the mask for the current PF, meaning the last entry in the output[AF]\n",
    "            current_mask = output[len(output)-1]#.view(target_height, target_width)\n",
    "            mask.append(current_mask)\n",
    "\n",
    "        # add the mean loss for this training epoch for ploting\n",
    "        plot_train_loss.append(np.mean(train_loss))\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        ## evaluation part \n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "#             network.eval()\n",
    "            with torch.no_grad():\n",
    "                for index, (input, target) in enumerate(test_set.items()):\n",
    "\n",
    "                    # if cuda is available, send (input, target) to gpu\n",
    "        #             target_view = target.view(-1, target.shape[0] * target.shape[1])\n",
    "                    if torch.cuda.is_available():\n",
    "                        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "                    # 1. forward propagation\n",
    "                    output = network.forward(input)\n",
    "\n",
    "                    # 2. loss calculation\n",
    "                    loss = loss_function(target, output[len(output)-1]).detach().item()\n",
    "\n",
    "                    # 6. save the mask for the current PF, meaning the last entry in the output\n",
    "                    current_mask = output[len(output)-1]#.view(target_height, target_width)\n",
    "                    mask_test.append(current_mask)\n",
    "\n",
    "                    if loss != last_valid_loss:\n",
    "                        last_valid_loss = loss\n",
    "                    else:\n",
    "                        nb_of_epochs_for_valid_loss += 1\n",
    "\n",
    "                    valid_loss.append(loss)\n",
    "\n",
    "                # add the mean loss for this training epoch for ploting\n",
    "                plot_valid_loss.append(np.mean(valid_loss))\n",
    "\n",
    "                # check if we have the same loss for validation set\n",
    "                if last_valid_loss == (np.mean(valid_loss)):\n",
    "                    nb_of_epochs_for_valid_loss += 1\n",
    "                    print(\"nb_of_epochs_for_valid_loss increased with one, loss:\", last_valid_loss)\n",
    "                else:\n",
    "                    last_valid_loss = np.mean(valid_loss)\n",
    "\n",
    "                \n",
    "        print (\"\\nTraining Loss: \", np.mean(train_loss), \"\\nValid Loss: \", np.mean(valid_loss))\n",
    "        if(nb_of_epochs_for_valid_loss == 20):\n",
    "            print(\"Early stopping training, same loss for 20 epochs!\")\n",
    "            break\n",
    "\n",
    "    torch.save(network, './model.pth')\n",
    "#     plt.plot(plot_train_loss)\n",
    "#     plt.title('train losses')\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.plot(plot_valid_loss)\n",
    "#     plt.title('valid losses')\n",
    "#     plt.show()\n",
    "    \n",
    "    plt.plot(plot_train_loss, label='Training loss')\n",
    "    plt.plot(plot_valid_loss, label='Validation loss')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.show()\n",
    "    \n",
    "    return mask, mask_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECONSTRUIRE SUNET PE FRAME-URI, FOLOSIND TARGETURILE GENERATE PT SETUL DE ANTRENARE\n",
    "def split_audios_epoch(name, mask):\n",
    "    frame_pos = (int)(nb_of_frames_in_AF/2)\n",
    "#     n_mask = mask.detach().numpy()\n",
    "\n",
    "\n",
    "    mask_stack = torch.stack(mask)\n",
    "#     print(mask_stack.shape)\n",
    "\n",
    "    cpu_mask = mask_stack.cpu()\n",
    "#     print(cpu_mask.shape)\n",
    "\n",
    "    n_mask = cpu_mask.detach().numpy()\n",
    "#     print(n_mask.shape)\n",
    "\n",
    "    mix_frames = np.array([mix[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    sound1 = np.empty([0,])\n",
    "    sound2 = np.empty([0,])\n",
    "\n",
    "\n",
    "    for i in range (0, len(mix_frames)):\n",
    "        stft_mix = librosa.stft(librosa.to_mono(mix_frames[i]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "\n",
    "        # reteaua nu a invatat mastile pentru primele frame_pos bucati [PF] din mix\n",
    "        # deci las matricea STFT asa cum e in mix\n",
    "        if i < frame_pos :\n",
    "            y_frame_1_stft_with_mask = stft_mix\n",
    "            y_frame_2_stft_with_mask = stft_mix\n",
    "\n",
    "            inverse_sound1_stft = librosa.istft(y_frame_1_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "            inverse_sound2_stft = librosa.istft(y_frame_2_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "\n",
    "            sound1 = np.concatenate((sound1, inverse_sound1_stft))\n",
    "            sound2 = np.concatenate((sound2, inverse_sound2_stft))\n",
    "            \n",
    "        else:\n",
    "            # n_mask[i-7] pentru ca reteaua invata pt AF-uri facute pe cate 8 PF-uri -> 15 items pt AF\n",
    "            # abia de cand ajunge la primul AF care s a putut compune, folosesc masca invata de retea\n",
    "            \n",
    "            y_frame_1_stft_with_mask = np.multiply(n_mask[i-frame_pos], stft_mix)\n",
    "            y_frame_2_stft_with_mask = np.multiply((1 -  n_mask[i-frame_pos]), stft_mix)\n",
    "\n",
    "            inverse_sound1_stft = librosa.istft(y_frame_1_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "            inverse_sound2_stft = librosa.istft(y_frame_2_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "\n",
    "            sound1 = np.concatenate((sound1, inverse_sound1_stft))\n",
    "            sound2 = np.concatenate((sound2, inverse_sound2_stft))\n",
    "\n",
    "\n",
    "    librosa.output.write_wav(\"recordings/DNN-voice1-\"+ str(name) + \".wav\", sound1, sr = 16000)\n",
    "    librosa.output.write_wav(\"recordings/DNN-voice2-\"+ str(name) + \".wav\", sound2, sr = 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(network, test_set):\n",
    "\n",
    "#     network.eval()\n",
    "                \n",
    "    valid_loss, valid_mask = [], []\n",
    "    target_height, target_width = 0, 0\n",
    "\n",
    "    for index, (input, target) in enumerate(test_set.items()):\n",
    "\n",
    "            # if cuda is available, send (input, target) to gpu\n",
    "#             target_view = target.view(-1, target.shape[0] * target.shape[1])\n",
    "        if torch.cuda.is_available():\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # 1. forward propagation\n",
    "        output = network.forward(input)\n",
    "\n",
    "        # 2. loss calculation\n",
    "        loss = loss_function(target, output[len(output)-1]).detach().item()\n",
    "\n",
    "        # 6. save the mask for the current PF, meaning the last entry in the output\n",
    "        current_mask = output[len(output)-1]#.view(target_height, target_width)\n",
    "        valid_mask.append(current_mask)\n",
    "\n",
    "        valid_loss.append(loss)\n",
    "#         print(\"Loss\", loss)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Validation set loss:\", np.mean(valid_loss))\n",
    "\n",
    "    plt.plot(valid_loss)\n",
    "    plt.title('valid losses')\n",
    "    plt.show()  \n",
    "    \n",
    "    return valid_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------Epoch 1 ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\envs\\machine-learning\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Lenovo\\Anaconda3\\envs\\machine-learning\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.36539572001580733 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 2 ------------------------------\n",
      "\n",
      "Training Loss:  0.21507250053067523 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 3 ------------------------------\n",
      "\n",
      "Training Loss:  0.16559216930684315 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 4 ------------------------------\n",
      "\n",
      "Training Loss:  0.1500386393902053 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 5 ------------------------------\n",
      "\n",
      "Training Loss:  0.14061795531895505 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 6 ------------------------------\n",
      "\n",
      "Training Loss:  0.14028936908786077 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 7 ------------------------------\n",
      "\n",
      "Training Loss:  0.13862546241606574 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 8 ------------------------------\n",
      "\n",
      "Training Loss:  0.13429312227690315 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 9 ------------------------------\n",
      "\n",
      "Training Loss:  0.1297867829915151 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 10 ------------------------------\n",
      "\n",
      "Training Loss:  0.1316822266812066 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 11 ------------------------------\n",
      "\n",
      "Training Loss:  0.13698481749294392 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 12 ------------------------------\n",
      "\n",
      "Training Loss:  0.12584556996762514 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 13 ------------------------------\n",
      "\n",
      "Training Loss:  0.123213163083307 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 14 ------------------------------\n",
      "\n",
      "Training Loss:  0.12663692716942973 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 15 ------------------------------\n",
      "\n",
      "Training Loss:  0.12282420572521816 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 16 ------------------------------\n",
      "\n",
      "Training Loss:  0.12489249314140363 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 17 ------------------------------\n",
      "\n",
      "Training Loss:  0.1242606641283356 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 18 ------------------------------\n",
      "\n",
      "Training Loss:  0.12035890190626483 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 19 ------------------------------\n",
      "\n",
      "Training Loss:  0.11477828770501905 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 20 ------------------------------\n",
      "\n",
      "Training Loss:  0.11195396277193265 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 21 ------------------------------\n",
      "\n",
      "Training Loss:  0.11737924243087869 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 22 ------------------------------\n",
      "\n",
      "Training Loss:  0.11734379702303784 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 23 ------------------------------\n",
      "\n",
      "Training Loss:  0.1148661104401192 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 24 ------------------------------\n",
      "\n",
      "Training Loss:  0.11678833169413798 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 25 ------------------------------\n",
      "\n",
      "Training Loss:  0.12066782841152215 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 26 ------------------------------\n",
      "\n",
      "Training Loss:  0.11652951274650643 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 27 ------------------------------\n",
      "\n",
      "Training Loss:  0.11451434794940563 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 28 ------------------------------\n",
      "\n",
      "Training Loss:  0.11008449058500998 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 29 ------------------------------\n",
      "\n",
      "Training Loss:  0.11612572390436528 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 30 ------------------------------\n",
      "\n",
      "Training Loss:  0.11932942739138043 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 31 ------------------------------\n",
      "\n",
      "Training Loss:  0.11891715813532891 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 32 ------------------------------\n",
      "\n",
      "Training Loss:  0.11818974027148332 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 33 ------------------------------\n",
      "\n",
      "Training Loss:  0.11146293020242429 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 34 ------------------------------\n",
      "\n",
      "Training Loss:  0.11198912251246981 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 35 ------------------------------\n",
      "\n",
      "Training Loss:  0.11681019358082588 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 36 ------------------------------\n",
      "\n",
      "Training Loss:  0.10637560794055162 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 37 ------------------------------\n",
      "\n",
      "Training Loss:  0.11771733712883718 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 38 ------------------------------\n",
      "\n",
      "Training Loss:  0.1177763173947612 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 39 ------------------------------\n",
      "\n",
      "Training Loss:  0.12067521152920632 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 40 ------------------------------\n",
      "\n",
      "Training Loss:  0.11534756606437235 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 41 ------------------------------\n",
      "\n",
      "Training Loss:  0.12213518898784516 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 42 ------------------------------\n",
      "\n",
      "Training Loss:  0.11358053438799329 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 43 ------------------------------\n",
      "\n",
      "Training Loss:  0.10646058078743069 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 44 ------------------------------\n",
      "\n",
      "Training Loss:  0.10872629333855996 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 45 ------------------------------\n",
      "\n",
      "Training Loss:  0.11289844925638783 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 46 ------------------------------\n",
      "\n",
      "Training Loss:  0.10832182333868251 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 47 ------------------------------\n",
      "\n",
      "Training Loss:  0.11537324493174932 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 48 ------------------------------\n",
      "\n",
      "Training Loss:  0.11145069527002163 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 49 ------------------------------\n",
      "\n",
      "Training Loss:  0.11180725966367018 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 50 ------------------------------\n",
      "\n",
      "Training Loss:  0.1118592805505906 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 51 ------------------------------\n",
      "\n",
      "Training Loss:  0.10511102558611088 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 52 ------------------------------\n",
      "\n",
      "Training Loss:  0.10576133043262136 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 53 ------------------------------\n",
      "\n",
      "Training Loss:  0.10329791004900413 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 54 ------------------------------\n",
      "\n",
      "Training Loss:  0.09348292481229009 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 55 ------------------------------\n",
      "\n",
      "Training Loss:  0.10121601465790267 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 56 ------------------------------\n",
      "\n",
      "Training Loss:  0.10208593800955687 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 57 ------------------------------\n",
      "\n",
      "Training Loss:  0.09945032763459447 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 58 ------------------------------\n",
      "\n",
      "Training Loss:  0.09203924735761192 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 59 ------------------------------\n",
      "\n",
      "Training Loss:  0.09130373108761766 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 60 ------------------------------\n",
      "\n",
      "Training Loss:  0.09262601202250978 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 61 ------------------------------\n",
      "\n",
      "Training Loss:  0.09532643415465816 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 62 ------------------------------\n",
      "\n",
      "Training Loss:  0.0968348861281128 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 63 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.09073808276838806 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 64 ------------------------------\n",
      "\n",
      "Training Loss:  0.09912023430289962 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 65 ------------------------------\n",
      "\n",
      "Training Loss:  0.09982654434651227 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 66 ------------------------------\n",
      "\n",
      "Training Loss:  0.09288020264198368 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 67 ------------------------------\n",
      "\n",
      "Training Loss:  0.09116957373529051 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 68 ------------------------------\n",
      "\n",
      "Training Loss:  0.08841049746609289 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 69 ------------------------------\n",
      "\n",
      "Training Loss:  0.08761457469707998 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 70 ------------------------------\n",
      "\n",
      "Training Loss:  0.08804241290276713 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 71 ------------------------------\n",
      "\n",
      "Training Loss:  0.08613782216225335 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 72 ------------------------------\n",
      "\n",
      "Training Loss:  0.08360179690747002 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 73 ------------------------------\n",
      "\n",
      "Training Loss:  0.08629160815109799 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 74 ------------------------------\n",
      "\n",
      "Training Loss:  0.08385261377737199 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 75 ------------------------------\n",
      "\n",
      "Training Loss:  0.08846041673736414 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 76 ------------------------------\n",
      "\n",
      "Training Loss:  0.08706040000676245 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 77 ------------------------------\n",
      "\n",
      "Training Loss:  0.08290911988524934 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 78 ------------------------------\n",
      "\n",
      "Training Loss:  0.0826791191316279 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 79 ------------------------------\n",
      "\n",
      "Training Loss:  0.0773889439551129 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 80 ------------------------------\n",
      "\n",
      "Training Loss:  0.08472594313662926 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 81 ------------------------------\n",
      "\n",
      "Training Loss:  0.08250582129800885 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 82 ------------------------------\n",
      "\n",
      "Training Loss:  0.0856629317056787 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 83 ------------------------------\n",
      "\n",
      "Training Loss:  0.08986514064401059 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 84 ------------------------------\n",
      "\n",
      "Training Loss:  0.08548313384845499 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 85 ------------------------------\n",
      "\n",
      "Training Loss:  0.08527271374994627 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 86 ------------------------------\n",
      "\n",
      "Training Loss:  0.08010224879258915 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 87 ------------------------------\n",
      "\n",
      "Training Loss:  0.07680639896295678 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 88 ------------------------------\n",
      "\n",
      "Training Loss:  0.0830522732022915 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 89 ------------------------------\n",
      "\n",
      "Training Loss:  0.07585595219608372 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 90 ------------------------------\n",
      "\n",
      "Training Loss:  0.07929883461175162 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 91 ------------------------------\n",
      "\n",
      "Training Loss:  0.07740234824438766 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 92 ------------------------------\n",
      "\n",
      "Training Loss:  0.0802229441717473 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 93 ------------------------------\n",
      "\n",
      "Training Loss:  0.07962014354888083 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 94 ------------------------------\n",
      "\n",
      "Training Loss:  0.08027750471026877 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 95 ------------------------------\n",
      "\n",
      "Training Loss:  0.08012344511705916 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 96 ------------------------------\n",
      "\n",
      "Training Loss:  0.07844583886360186 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 97 ------------------------------\n",
      "\n",
      "Training Loss:  0.07855672078758617 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 98 ------------------------------\n",
      "\n",
      "Training Loss:  0.07788806576807467 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 99 ------------------------------\n",
      "\n",
      "Training Loss:  0.07792201050794088 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 100 ------------------------------\n",
      "\n",
      "Training Loss:  0.07423308195011413 \n",
      "Valid Loss:  0.09974616547023686\n",
      "--------------------------------------Epoch 101 ------------------------------\n",
      "\n",
      "Training Loss:  0.07760743201435112 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 102 ------------------------------\n",
      "\n",
      "Training Loss:  0.07456956897580364 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 103 ------------------------------\n",
      "\n",
      "Training Loss:  0.07140301753773395 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 104 ------------------------------\n",
      "\n",
      "Training Loss:  0.07082671060970641 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 105 ------------------------------\n",
      "\n",
      "Training Loss:  0.07088978557385052 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 106 ------------------------------\n",
      "\n",
      "Training Loss:  0.07064957492329119 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 107 ------------------------------\n",
      "\n",
      "Training Loss:  0.07276400280728973 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 108 ------------------------------\n",
      "\n",
      "Training Loss:  0.07589733496030043 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 109 ------------------------------\n",
      "\n",
      "Training Loss:  0.07783858330587251 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 110 ------------------------------\n",
      "\n",
      "Training Loss:  0.07914295610138666 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 111 ------------------------------\n",
      "\n",
      "Training Loss:  0.07821873328274628 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 112 ------------------------------\n",
      "\n",
      "Training Loss:  0.07800415820700612 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 113 ------------------------------\n",
      "\n",
      "Training Loss:  0.07465326658440614 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 114 ------------------------------\n",
      "\n",
      "Training Loss:  0.0740308151855068 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 115 ------------------------------\n",
      "\n",
      "Training Loss:  0.07509077776226826 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 116 ------------------------------\n",
      "\n",
      "Training Loss:  0.07257253398701831 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 117 ------------------------------\n",
      "\n",
      "Training Loss:  0.07226776549924328 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 118 ------------------------------\n",
      "\n",
      "Training Loss:  0.07134535822444238 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 119 ------------------------------\n",
      "\n",
      "Training Loss:  0.07128084831584422 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 120 ------------------------------\n",
      "\n",
      "Training Loss:  0.0771835609050477 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 121 ------------------------------\n",
      "\n",
      "Training Loss:  0.07145665330848218 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 122 ------------------------------\n",
      "\n",
      "Training Loss:  0.07046116933127973 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 123 ------------------------------\n",
      "\n",
      "Training Loss:  0.07467309488666034 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 124 ------------------------------\n",
      "\n",
      "Training Loss:  0.07383902393838027 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 125 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.07366870045205608 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 126 ------------------------------\n",
      "\n",
      "Training Loss:  0.07043719779577043 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 127 ------------------------------\n",
      "\n",
      "Training Loss:  0.06968065720403652 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 128 ------------------------------\n",
      "\n",
      "Training Loss:  0.06592690776222984 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 129 ------------------------------\n",
      "\n",
      "Training Loss:  0.06627932462023853 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 130 ------------------------------\n",
      "\n",
      "Training Loss:  0.06781287573238912 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 131 ------------------------------\n",
      "\n",
      "Training Loss:  0.06932060464986896 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 132 ------------------------------\n",
      "\n",
      "Training Loss:  0.07002549578628459 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 133 ------------------------------\n",
      "\n",
      "Training Loss:  0.06770655279391956 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 134 ------------------------------\n",
      "\n",
      "Training Loss:  0.06989696800494027 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 135 ------------------------------\n",
      "\n",
      "Training Loss:  0.06899088966525435 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 136 ------------------------------\n",
      "\n",
      "Training Loss:  0.06700142053414969 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 137 ------------------------------\n",
      "\n",
      "Training Loss:  0.07043554722282812 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 138 ------------------------------\n",
      "\n",
      "Training Loss:  0.06500779909046137 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 139 ------------------------------\n",
      "\n",
      "Training Loss:  0.06393102052644248 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 140 ------------------------------\n",
      "\n",
      "Training Loss:  0.06657064730367698 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 141 ------------------------------\n",
      "\n",
      "Training Loss:  0.0734719309202445 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 142 ------------------------------\n",
      "\n",
      "Training Loss:  0.06965475166336078 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 143 ------------------------------\n",
      "\n",
      "Training Loss:  0.06992315492949551 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 144 ------------------------------\n",
      "\n",
      "Training Loss:  0.07086088118506972 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 145 ------------------------------\n",
      "\n",
      "Training Loss:  0.07110127028237956 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 146 ------------------------------\n",
      "\n",
      "Training Loss:  0.07322440007252466 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 147 ------------------------------\n",
      "\n",
      "Training Loss:  0.07296542968627448 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 148 ------------------------------\n",
      "\n",
      "Training Loss:  0.07015915478096069 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 149 ------------------------------\n",
      "\n",
      "Training Loss:  0.06840901866704978 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 150 ------------------------------\n",
      "\n",
      "Training Loss:  0.06943109391079315 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 151 ------------------------------\n",
      "\n",
      "Training Loss:  0.06646288548725454 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 152 ------------------------------\n",
      "\n",
      "Training Loss:  0.0674144192092073 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 153 ------------------------------\n",
      "\n",
      "Training Loss:  0.06880873454030413 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 154 ------------------------------\n",
      "\n",
      "Training Loss:  0.0709090655247243 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 155 ------------------------------\n",
      "\n",
      "Training Loss:  0.06751978060229927 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 156 ------------------------------\n",
      "\n",
      "Training Loss:  0.06568205124864678 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 157 ------------------------------\n",
      "\n",
      "Training Loss:  0.07058095767895639 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 158 ------------------------------\n",
      "\n",
      "Training Loss:  0.06443835931273464 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 159 ------------------------------\n",
      "\n",
      "Training Loss:  0.06680295538647708 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 160 ------------------------------\n",
      "\n",
      "Training Loss:  0.06861494975393806 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 161 ------------------------------\n",
      "\n",
      "Training Loss:  0.0695846000260144 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 162 ------------------------------\n",
      "\n",
      "Training Loss:  0.06949037498415178 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 163 ------------------------------\n",
      "\n",
      "Training Loss:  0.0671659383600604 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 164 ------------------------------\n",
      "\n",
      "Training Loss:  0.06587453161689363 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 165 ------------------------------\n",
      "\n",
      "Training Loss:  0.06579235905138124 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 166 ------------------------------\n",
      "\n",
      "Training Loss:  0.06342484673251715 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 167 ------------------------------\n",
      "\n",
      "Training Loss:  0.062313495842681366 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 168 ------------------------------\n",
      "\n",
      "Training Loss:  0.06504547669065362 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 169 ------------------------------\n",
      "\n",
      "Training Loss:  0.06121228049137043 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 170 ------------------------------\n",
      "\n",
      "Training Loss:  0.06204066899923751 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 171 ------------------------------\n",
      "\n",
      "Training Loss:  0.06430595004227019 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 172 ------------------------------\n",
      "\n",
      "Training Loss:  0.06207865663293022 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 173 ------------------------------\n",
      "\n",
      "Training Loss:  0.06161369782113841 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 174 ------------------------------\n",
      "\n",
      "Training Loss:  0.062499894420489366 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 175 ------------------------------\n",
      "\n",
      "Training Loss:  0.06559618615440387 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 176 ------------------------------\n",
      "\n",
      "Training Loss:  0.06585249811336595 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 177 ------------------------------\n",
      "\n",
      "Training Loss:  0.06851934294883108 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 178 ------------------------------\n",
      "\n",
      "Training Loss:  0.0722509364470141 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 179 ------------------------------\n",
      "\n",
      "Training Loss:  0.07747906661266044 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 180 ------------------------------\n",
      "\n",
      "Training Loss:  0.08025596825735332 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 181 ------------------------------\n",
      "\n",
      "Training Loss:  0.0715132586290923 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 182 ------------------------------\n",
      "\n",
      "Training Loss:  0.07500876539147447 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 183 ------------------------------\n",
      "\n",
      "Training Loss:  0.07059641464162546 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 184 ------------------------------\n",
      "\n",
      "Training Loss:  0.0677624520797519 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 185 ------------------------------\n",
      "\n",
      "Training Loss:  0.06997193301313165 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 186 ------------------------------\n",
      "\n",
      "Training Loss:  0.0724844309431527 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 187 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.07140499168066942 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 188 ------------------------------\n",
      "\n",
      "Training Loss:  0.06916803047379216 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 189 ------------------------------\n",
      "\n",
      "Training Loss:  0.06978139157391675 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 190 ------------------------------\n",
      "\n",
      "Training Loss:  0.06510879728955798 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 191 ------------------------------\n",
      "\n",
      "Training Loss:  0.06433016111094435 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 192 ------------------------------\n",
      "\n",
      "Training Loss:  0.061808839026209726 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 193 ------------------------------\n",
      "\n",
      "Training Loss:  0.06019570242020987 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 194 ------------------------------\n",
      "\n",
      "Training Loss:  0.06331391214470784 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 195 ------------------------------\n",
      "\n",
      "Training Loss:  0.06358530203914849 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 196 ------------------------------\n",
      "\n",
      "Training Loss:  0.06339672225319655 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 197 ------------------------------\n",
      "\n",
      "Training Loss:  0.06297305670226794 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 198 ------------------------------\n",
      "\n",
      "Training Loss:  0.05907960038369577 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 199 ------------------------------\n",
      "\n",
      "Training Loss:  0.0593890456113484 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 200 ------------------------------\n",
      "\n",
      "Training Loss:  0.0604998462980963 \n",
      "Valid Loss:  0.05960099218395862\n",
      "--------------------------------------Epoch 201 ------------------------------\n",
      "\n",
      "Training Loss:  0.05703470475532687 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 202 ------------------------------\n",
      "\n",
      "Training Loss:  0.06297402657467037 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 203 ------------------------------\n",
      "\n",
      "Training Loss:  0.06642079905192902 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 204 ------------------------------\n",
      "\n",
      "Training Loss:  0.06200373401786117 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 205 ------------------------------\n",
      "\n",
      "Training Loss:  0.059956431276220086 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 206 ------------------------------\n",
      "\n",
      "Training Loss:  0.05914218978884298 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 207 ------------------------------\n",
      "\n",
      "Training Loss:  0.05473686803007735 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 208 ------------------------------\n",
      "\n",
      "Training Loss:  0.0551453519736673 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 209 ------------------------------\n",
      "\n",
      "Training Loss:  0.05727555997641119 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 210 ------------------------------\n",
      "\n",
      "Training Loss:  0.05681773083904546 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 211 ------------------------------\n",
      "\n",
      "Training Loss:  0.05881453098625782 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 212 ------------------------------\n",
      "\n",
      "Training Loss:  0.05881436096297879 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 213 ------------------------------\n",
      "\n",
      "Training Loss:  0.05735900561185153 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 214 ------------------------------\n",
      "\n",
      "Training Loss:  0.062095200309157 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 215 ------------------------------\n",
      "\n",
      "Training Loss:  0.060837133508021474 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 216 ------------------------------\n",
      "\n",
      "Training Loss:  0.0638689677479357 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 217 ------------------------------\n",
      "\n",
      "Training Loss:  0.060327315567859384 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 218 ------------------------------\n",
      "\n",
      "Training Loss:  0.061239694371374845 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 219 ------------------------------\n",
      "\n",
      "Training Loss:  0.05627398776771744 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 220 ------------------------------\n",
      "\n",
      "Training Loss:  0.05562416081883965 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 221 ------------------------------\n",
      "\n",
      "Training Loss:  0.05660699382038166 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 222 ------------------------------\n",
      "\n",
      "Training Loss:  0.05355039164430992 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 223 ------------------------------\n",
      "\n",
      "Training Loss:  0.0533962928451241 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 224 ------------------------------\n",
      "\n",
      "Training Loss:  0.05328163547927994 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 225 ------------------------------\n",
      "\n",
      "Training Loss:  0.05552924365817588 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 226 ------------------------------\n",
      "\n",
      "Training Loss:  0.05849466677351068 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 227 ------------------------------\n",
      "\n",
      "Training Loss:  0.06149389556150114 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 228 ------------------------------\n",
      "\n",
      "Training Loss:  0.05533380537339925 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 229 ------------------------------\n",
      "\n",
      "Training Loss:  0.05407735875172488 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 230 ------------------------------\n",
      "\n",
      "Training Loss:  0.0569659791154393 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 231 ------------------------------\n",
      "\n",
      "Training Loss:  0.05464287953508022 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 232 ------------------------------\n",
      "\n",
      "Training Loss:  0.05475725800757741 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 233 ------------------------------\n",
      "\n",
      "Training Loss:  0.0560889279129315 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 234 ------------------------------\n",
      "\n",
      "Training Loss:  0.05970572011398349 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 235 ------------------------------\n",
      "\n",
      "Training Loss:  0.05895695240785489 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 236 ------------------------------\n",
      "\n",
      "Training Loss:  0.059974437088224745 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 237 ------------------------------\n",
      "\n",
      "Training Loss:  0.05684712132404595 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 238 ------------------------------\n",
      "\n",
      "Training Loss:  0.05646322880661373 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 239 ------------------------------\n",
      "\n",
      "Training Loss:  0.05487881469995104 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 240 ------------------------------\n",
      "\n",
      "Training Loss:  0.056472112328055185 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 241 ------------------------------\n",
      "\n",
      "Training Loss:  0.05818211469692274 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 242 ------------------------------\n",
      "\n",
      "Training Loss:  0.05882443291250503 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 243 ------------------------------\n",
      "\n",
      "Training Loss:  0.05654171001000701 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 244 ------------------------------\n",
      "\n",
      "Training Loss:  0.055284337521060656 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 245 ------------------------------\n",
      "\n",
      "Training Loss:  0.055969885711556894 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 246 ------------------------------\n",
      "\n",
      "Training Loss:  0.055167902317973576 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 247 ------------------------------\n",
      "\n",
      "Training Loss:  0.05455825579338673 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 248 ------------------------------\n",
      "\n",
      "Training Loss:  0.0587426961817482 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 249 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.05945669891909244 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 250 ------------------------------\n",
      "\n",
      "Training Loss:  0.055674703476734616 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 251 ------------------------------\n",
      "\n",
      "Training Loss:  0.052609042752324406 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 252 ------------------------------\n",
      "\n",
      "Training Loss:  0.052512195150395756 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 253 ------------------------------\n",
      "\n",
      "Training Loss:  0.052555678412194044 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 254 ------------------------------\n",
      "\n",
      "Training Loss:  0.05408506151950378 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 255 ------------------------------\n",
      "\n",
      "Training Loss:  0.05704600363223214 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 256 ------------------------------\n",
      "\n",
      "Training Loss:  0.05664833718176223 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 257 ------------------------------\n",
      "\n",
      "Training Loss:  0.05363203814442064 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 258 ------------------------------\n",
      "\n",
      "Training Loss:  0.05382368600885639 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 259 ------------------------------\n",
      "\n",
      "Training Loss:  0.05437207224653354 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 260 ------------------------------\n",
      "\n",
      "Training Loss:  0.05480891126033377 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 261 ------------------------------\n",
      "\n",
      "Training Loss:  0.05365620698977953 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 262 ------------------------------\n",
      "\n",
      "Training Loss:  0.054449096419612986 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 263 ------------------------------\n",
      "\n",
      "Training Loss:  0.061807718610775206 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 264 ------------------------------\n",
      "\n",
      "Training Loss:  0.05999615712231703 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 265 ------------------------------\n",
      "\n",
      "Training Loss:  0.05507217147964637 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 266 ------------------------------\n",
      "\n",
      "Training Loss:  0.05358827533144681 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 267 ------------------------------\n",
      "\n",
      "Training Loss:  0.05233545627356579 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 268 ------------------------------\n",
      "\n",
      "Training Loss:  0.05592282998715388 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 269 ------------------------------\n",
      "\n",
      "Training Loss:  0.05619041553307281 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 270 ------------------------------\n",
      "\n",
      "Training Loss:  0.05402510732589386 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 271 ------------------------------\n",
      "\n",
      "Training Loss:  0.05064978468202749 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 272 ------------------------------\n",
      "\n",
      "Training Loss:  0.05127826802778514 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 273 ------------------------------\n",
      "\n",
      "Training Loss:  0.05335187237106258 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 274 ------------------------------\n",
      "\n",
      "Training Loss:  0.055907292782997074 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 275 ------------------------------\n",
      "\n",
      "Training Loss:  0.054906272875198274 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 276 ------------------------------\n",
      "\n",
      "Training Loss:  0.05000625056665747 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 277 ------------------------------\n",
      "\n",
      "Training Loss:  0.04829256227494954 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 278 ------------------------------\n",
      "\n",
      "Training Loss:  0.049681103748464676 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 279 ------------------------------\n",
      "\n",
      "Training Loss:  0.047752722391384984 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 280 ------------------------------\n",
      "\n",
      "Training Loss:  0.0492977164647794 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 281 ------------------------------\n",
      "\n",
      "Training Loss:  0.05199356892571412 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 282 ------------------------------\n",
      "\n",
      "Training Loss:  0.04821060625959784 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 283 ------------------------------\n",
      "\n",
      "Training Loss:  0.04874057152444064 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 284 ------------------------------\n",
      "\n",
      "Training Loss:  0.05192478806202234 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 285 ------------------------------\n",
      "\n",
      "Training Loss:  0.05104921858609227 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 286 ------------------------------\n",
      "\n",
      "Training Loss:  0.05351898399785167 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 287 ------------------------------\n",
      "\n",
      "Training Loss:  0.052562538065003134 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 288 ------------------------------\n",
      "\n",
      "Training Loss:  0.05085563824632593 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 289 ------------------------------\n",
      "\n",
      "Training Loss:  0.050239298249307174 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 290 ------------------------------\n",
      "\n",
      "Training Loss:  0.04829438188525671 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 291 ------------------------------\n",
      "\n",
      "Training Loss:  0.05209277496391019 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 292 ------------------------------\n",
      "\n",
      "Training Loss:  0.05223513369906616 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 293 ------------------------------\n",
      "\n",
      "Training Loss:  0.05155268048933173 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 294 ------------------------------\n",
      "\n",
      "Training Loss:  0.05156398893446917 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 295 ------------------------------\n",
      "\n",
      "Training Loss:  0.04976976343490729 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 296 ------------------------------\n",
      "\n",
      "Training Loss:  0.05030588134984893 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 297 ------------------------------\n",
      "\n",
      "Training Loss:  0.04974090985272954 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 298 ------------------------------\n",
      "\n",
      "Training Loss:  0.05374391227975142 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 299 ------------------------------\n",
      "\n",
      "Training Loss:  0.051522254700128646 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 300 ------------------------------\n",
      "\n",
      "Training Loss:  0.04992501637321861 \n",
      "Valid Loss:  0.04801454496626423\n",
      "--------------------------------------Epoch 301 ------------------------------\n",
      "\n",
      "Training Loss:  0.048571255452263144 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 302 ------------------------------\n",
      "\n",
      "Training Loss:  0.0473875456079877 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 303 ------------------------------\n",
      "\n",
      "Training Loss:  0.04713321102204704 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 304 ------------------------------\n",
      "\n",
      "Training Loss:  0.048366704838668104 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 305 ------------------------------\n",
      "\n",
      "Training Loss:  0.0486172568947928 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 306 ------------------------------\n",
      "\n",
      "Training Loss:  0.04669401276411067 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 307 ------------------------------\n",
      "\n",
      "Training Loss:  0.04681134704454447 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 308 ------------------------------\n",
      "\n",
      "Training Loss:  0.046331213026918935 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 309 ------------------------------\n",
      "\n",
      "Training Loss:  0.0486051339736309 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 310 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.0506495823686759 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 311 ------------------------------\n",
      "\n",
      "Training Loss:  0.049724713761902586 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 312 ------------------------------\n",
      "\n",
      "Training Loss:  0.048828792009245885 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 313 ------------------------------\n",
      "\n",
      "Training Loss:  0.05225077387633729 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 314 ------------------------------\n",
      "\n",
      "Training Loss:  0.05059695000808968 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 315 ------------------------------\n",
      "\n",
      "Training Loss:  0.046560696547338555 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 316 ------------------------------\n",
      "\n",
      "Training Loss:  0.05376989686976822 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 317 ------------------------------\n",
      "\n",
      "Training Loss:  0.05326976281064772 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 318 ------------------------------\n",
      "\n",
      "Training Loss:  0.05113979249316722 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 319 ------------------------------\n",
      "\n",
      "Training Loss:  0.05527236526290196 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 320 ------------------------------\n",
      "\n",
      "Training Loss:  0.05387221631553071 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 321 ------------------------------\n",
      "\n",
      "Training Loss:  0.052608866071346 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 322 ------------------------------\n",
      "\n",
      "Training Loss:  0.04981912487065496 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 323 ------------------------------\n",
      "\n",
      "Training Loss:  0.05227405519612747 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 324 ------------------------------\n",
      "\n",
      "Training Loss:  0.050834338507212966 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 325 ------------------------------\n",
      "\n",
      "Training Loss:  0.05302293778935834 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 326 ------------------------------\n",
      "\n",
      "Training Loss:  0.0510698970176024 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 327 ------------------------------\n",
      "\n",
      "Training Loss:  0.05191825618502312 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 328 ------------------------------\n",
      "\n",
      "Training Loss:  0.054759391041349394 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 329 ------------------------------\n",
      "\n",
      "Training Loss:  0.050396233822793984 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 330 ------------------------------\n",
      "\n",
      "Training Loss:  0.04897482229092981 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 331 ------------------------------\n",
      "\n",
      "Training Loss:  0.048773107355365906 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 332 ------------------------------\n",
      "\n",
      "Training Loss:  0.04992865555282078 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 333 ------------------------------\n",
      "\n",
      "Training Loss:  0.05147023751711444 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 334 ------------------------------\n",
      "\n",
      "Training Loss:  0.050226126316264996 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 335 ------------------------------\n",
      "\n",
      "Training Loss:  0.05013105760982292 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 336 ------------------------------\n",
      "\n",
      "Training Loss:  0.052374777366409206 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 337 ------------------------------\n",
      "\n",
      "Training Loss:  0.052015159327591605 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 338 ------------------------------\n",
      "\n",
      "Training Loss:  0.05462773419709177 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 339 ------------------------------\n",
      "\n",
      "Training Loss:  0.05158740646690483 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 340 ------------------------------\n",
      "\n",
      "Training Loss:  0.04971544139030961 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 341 ------------------------------\n",
      "\n",
      "Training Loss:  0.04793898572328741 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 342 ------------------------------\n",
      "\n",
      "Training Loss:  0.047914522620613575 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 343 ------------------------------\n",
      "\n",
      "Training Loss:  0.0484744075264932 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 344 ------------------------------\n",
      "\n",
      "Training Loss:  0.04911094326737119 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 345 ------------------------------\n",
      "\n",
      "Training Loss:  0.04987889352376983 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 346 ------------------------------\n",
      "\n",
      "Training Loss:  0.048695268935242814 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 347 ------------------------------\n",
      "\n",
      "Training Loss:  0.04912171114006836 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 348 ------------------------------\n",
      "\n",
      "Training Loss:  0.046919542501727224 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 349 ------------------------------\n",
      "\n",
      "Training Loss:  0.04594265694006398 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 350 ------------------------------\n",
      "\n",
      "Training Loss:  0.04695155732010296 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 351 ------------------------------\n",
      "\n",
      "Training Loss:  0.047540482012528994 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 352 ------------------------------\n",
      "\n",
      "Training Loss:  0.04680924535010074 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 353 ------------------------------\n",
      "\n",
      "Training Loss:  0.04821041899080535 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 354 ------------------------------\n",
      "\n",
      "Training Loss:  0.04708527644660111 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 355 ------------------------------\n",
      "\n",
      "Training Loss:  0.046246212361550985 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 356 ------------------------------\n",
      "\n",
      "Training Loss:  0.046171121447513536 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 357 ------------------------------\n",
      "\n",
      "Training Loss:  0.050805814289609966 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 358 ------------------------------\n",
      "\n",
      "Training Loss:  0.050783401473104076 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 359 ------------------------------\n",
      "\n",
      "Training Loss:  0.051093711210066235 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 360 ------------------------------\n",
      "\n",
      "Training Loss:  0.04679364981644817 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 361 ------------------------------\n",
      "\n",
      "Training Loss:  0.044284687860012176 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 362 ------------------------------\n",
      "\n",
      "Training Loss:  0.04574065805846899 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 363 ------------------------------\n",
      "\n",
      "Training Loss:  0.04646780057157091 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 364 ------------------------------\n",
      "\n",
      "Training Loss:  0.04887720319665237 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 365 ------------------------------\n",
      "\n",
      "Training Loss:  0.04646138810224979 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 366 ------------------------------\n",
      "\n",
      "Training Loss:  0.04395258401540099 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 367 ------------------------------\n",
      "\n",
      "Training Loss:  0.04696199297062864 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 368 ------------------------------\n",
      "\n",
      "Training Loss:  0.04599469574474868 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 369 ------------------------------\n",
      "\n",
      "Training Loss:  0.04978457577390255 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 370 ------------------------------\n",
      "\n",
      "Training Loss:  0.048048289361118976 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 371 ------------------------------\n",
      "\n",
      "Training Loss:  0.045531698529231224 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 372 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.04536193935722162 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 373 ------------------------------\n",
      "\n",
      "Training Loss:  0.04516731998815414 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 374 ------------------------------\n",
      "\n",
      "Training Loss:  0.050796889110148936 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 375 ------------------------------\n",
      "\n",
      "Training Loss:  0.05333191714062451 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 376 ------------------------------\n",
      "\n",
      "Training Loss:  0.04950691804718 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 377 ------------------------------\n",
      "\n",
      "Training Loss:  0.046017467147670375 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 378 ------------------------------\n",
      "\n",
      "Training Loss:  0.04644096138031181 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 379 ------------------------------\n",
      "\n",
      "Training Loss:  0.04377303669726903 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 380 ------------------------------\n",
      "\n",
      "Training Loss:  0.044161494836700226 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 381 ------------------------------\n",
      "\n",
      "Training Loss:  0.041887530565896766 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 382 ------------------------------\n",
      "\n",
      "Training Loss:  0.04075835803977695 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 383 ------------------------------\n",
      "\n",
      "Training Loss:  0.04720610427144478 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 384 ------------------------------\n",
      "\n",
      "Training Loss:  0.04422846055045048 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 385 ------------------------------\n",
      "\n",
      "Training Loss:  0.04209223388342943 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 386 ------------------------------\n",
      "\n",
      "Training Loss:  0.04342864331521785 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 387 ------------------------------\n",
      "\n",
      "Training Loss:  0.04340362342398396 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 388 ------------------------------\n",
      "\n",
      "Training Loss:  0.04888782257877488 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 389 ------------------------------\n",
      "\n",
      "Training Loss:  0.04859177283561625 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 390 ------------------------------\n",
      "\n",
      "Training Loss:  0.04750912389934375 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 391 ------------------------------\n",
      "\n",
      "Training Loss:  0.04714960437897104 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 392 ------------------------------\n",
      "\n",
      "Training Loss:  0.0474606162894348 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 393 ------------------------------\n",
      "\n",
      "Training Loss:  0.04700028772510293 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 394 ------------------------------\n",
      "\n",
      "Training Loss:  0.048609342977719475 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 395 ------------------------------\n",
      "\n",
      "Training Loss:  0.046819994941476824 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 396 ------------------------------\n",
      "\n",
      "Training Loss:  0.04593976723038351 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 397 ------------------------------\n",
      "\n",
      "Training Loss:  0.04548986263902239 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 398 ------------------------------\n",
      "\n",
      "Training Loss:  0.04714824150945729 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 399 ------------------------------\n",
      "\n",
      "Training Loss:  0.05040576445895041 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 400 ------------------------------\n",
      "\n",
      "Training Loss:  0.051215766075693614 \n",
      "Valid Loss:  0.055335939767042056\n",
      "--------------------------------------Epoch 401 ------------------------------\n",
      "\n",
      "Training Loss:  0.052992872570707714 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 402 ------------------------------\n",
      "\n",
      "Training Loss:  0.05232084736147877 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 403 ------------------------------\n",
      "\n",
      "Training Loss:  0.04926114426674791 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 404 ------------------------------\n",
      "\n",
      "Training Loss:  0.050541805551504414 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 405 ------------------------------\n",
      "\n",
      "Training Loss:  0.04847085206209972 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 406 ------------------------------\n",
      "\n",
      "Training Loss:  0.048922931737493 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 407 ------------------------------\n",
      "\n",
      "Training Loss:  0.048951978652844244 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 408 ------------------------------\n",
      "\n",
      "Training Loss:  0.051111744278797965 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 409 ------------------------------\n",
      "\n",
      "Training Loss:  0.04812568706443035 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 410 ------------------------------\n",
      "\n",
      "Training Loss:  0.04654292538306487 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 411 ------------------------------\n",
      "\n",
      "Training Loss:  0.05173071553432296 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 412 ------------------------------\n",
      "\n",
      "Training Loss:  0.05103153955473222 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 413 ------------------------------\n",
      "\n",
      "Training Loss:  0.04944976957973545 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 414 ------------------------------\n",
      "\n",
      "Training Loss:  0.04734384193663087 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 415 ------------------------------\n",
      "\n",
      "Training Loss:  0.04523957669562425 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 416 ------------------------------\n",
      "\n",
      "Training Loss:  0.04490923344337716 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 417 ------------------------------\n",
      "\n",
      "Training Loss:  0.0487565902951733 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 418 ------------------------------\n",
      "\n",
      "Training Loss:  0.046841772622699125 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 419 ------------------------------\n",
      "\n",
      "Training Loss:  0.04846466833068423 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 420 ------------------------------\n",
      "\n",
      "Training Loss:  0.04687689878191137 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 421 ------------------------------\n",
      "\n",
      "Training Loss:  0.04508518183546216 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 422 ------------------------------\n",
      "\n",
      "Training Loss:  0.045149530252216886 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 423 ------------------------------\n",
      "\n",
      "Training Loss:  0.045883879026483866 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 424 ------------------------------\n",
      "\n",
      "Training Loss:  0.04590992096005852 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 425 ------------------------------\n",
      "\n",
      "Training Loss:  0.047147683932541574 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 426 ------------------------------\n",
      "\n",
      "Training Loss:  0.0474872843921564 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 427 ------------------------------\n",
      "\n",
      "Training Loss:  0.04810020112379051 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 428 ------------------------------\n",
      "\n",
      "Training Loss:  0.04851466641366642 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 429 ------------------------------\n",
      "\n",
      "Training Loss:  0.0483112192900533 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 430 ------------------------------\n",
      "\n",
      "Training Loss:  0.04543554517503268 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 431 ------------------------------\n",
      "\n",
      "Training Loss:  0.04428219561518507 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 432 ------------------------------\n",
      "\n",
      "Training Loss:  0.044372721188168554 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 433 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.04301184707110334 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 434 ------------------------------\n",
      "\n",
      "Training Loss:  0.04178503005879653 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 435 ------------------------------\n",
      "\n",
      "Training Loss:  0.0433241531171054 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 436 ------------------------------\n",
      "\n",
      "Training Loss:  0.04501135151261783 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 437 ------------------------------\n",
      "\n",
      "Training Loss:  0.045068848857382986 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 438 ------------------------------\n",
      "\n",
      "Training Loss:  0.04350742501823019 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 439 ------------------------------\n",
      "\n",
      "Training Loss:  0.04375315372620556 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 440 ------------------------------\n",
      "\n",
      "Training Loss:  0.043170049831810484 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 441 ------------------------------\n",
      "\n",
      "Training Loss:  0.04293903236974159 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 442 ------------------------------\n",
      "\n",
      "Training Loss:  0.044887415550519505 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 443 ------------------------------\n",
      "\n",
      "Training Loss:  0.04570559552898995 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 444 ------------------------------\n",
      "\n",
      "Training Loss:  0.04601874568121242 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 445 ------------------------------\n",
      "\n",
      "Training Loss:  0.042681212592799465 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 446 ------------------------------\n",
      "\n",
      "Training Loss:  0.04403667253693967 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 447 ------------------------------\n",
      "\n",
      "Training Loss:  0.04194834509084501 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 448 ------------------------------\n",
      "\n",
      "Training Loss:  0.04212901520058237 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 449 ------------------------------\n",
      "\n",
      "Training Loss:  0.04192911419404463 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 450 ------------------------------\n",
      "\n",
      "Training Loss:  0.04495254851336398 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 451 ------------------------------\n",
      "\n",
      "Training Loss:  0.04461727661948433 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 452 ------------------------------\n",
      "\n",
      "Training Loss:  0.042565341942065286 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 453 ------------------------------\n",
      "\n",
      "Training Loss:  0.04350269374256743 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 454 ------------------------------\n",
      "\n",
      "Training Loss:  0.047467154614864684 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 455 ------------------------------\n",
      "\n",
      "Training Loss:  0.048001913031502697 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 456 ------------------------------\n",
      "\n",
      "Training Loss:  0.048856629260862094 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 457 ------------------------------\n",
      "\n",
      "Training Loss:  0.04894107541273356 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 458 ------------------------------\n",
      "\n",
      "Training Loss:  0.05067649502079547 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 459 ------------------------------\n",
      "\n",
      "Training Loss:  0.04690049389721909 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 460 ------------------------------\n",
      "\n",
      "Training Loss:  0.047189758201550074 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 461 ------------------------------\n",
      "\n",
      "Training Loss:  0.049757804232795244 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 462 ------------------------------\n",
      "\n",
      "Training Loss:  0.04888228726440502 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 463 ------------------------------\n",
      "\n",
      "Training Loss:  0.049114707926826186 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 464 ------------------------------\n",
      "\n",
      "Training Loss:  0.04853897643030255 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 465 ------------------------------\n",
      "\n",
      "Training Loss:  0.04724448802315705 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 466 ------------------------------\n",
      "\n",
      "Training Loss:  0.04891360731358055 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 467 ------------------------------\n",
      "\n",
      "Training Loss:  0.05015322905314112 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 468 ------------------------------\n",
      "\n",
      "Training Loss:  0.04872166738359701 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 469 ------------------------------\n",
      "\n",
      "Training Loss:  0.05052786302736517 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 470 ------------------------------\n",
      "\n",
      "Training Loss:  0.047600233435863934 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 471 ------------------------------\n",
      "\n",
      "Training Loss:  0.04761357463815651 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 472 ------------------------------\n",
      "\n",
      "Training Loss:  0.04765214203833633 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 473 ------------------------------\n",
      "\n",
      "Training Loss:  0.04745480565403418 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 474 ------------------------------\n",
      "\n",
      "Training Loss:  0.04640457241523116 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 475 ------------------------------\n",
      "\n",
      "Training Loss:  0.048964201522572096 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 476 ------------------------------\n",
      "\n",
      "Training Loss:  0.04762177378902142 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 477 ------------------------------\n",
      "\n",
      "Training Loss:  0.04398076769409334 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 478 ------------------------------\n",
      "\n",
      "Training Loss:  0.04826689116766628 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 479 ------------------------------\n",
      "\n",
      "Training Loss:  0.04830631779579021 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 480 ------------------------------\n",
      "\n",
      "Training Loss:  0.048141177736945255 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 481 ------------------------------\n",
      "\n",
      "Training Loss:  0.04412612096460081 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 482 ------------------------------\n",
      "\n",
      "Training Loss:  0.04423712957152685 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 483 ------------------------------\n",
      "\n",
      "Training Loss:  0.04655807909227906 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 484 ------------------------------\n",
      "\n",
      "Training Loss:  0.04456424964825188 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 485 ------------------------------\n",
      "\n",
      "Training Loss:  0.04463470141656333 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 486 ------------------------------\n",
      "\n",
      "Training Loss:  0.04308349232933068 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 487 ------------------------------\n",
      "\n",
      "Training Loss:  0.04327229912930679 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 488 ------------------------------\n",
      "\n",
      "Training Loss:  0.04247083391004503 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 489 ------------------------------\n",
      "\n",
      "Training Loss:  0.04381530538819014 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 490 ------------------------------\n",
      "\n",
      "Training Loss:  0.045361692964284286 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 491 ------------------------------\n",
      "\n",
      "Training Loss:  0.04390870076768846 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 492 ------------------------------\n",
      "\n",
      "Training Loss:  0.04568282460893324 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 493 ------------------------------\n",
      "\n",
      "Training Loss:  0.0445358660737106 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 494 ------------------------------\n",
      "\n",
      "Training Loss:  0.04396767400929451 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 495 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.04379511165858042 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 496 ------------------------------\n",
      "\n",
      "Training Loss:  0.04419664232782397 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 497 ------------------------------\n",
      "\n",
      "Training Loss:  0.04297534655774304 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 498 ------------------------------\n",
      "\n",
      "Training Loss:  0.04452284089546149 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 499 ------------------------------\n",
      "\n",
      "Training Loss:  0.046097366772888185 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 500 ------------------------------\n",
      "\n",
      "Training Loss:  0.044952016807035096 \n",
      "Valid Loss:  0.0512700126592739\n",
      "--------------------------------------Epoch 501 ------------------------------\n",
      "\n",
      "Training Loss:  0.04182038646468758 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 502 ------------------------------\n",
      "\n",
      "Training Loss:  0.04128953695906595 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 503 ------------------------------\n",
      "\n",
      "Training Loss:  0.04019032963429822 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 504 ------------------------------\n",
      "\n",
      "Training Loss:  0.04243551116461864 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 505 ------------------------------\n",
      "\n",
      "Training Loss:  0.04607635948506291 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 506 ------------------------------\n",
      "\n",
      "Training Loss:  0.04854648223394029 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 507 ------------------------------\n",
      "\n",
      "Training Loss:  0.044965463589870164 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 508 ------------------------------\n",
      "\n",
      "Training Loss:  0.043853621734430255 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 509 ------------------------------\n",
      "\n",
      "Training Loss:  0.040534608908534345 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 510 ------------------------------\n",
      "\n",
      "Training Loss:  0.04166869957200095 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 511 ------------------------------\n",
      "\n",
      "Training Loss:  0.043008514087990495 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 512 ------------------------------\n",
      "\n",
      "Training Loss:  0.040789558109902584 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 513 ------------------------------\n",
      "\n",
      "Training Loss:  0.039633772093780814 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 514 ------------------------------\n",
      "\n",
      "Training Loss:  0.04336491048855763 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 515 ------------------------------\n",
      "\n",
      "Training Loss:  0.04516018464350434 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 516 ------------------------------\n",
      "\n",
      "Training Loss:  0.044719104896539574 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 517 ------------------------------\n",
      "\n",
      "Training Loss:  0.043415156254896725 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 518 ------------------------------\n",
      "\n",
      "Training Loss:  0.04582483721518579 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 519 ------------------------------\n",
      "\n",
      "Training Loss:  0.04501763795181808 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 520 ------------------------------\n",
      "\n",
      "Training Loss:  0.042381119492635906 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 521 ------------------------------\n",
      "\n",
      "Training Loss:  0.03905360938552807 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 522 ------------------------------\n",
      "\n",
      "Training Loss:  0.038259907409221604 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 523 ------------------------------\n",
      "\n",
      "Training Loss:  0.04172464269829497 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 524 ------------------------------\n",
      "\n",
      "Training Loss:  0.04385426554331827 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 525 ------------------------------\n",
      "\n",
      "Training Loss:  0.04287265981073903 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 526 ------------------------------\n",
      "\n",
      "Training Loss:  0.0452299082364839 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 527 ------------------------------\n",
      "\n",
      "Training Loss:  0.04216032000649977 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 528 ------------------------------\n",
      "\n",
      "Training Loss:  0.044248257836403196 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 529 ------------------------------\n",
      "\n",
      "Training Loss:  0.043865527887206615 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 530 ------------------------------\n",
      "\n",
      "Training Loss:  0.04412319021935218 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 531 ------------------------------\n",
      "\n",
      "Training Loss:  0.04633526960732119 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 532 ------------------------------\n",
      "\n",
      "Training Loss:  0.044657348682045475 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 533 ------------------------------\n",
      "\n",
      "Training Loss:  0.043886552809616404 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 534 ------------------------------\n",
      "\n",
      "Training Loss:  0.04758163981301572 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 535 ------------------------------\n",
      "\n",
      "Training Loss:  0.05123507239363881 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 536 ------------------------------\n",
      "\n",
      "Training Loss:  0.04475892868184194 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 537 ------------------------------\n",
      "\n",
      "Training Loss:  0.047087224284428617 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 538 ------------------------------\n",
      "\n",
      "Training Loss:  0.04815204599310025 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 539 ------------------------------\n",
      "\n",
      "Training Loss:  0.045925123455665205 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 540 ------------------------------\n",
      "\n",
      "Training Loss:  0.04963780247660279 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 541 ------------------------------\n",
      "\n",
      "Training Loss:  0.04757677812713027 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 542 ------------------------------\n",
      "\n",
      "Training Loss:  0.04762595338109963 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 543 ------------------------------\n",
      "\n",
      "Training Loss:  0.047826844844654395 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 544 ------------------------------\n",
      "\n",
      "Training Loss:  0.0457051103402478 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 545 ------------------------------\n",
      "\n",
      "Training Loss:  0.04522524252648549 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 546 ------------------------------\n",
      "\n",
      "Training Loss:  0.04516747606199518 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 547 ------------------------------\n",
      "\n",
      "Training Loss:  0.044623563636480366 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 548 ------------------------------\n",
      "\n",
      "Training Loss:  0.0448850819061709 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 549 ------------------------------\n",
      "\n",
      "Training Loss:  0.044393001480156485 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 550 ------------------------------\n",
      "\n",
      "Training Loss:  0.04331556112561249 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 551 ------------------------------\n",
      "\n",
      "Training Loss:  0.043332653734503686 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 552 ------------------------------\n",
      "\n",
      "Training Loss:  0.042919134204450975 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 553 ------------------------------\n",
      "\n",
      "Training Loss:  0.04437894017924539 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 554 ------------------------------\n",
      "\n",
      "Training Loss:  0.04661141813348293 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 555 ------------------------------\n",
      "\n",
      "Training Loss:  0.04544202802863294 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 556 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.04920542618246434 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 557 ------------------------------\n",
      "\n",
      "Training Loss:  0.04910479966546076 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 558 ------------------------------\n",
      "\n",
      "Training Loss:  0.04255822990073437 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 559 ------------------------------\n",
      "\n",
      "Training Loss:  0.04551861531886919 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 560 ------------------------------\n",
      "\n",
      "Training Loss:  0.04950255345710206 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 561 ------------------------------\n",
      "\n",
      "Training Loss:  0.04313397950268318 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 562 ------------------------------\n",
      "\n",
      "Training Loss:  0.04565523465454138 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 563 ------------------------------\n",
      "\n",
      "Training Loss:  0.04415621973370742 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 564 ------------------------------\n",
      "\n",
      "Training Loss:  0.046837158999669115 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 565 ------------------------------\n",
      "\n",
      "Training Loss:  0.04941868236154418 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 566 ------------------------------\n",
      "\n",
      "Training Loss:  0.047116165817723626 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 567 ------------------------------\n",
      "\n",
      "Training Loss:  0.04555039635896533 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 568 ------------------------------\n",
      "\n",
      "Training Loss:  0.045287329057219106 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 569 ------------------------------\n",
      "\n",
      "Training Loss:  0.04396547405199034 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 570 ------------------------------\n",
      "\n",
      "Training Loss:  0.045743761648820845 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 571 ------------------------------\n",
      "\n",
      "Training Loss:  0.04870777735326388 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 572 ------------------------------\n",
      "\n",
      "Training Loss:  0.04790360207706759 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 573 ------------------------------\n",
      "\n",
      "Training Loss:  0.045162959775936255 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 574 ------------------------------\n",
      "\n",
      "Training Loss:  0.04480520633269663 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 575 ------------------------------\n",
      "\n",
      "Training Loss:  0.043187054017292774 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 576 ------------------------------\n",
      "\n",
      "Training Loss:  0.04451493922160406 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 577 ------------------------------\n",
      "\n",
      "Training Loss:  0.041152191776758676 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 578 ------------------------------\n",
      "\n",
      "Training Loss:  0.04226220402792531 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 579 ------------------------------\n",
      "\n",
      "Training Loss:  0.04173017920905306 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 580 ------------------------------\n",
      "\n",
      "Training Loss:  0.04163261659432081 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 581 ------------------------------\n",
      "\n",
      "Training Loss:  0.0417295141721414 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 582 ------------------------------\n",
      "\n",
      "Training Loss:  0.043473532656624986 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 583 ------------------------------\n",
      "\n",
      "Training Loss:  0.043106711648295416 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 584 ------------------------------\n",
      "\n",
      "Training Loss:  0.04362213308520842 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 585 ------------------------------\n",
      "\n",
      "Training Loss:  0.04530106175921759 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 586 ------------------------------\n",
      "\n",
      "Training Loss:  0.04950590193784482 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 587 ------------------------------\n",
      "\n",
      "Training Loss:  0.044301732833868165 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 588 ------------------------------\n",
      "\n",
      "Training Loss:  0.04274479585863987 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 589 ------------------------------\n",
      "\n",
      "Training Loss:  0.043403489323233575 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 590 ------------------------------\n",
      "\n",
      "Training Loss:  0.041188871955997804 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 591 ------------------------------\n",
      "\n",
      "Training Loss:  0.04282281298292511 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 592 ------------------------------\n",
      "\n",
      "Training Loss:  0.042334976289269154 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 593 ------------------------------\n",
      "\n",
      "Training Loss:  0.0435290150863076 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 594 ------------------------------\n",
      "\n",
      "Training Loss:  0.04491317286828413 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 595 ------------------------------\n",
      "\n",
      "Training Loss:  0.04241423213177249 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 596 ------------------------------\n",
      "\n",
      "Training Loss:  0.04056438559796352 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 597 ------------------------------\n",
      "\n",
      "Training Loss:  0.03994051810513152 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 598 ------------------------------\n",
      "\n",
      "Training Loss:  0.04083545636467534 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 599 ------------------------------\n",
      "\n",
      "Training Loss:  0.04237355221760821 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 600 ------------------------------\n",
      "\n",
      "Training Loss:  0.04098948259929218 \n",
      "Valid Loss:  0.036746568843496044\n",
      "--------------------------------------Epoch 601 ------------------------------\n",
      "\n",
      "Training Loss:  0.039042242000430334 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 602 ------------------------------\n",
      "\n",
      "Training Loss:  0.03906281794002919 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 603 ------------------------------\n",
      "\n",
      "Training Loss:  0.04248599815198039 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 604 ------------------------------\n",
      "\n",
      "Training Loss:  0.042099643254099116 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 605 ------------------------------\n",
      "\n",
      "Training Loss:  0.0443219998446392 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 606 ------------------------------\n",
      "\n",
      "Training Loss:  0.04771869052810469 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 607 ------------------------------\n",
      "\n",
      "Training Loss:  0.04705548669890451 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 608 ------------------------------\n",
      "\n",
      "Training Loss:  0.04145087888989647 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 609 ------------------------------\n",
      "\n",
      "Training Loss:  0.040249543311604734 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 610 ------------------------------\n",
      "\n",
      "Training Loss:  0.04114335051120263 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 611 ------------------------------\n",
      "\n",
      "Training Loss:  0.040921129465596645 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 612 ------------------------------\n",
      "\n",
      "Training Loss:  0.04068519180047376 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 613 ------------------------------\n",
      "\n",
      "Training Loss:  0.04147278699875609 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 614 ------------------------------\n",
      "\n",
      "Training Loss:  0.04006390526327993 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 615 ------------------------------\n",
      "\n",
      "Training Loss:  0.040576454034321266 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 616 ------------------------------\n",
      "\n",
      "Training Loss:  0.03996825749045073 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 617 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.04163075132975287 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 618 ------------------------------\n",
      "\n",
      "Training Loss:  0.03753590346540218 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 619 ------------------------------\n",
      "\n",
      "Training Loss:  0.03798630552351408 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 620 ------------------------------\n",
      "\n",
      "Training Loss:  0.03986709956799391 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 621 ------------------------------\n",
      "\n",
      "Training Loss:  0.040647134120695136 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 622 ------------------------------\n",
      "\n",
      "Training Loss:  0.04166218997621202 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 623 ------------------------------\n",
      "\n",
      "Training Loss:  0.04107984770127795 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 624 ------------------------------\n",
      "\n",
      "Training Loss:  0.042669878416667105 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 625 ------------------------------\n",
      "\n",
      "Training Loss:  0.04038962875567968 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 626 ------------------------------\n",
      "\n",
      "Training Loss:  0.040471952425631674 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 627 ------------------------------\n",
      "\n",
      "Training Loss:  0.04039832764077066 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 628 ------------------------------\n",
      "\n",
      "Training Loss:  0.04205686461104102 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 629 ------------------------------\n",
      "\n",
      "Training Loss:  0.04314497923345457 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 630 ------------------------------\n",
      "\n",
      "Training Loss:  0.04013043842891179 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 631 ------------------------------\n",
      "\n",
      "Training Loss:  0.039606761179902056 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 632 ------------------------------\n",
      "\n",
      "Training Loss:  0.04175415142714908 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 633 ------------------------------\n",
      "\n",
      "Training Loss:  0.038713838592281016 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 634 ------------------------------\n",
      "\n",
      "Training Loss:  0.03893812521950127 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 635 ------------------------------\n",
      "\n",
      "Training Loss:  0.03907512218583759 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 636 ------------------------------\n",
      "\n",
      "Training Loss:  0.03919417228161799 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 637 ------------------------------\n",
      "\n",
      "Training Loss:  0.0407736812963862 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 638 ------------------------------\n",
      "\n",
      "Training Loss:  0.038333037151275846 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 639 ------------------------------\n",
      "\n",
      "Training Loss:  0.0384742102325433 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 640 ------------------------------\n",
      "\n",
      "Training Loss:  0.03926090504429233 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 641 ------------------------------\n",
      "\n",
      "Training Loss:  0.04021602759439154 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 642 ------------------------------\n",
      "\n",
      "Training Loss:  0.04059617723828706 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 643 ------------------------------\n",
      "\n",
      "Training Loss:  0.03927970023708981 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 644 ------------------------------\n",
      "\n",
      "Training Loss:  0.040952153609186165 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 645 ------------------------------\n",
      "\n",
      "Training Loss:  0.04255071779472661 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 646 ------------------------------\n",
      "\n",
      "Training Loss:  0.04426503069284863 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 647 ------------------------------\n",
      "\n",
      "Training Loss:  0.03967846430921336 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 648 ------------------------------\n",
      "\n",
      "Training Loss:  0.03993767577930957 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 649 ------------------------------\n",
      "\n",
      "Training Loss:  0.040517624275954484 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 650 ------------------------------\n",
      "\n",
      "Training Loss:  0.04023081478833566 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 651 ------------------------------\n",
      "\n",
      "Training Loss:  0.04017040001770832 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 652 ------------------------------\n",
      "\n",
      "Training Loss:  0.038424043774011624 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 653 ------------------------------\n",
      "\n",
      "Training Loss:  0.03788290549556336 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 654 ------------------------------\n",
      "\n",
      "Training Loss:  0.038423671244940685 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 655 ------------------------------\n",
      "\n",
      "Training Loss:  0.03672478469503224 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 656 ------------------------------\n",
      "\n",
      "Training Loss:  0.038199383473198054 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 657 ------------------------------\n",
      "\n",
      "Training Loss:  0.036827656165831536 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 658 ------------------------------\n",
      "\n",
      "Training Loss:  0.03487013627356799 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 659 ------------------------------\n",
      "\n",
      "Training Loss:  0.03465695073219763 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 660 ------------------------------\n",
      "\n",
      "Training Loss:  0.03753566879207386 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 661 ------------------------------\n",
      "\n",
      "Training Loss:  0.03926015333060155 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 662 ------------------------------\n",
      "\n",
      "Training Loss:  0.038462225153379326 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 663 ------------------------------\n",
      "\n",
      "Training Loss:  0.04026989432128928 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 664 ------------------------------\n",
      "\n",
      "Training Loss:  0.03949575075662744 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 665 ------------------------------\n",
      "\n",
      "Training Loss:  0.04099452721079597 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 666 ------------------------------\n",
      "\n",
      "Training Loss:  0.03869849620946754 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 667 ------------------------------\n",
      "\n",
      "Training Loss:  0.03696327103165528 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 668 ------------------------------\n",
      "\n",
      "Training Loss:  0.038044790690930246 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 669 ------------------------------\n",
      "\n",
      "Training Loss:  0.03665084186122411 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 670 ------------------------------\n",
      "\n",
      "Training Loss:  0.03569474909931795 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 671 ------------------------------\n",
      "\n",
      "Training Loss:  0.04099569486850369 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 672 ------------------------------\n",
      "\n",
      "Training Loss:  0.03690351069967637 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 673 ------------------------------\n",
      "\n",
      "Training Loss:  0.03595527493802179 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 674 ------------------------------\n",
      "\n",
      "Training Loss:  0.04527073099744707 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 675 ------------------------------\n",
      "\n",
      "Training Loss:  0.043060559074136286 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 676 ------------------------------\n",
      "\n",
      "Training Loss:  0.04278940965302142 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 677 ------------------------------\n",
      "\n",
      "Training Loss:  0.042666548339686604 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 678 ------------------------------\n",
      "\n",
      "Training Loss:  0.03711433220914878 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 679 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.03922135411946272 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 680 ------------------------------\n",
      "\n",
      "Training Loss:  0.039909205553637646 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 681 ------------------------------\n",
      "\n",
      "Training Loss:  0.03934123740994649 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 682 ------------------------------\n",
      "\n",
      "Training Loss:  0.036351339085362575 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 683 ------------------------------\n",
      "\n",
      "Training Loss:  0.038223961847263975 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 684 ------------------------------\n",
      "\n",
      "Training Loss:  0.04004292275079802 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 685 ------------------------------\n",
      "\n",
      "Training Loss:  0.03798399848748492 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 686 ------------------------------\n",
      "\n",
      "Training Loss:  0.03652781274089681 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 687 ------------------------------\n",
      "\n",
      "Training Loss:  0.03660142491307249 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 688 ------------------------------\n",
      "\n",
      "Training Loss:  0.0366685774749575 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 689 ------------------------------\n",
      "\n",
      "Training Loss:  0.040107375427462365 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 690 ------------------------------\n",
      "\n",
      "Training Loss:  0.038946660594458636 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 691 ------------------------------\n",
      "\n",
      "Training Loss:  0.0376303192902754 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 692 ------------------------------\n",
      "\n",
      "Training Loss:  0.03513209835570439 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 693 ------------------------------\n",
      "\n",
      "Training Loss:  0.034540551356880986 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 694 ------------------------------\n",
      "\n",
      "Training Loss:  0.03636473103796265 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 695 ------------------------------\n",
      "\n",
      "Training Loss:  0.03558565012959327 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 696 ------------------------------\n",
      "\n",
      "Training Loss:  0.03902479845675522 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 697 ------------------------------\n",
      "\n",
      "Training Loss:  0.036939633096946166 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 698 ------------------------------\n",
      "\n",
      "Training Loss:  0.03632513244212394 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 699 ------------------------------\n",
      "\n",
      "Training Loss:  0.03602725639445296 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 700 ------------------------------\n",
      "\n",
      "Training Loss:  0.038167110479489785 \n",
      "Valid Loss:  0.04009030432403042\n",
      "--------------------------------------Epoch 701 ------------------------------\n",
      "\n",
      "Training Loss:  0.03817199735376251 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 702 ------------------------------\n",
      "\n",
      "Training Loss:  0.03778425413597725 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 703 ------------------------------\n",
      "\n",
      "Training Loss:  0.03771140697598873 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 704 ------------------------------\n",
      "\n",
      "Training Loss:  0.03651688367008648 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 705 ------------------------------\n",
      "\n",
      "Training Loss:  0.035521266972545706 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 706 ------------------------------\n",
      "\n",
      "Training Loss:  0.03800967545401429 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 707 ------------------------------\n",
      "\n",
      "Training Loss:  0.03878266909941703 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 708 ------------------------------\n",
      "\n",
      "Training Loss:  0.03725381768228716 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 709 ------------------------------\n",
      "\n",
      "Training Loss:  0.037874516837294335 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 710 ------------------------------\n",
      "\n",
      "Training Loss:  0.038592935404039505 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 711 ------------------------------\n",
      "\n",
      "Training Loss:  0.03660861444003636 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 712 ------------------------------\n",
      "\n",
      "Training Loss:  0.03431707206841365 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 713 ------------------------------\n",
      "\n",
      "Training Loss:  0.03371353244196541 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 714 ------------------------------\n",
      "\n",
      "Training Loss:  0.034795916161224835 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 715 ------------------------------\n",
      "\n",
      "Training Loss:  0.03418429899418416 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 716 ------------------------------\n",
      "\n",
      "Training Loss:  0.03379607753032519 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 717 ------------------------------\n",
      "\n",
      "Training Loss:  0.03541693033937518 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 718 ------------------------------\n",
      "\n",
      "Training Loss:  0.03578209357545486 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 719 ------------------------------\n",
      "\n",
      "Training Loss:  0.03891408558895482 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 720 ------------------------------\n",
      "\n",
      "Training Loss:  0.03561654476772425 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 721 ------------------------------\n",
      "\n",
      "Training Loss:  0.035010819446853136 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 722 ------------------------------\n",
      "\n",
      "Training Loss:  0.03884814832082087 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 723 ------------------------------\n",
      "\n",
      "Training Loss:  0.038664641067356 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 724 ------------------------------\n",
      "\n",
      "Training Loss:  0.04020200347043331 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 725 ------------------------------\n",
      "\n",
      "Training Loss:  0.04009039246332953 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 726 ------------------------------\n",
      "\n",
      "Training Loss:  0.04028587619950303 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 727 ------------------------------\n",
      "\n",
      "Training Loss:  0.03647151099402067 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 728 ------------------------------\n",
      "\n",
      "Training Loss:  0.037200378004398765 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 729 ------------------------------\n",
      "\n",
      "Training Loss:  0.03869754216180721 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 730 ------------------------------\n",
      "\n",
      "Training Loss:  0.03752640159809086 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 731 ------------------------------\n",
      "\n",
      "Training Loss:  0.03752699407869753 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 732 ------------------------------\n",
      "\n",
      "Training Loss:  0.03838671526229161 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 733 ------------------------------\n",
      "\n",
      "Training Loss:  0.0380235411564371 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 734 ------------------------------\n",
      "\n",
      "Training Loss:  0.04054905317014119 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 735 ------------------------------\n",
      "\n",
      "Training Loss:  0.0371681254837815 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 736 ------------------------------\n",
      "\n",
      "Training Loss:  0.03853449502164412 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 737 ------------------------------\n",
      "\n",
      "Training Loss:  0.04038149127884946 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 738 ------------------------------\n",
      "\n",
      "Training Loss:  0.03840200830134574 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 739 ------------------------------\n",
      "\n",
      "Training Loss:  0.038870357792809986 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 740 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.03754990969058686 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 741 ------------------------------\n",
      "\n",
      "Training Loss:  0.03843804833174917 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 742 ------------------------------\n",
      "\n",
      "Training Loss:  0.03858994791228297 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 743 ------------------------------\n",
      "\n",
      "Training Loss:  0.03849093725785043 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 744 ------------------------------\n",
      "\n",
      "Training Loss:  0.03996584935752724 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 745 ------------------------------\n",
      "\n",
      "Training Loss:  0.04276463478906013 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 746 ------------------------------\n",
      "\n",
      "Training Loss:  0.04150866431940407 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 747 ------------------------------\n",
      "\n",
      "Training Loss:  0.03970611840342797 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 748 ------------------------------\n",
      "\n",
      "Training Loss:  0.03810373857748502 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 749 ------------------------------\n",
      "\n",
      "Training Loss:  0.04121428165329684 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 750 ------------------------------\n",
      "\n",
      "Training Loss:  0.03892041399087638 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 751 ------------------------------\n",
      "\n",
      "Training Loss:  0.03669325375777173 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 752 ------------------------------\n",
      "\n",
      "Training Loss:  0.039277127454732615 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 753 ------------------------------\n",
      "\n",
      "Training Loss:  0.042369748463883236 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 754 ------------------------------\n",
      "\n",
      "Training Loss:  0.03980981888706624 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 755 ------------------------------\n",
      "\n",
      "Training Loss:  0.03671309812189047 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 756 ------------------------------\n",
      "\n",
      "Training Loss:  0.03648056159340329 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 757 ------------------------------\n",
      "\n",
      "Training Loss:  0.038737140628853806 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 758 ------------------------------\n",
      "\n",
      "Training Loss:  0.03951926004888301 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 759 ------------------------------\n",
      "\n",
      "Training Loss:  0.038913627441274454 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 760 ------------------------------\n",
      "\n",
      "Training Loss:  0.04347269693198267 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 761 ------------------------------\n",
      "\n",
      "Training Loss:  0.04021552269474601 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 762 ------------------------------\n",
      "\n",
      "Training Loss:  0.038557329319116825 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 763 ------------------------------\n",
      "\n",
      "Training Loss:  0.040384435122568646 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 764 ------------------------------\n",
      "\n",
      "Training Loss:  0.03912989555501883 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 765 ------------------------------\n",
      "\n",
      "Training Loss:  0.03884963996886081 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 766 ------------------------------\n",
      "\n",
      "Training Loss:  0.04317662020251549 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 767 ------------------------------\n",
      "\n",
      "Training Loss:  0.042579123397257876 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 768 ------------------------------\n",
      "\n",
      "Training Loss:  0.041433780270885304 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 769 ------------------------------\n",
      "\n",
      "Training Loss:  0.038680198528989636 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 770 ------------------------------\n",
      "\n",
      "Training Loss:  0.038550310642893194 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 771 ------------------------------\n",
      "\n",
      "Training Loss:  0.03807682792378465 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 772 ------------------------------\n",
      "\n",
      "Training Loss:  0.043876216881573275 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 773 ------------------------------\n",
      "\n",
      "Training Loss:  0.038329282602658846 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 774 ------------------------------\n",
      "\n",
      "Training Loss:  0.03653605883242511 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 775 ------------------------------\n",
      "\n",
      "Training Loss:  0.03775272195504345 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 776 ------------------------------\n",
      "\n",
      "Training Loss:  0.03778673840961748 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 777 ------------------------------\n",
      "\n",
      "Training Loss:  0.037564463933852286 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 778 ------------------------------\n",
      "\n",
      "Training Loss:  0.03696853341989846 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 779 ------------------------------\n",
      "\n",
      "Training Loss:  0.03527979444452668 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 780 ------------------------------\n",
      "\n",
      "Training Loss:  0.034767265357379666 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 781 ------------------------------\n",
      "\n",
      "Training Loss:  0.03608239359708061 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 782 ------------------------------\n",
      "\n",
      "Training Loss:  0.03635429874782495 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 783 ------------------------------\n",
      "\n",
      "Training Loss:  0.0356152391534524 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 784 ------------------------------\n",
      "\n",
      "Training Loss:  0.034856015225164835 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 785 ------------------------------\n",
      "\n",
      "Training Loss:  0.0348416201561257 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 786 ------------------------------\n",
      "\n",
      "Training Loss:  0.03284888707509734 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 787 ------------------------------\n",
      "\n",
      "Training Loss:  0.03369582993303058 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 788 ------------------------------\n",
      "\n",
      "Training Loss:  0.03142111926303576 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 789 ------------------------------\n",
      "\n",
      "Training Loss:  0.030626246913048036 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 790 ------------------------------\n",
      "\n",
      "Training Loss:  0.03128594523301641 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 791 ------------------------------\n",
      "\n",
      "Training Loss:  0.03570607916926163 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 792 ------------------------------\n",
      "\n",
      "Training Loss:  0.03384440661350337 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 793 ------------------------------\n",
      "\n",
      "Training Loss:  0.03162348567155023 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 794 ------------------------------\n",
      "\n",
      "Training Loss:  0.0357705106798895 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 795 ------------------------------\n",
      "\n",
      "Training Loss:  0.03613935262560846 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 796 ------------------------------\n",
      "\n",
      "Training Loss:  0.03645103725705349 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 797 ------------------------------\n",
      "\n",
      "Training Loss:  0.03608804709566598 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 798 ------------------------------\n",
      "\n",
      "Training Loss:  0.036029817576213444 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 799 ------------------------------\n",
      "\n",
      "Training Loss:  0.038217513256075065 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 800 ------------------------------\n",
      "\n",
      "Training Loss:  0.03849799094372845 \n",
      "Valid Loss:  0.03861629368053714\n",
      "--------------------------------------Epoch 801 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.039308520780036174 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 802 ------------------------------\n",
      "\n",
      "Training Loss:  0.03986526386453 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 803 ------------------------------\n",
      "\n",
      "Training Loss:  0.038696697706256826 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 804 ------------------------------\n",
      "\n",
      "Training Loss:  0.03883435388262794 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 805 ------------------------------\n",
      "\n",
      "Training Loss:  0.03509429487660756 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 806 ------------------------------\n",
      "\n",
      "Training Loss:  0.03403876147558366 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 807 ------------------------------\n",
      "\n",
      "Training Loss:  0.034669433320021266 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 808 ------------------------------\n",
      "\n",
      "Training Loss:  0.0357407841843817 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 809 ------------------------------\n",
      "\n",
      "Training Loss:  0.03430236742145328 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 810 ------------------------------\n",
      "\n",
      "Training Loss:  0.033474016769566815 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 811 ------------------------------\n",
      "\n",
      "Training Loss:  0.03562813350152056 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 812 ------------------------------\n",
      "\n",
      "Training Loss:  0.036693543100533445 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 813 ------------------------------\n",
      "\n",
      "Training Loss:  0.03747543529203197 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 814 ------------------------------\n",
      "\n",
      "Training Loss:  0.03654700106756812 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 815 ------------------------------\n",
      "\n",
      "Training Loss:  0.03715532226989312 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 816 ------------------------------\n",
      "\n",
      "Training Loss:  0.03767368170044856 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 817 ------------------------------\n",
      "\n",
      "Training Loss:  0.03832781823851851 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 818 ------------------------------\n",
      "\n",
      "Training Loss:  0.03747911327208345 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 819 ------------------------------\n",
      "\n",
      "Training Loss:  0.03701861870117737 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 820 ------------------------------\n",
      "\n",
      "Training Loss:  0.0374935397518585 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 821 ------------------------------\n",
      "\n",
      "Training Loss:  0.04017677924211346 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 822 ------------------------------\n",
      "\n",
      "Training Loss:  0.03954116564211042 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 823 ------------------------------\n",
      "\n",
      "Training Loss:  0.03739284020895709 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 824 ------------------------------\n",
      "\n",
      "Training Loss:  0.0362561319463263 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 825 ------------------------------\n",
      "\n",
      "Training Loss:  0.03571830914792834 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 826 ------------------------------\n",
      "\n",
      "Training Loss:  0.038241136488336616 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 827 ------------------------------\n",
      "\n",
      "Training Loss:  0.03861950595910092 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 828 ------------------------------\n",
      "\n",
      "Training Loss:  0.038552502566910084 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 829 ------------------------------\n",
      "\n",
      "Training Loss:  0.036592370024709636 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 830 ------------------------------\n",
      "\n",
      "Training Loss:  0.03542726541810594 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 831 ------------------------------\n",
      "\n",
      "Training Loss:  0.038731534524002194 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 832 ------------------------------\n",
      "\n",
      "Training Loss:  0.03778721031540775 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 833 ------------------------------\n",
      "\n",
      "Training Loss:  0.03732746497589833 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 834 ------------------------------\n",
      "\n",
      "Training Loss:  0.03674099308454422 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 835 ------------------------------\n",
      "\n",
      "Training Loss:  0.038233421722796736 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 836 ------------------------------\n",
      "\n",
      "Training Loss:  0.037296433399102664 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 837 ------------------------------\n",
      "\n",
      "Training Loss:  0.03691736991535103 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 838 ------------------------------\n",
      "\n",
      "Training Loss:  0.03620807613850661 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 839 ------------------------------\n",
      "\n",
      "Training Loss:  0.03585762889895889 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 840 ------------------------------\n",
      "\n",
      "Training Loss:  0.03350460956838573 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 841 ------------------------------\n",
      "\n",
      "Training Loss:  0.033691427919572366 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 842 ------------------------------\n",
      "\n",
      "Training Loss:  0.037185660218959214 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 843 ------------------------------\n",
      "\n",
      "Training Loss:  0.03863149487484867 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 844 ------------------------------\n",
      "\n",
      "Training Loss:  0.03459407860648033 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 845 ------------------------------\n",
      "\n",
      "Training Loss:  0.03555142648310794 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 846 ------------------------------\n",
      "\n",
      "Training Loss:  0.034692334863715965 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 847 ------------------------------\n",
      "\n",
      "Training Loss:  0.034673735265582525 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 848 ------------------------------\n",
      "\n",
      "Training Loss:  0.035085248502819966 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 849 ------------------------------\n",
      "\n",
      "Training Loss:  0.03484260603342142 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 850 ------------------------------\n",
      "\n",
      "Training Loss:  0.03396644696361813 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 851 ------------------------------\n",
      "\n",
      "Training Loss:  0.03762306488826459 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 852 ------------------------------\n",
      "\n",
      "Training Loss:  0.03859656281515691 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 853 ------------------------------\n",
      "\n",
      "Training Loss:  0.037545599459114294 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 854 ------------------------------\n",
      "\n",
      "Training Loss:  0.0415486124770303 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 855 ------------------------------\n",
      "\n",
      "Training Loss:  0.04250203253907448 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 856 ------------------------------\n",
      "\n",
      "Training Loss:  0.040655265892357316 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 857 ------------------------------\n",
      "\n",
      "Training Loss:  0.04309309348016369 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 858 ------------------------------\n",
      "\n",
      "Training Loss:  0.04296655703444092 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 859 ------------------------------\n",
      "\n",
      "Training Loss:  0.04081323185394204 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 860 ------------------------------\n",
      "\n",
      "Training Loss:  0.037062741983379006 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 861 ------------------------------\n",
      "\n",
      "Training Loss:  0.03525907985523742 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 862 ------------------------------\n",
      "\n",
      "Training Loss:  0.03614443565085007 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 863 ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss:  0.03501626928294557 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 864 ------------------------------\n",
      "\n",
      "Training Loss:  0.032605868271837125 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 865 ------------------------------\n",
      "\n",
      "Training Loss:  0.03314694539981831 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 866 ------------------------------\n",
      "\n",
      "Training Loss:  0.033046849904640185 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 867 ------------------------------\n",
      "\n",
      "Training Loss:  0.03431940935944974 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 868 ------------------------------\n",
      "\n",
      "Training Loss:  0.03660753599214182 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 869 ------------------------------\n",
      "\n",
      "Training Loss:  0.03600405831154209 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 870 ------------------------------\n",
      "\n",
      "Training Loss:  0.03547860901021449 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 871 ------------------------------\n",
      "\n",
      "Training Loss:  0.03843006331542064 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 872 ------------------------------\n",
      "\n",
      "Training Loss:  0.036755600818481754 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 873 ------------------------------\n",
      "\n",
      "Training Loss:  0.03801819145117618 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 874 ------------------------------\n",
      "\n",
      "Training Loss:  0.035157456012754836 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 875 ------------------------------\n",
      "\n",
      "Training Loss:  0.03465825381337789 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 876 ------------------------------\n",
      "\n",
      "Training Loss:  0.033610712583226246 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 877 ------------------------------\n",
      "\n",
      "Training Loss:  0.03403166634691854 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 878 ------------------------------\n",
      "\n",
      "Training Loss:  0.03343242222828655 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 879 ------------------------------\n",
      "\n",
      "Training Loss:  0.03591202311009286 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 880 ------------------------------\n",
      "\n",
      "Training Loss:  0.035336960076308634 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 881 ------------------------------\n",
      "\n",
      "Training Loss:  0.03480181064384785 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 882 ------------------------------\n",
      "\n",
      "Training Loss:  0.036267169748460525 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 883 ------------------------------\n",
      "\n",
      "Training Loss:  0.037342373962419295 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 884 ------------------------------\n",
      "\n",
      "Training Loss:  0.03724365911861 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 885 ------------------------------\n",
      "\n",
      "Training Loss:  0.0363831737813735 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 886 ------------------------------\n",
      "\n",
      "Training Loss:  0.03731149707947904 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 887 ------------------------------\n",
      "\n",
      "Training Loss:  0.036826385255190566 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 888 ------------------------------\n",
      "\n",
      "Training Loss:  0.035968749076690544 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 889 ------------------------------\n",
      "\n",
      "Training Loss:  0.037572041686793144 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 890 ------------------------------\n",
      "\n",
      "Training Loss:  0.03790259113396088 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 891 ------------------------------\n",
      "\n",
      "Training Loss:  0.03583652890537849 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 892 ------------------------------\n",
      "\n",
      "Training Loss:  0.03670870200756739 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 893 ------------------------------\n",
      "\n",
      "Training Loss:  0.038490839601576385 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 894 ------------------------------\n",
      "\n",
      "Training Loss:  0.03579575325940264 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 895 ------------------------------\n",
      "\n",
      "Training Loss:  0.03720924081309738 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 896 ------------------------------\n",
      "\n",
      "Training Loss:  0.03798142843318265 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 897 ------------------------------\n",
      "\n",
      "Training Loss:  0.03755121441199471 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 898 ------------------------------\n",
      "\n",
      "Training Loss:  0.036809845449708854 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 899 ------------------------------\n",
      "\n",
      "Training Loss:  0.03460723738930846 \n",
      "Valid Loss:  nan\n",
      "--------------------------------------Epoch 900 ------------------------------\n",
      "\n",
      "Training Loss:  0.03454617597983141 \n",
      "Valid Loss:  0.03483397555371438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\envs\\machine-learning\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Network. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n",
      "torch.Size([513, 21])\n",
      "498\n",
      "torch.Size([513, 21])\n"
     ]
    }
   ],
   "source": [
    "mask, mask_test = train_network(nr_epochs=900, train_set=train_set, test_set=test_set)\n",
    "\n",
    "print(len(mask))\n",
    "print(mask[0].shape)\n",
    "\n",
    "print(len(mask_test))\n",
    "print(mask_test[0].shape)\n",
    "\n",
    "# apply generate mask from training, testing after training and validation \n",
    "split_audios_epoch(\"training-mask\", mask)\n",
    "split_audios_epoch(\"training-TEST-mask\", mask_test)\n",
    "\n",
    "# valid_mask = eval(network=network, test_set=test_set)\n",
    "# split_audios_epoch(\"validation-mask\", valid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set loss: 0.03483397478874761\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([513, 21])\n"
     ]
    }
   ],
   "source": [
    "# apply generate mask from training, testing after training and validation \n",
    "# split_audios_epoch(\"training-mask\", mask)\n",
    "# split_audios_epoch(\"training-TEST-mask\", mask_test)\n",
    "torch.cuda.empty_cache()\n",
    "network = torch.load('./model.pth')\n",
    "valid_mask = eval(network=network, test_set=test_set)\n",
    "\n",
    "print(valid_mask[0].shape)\n",
    "split_audios_epoch(\"validation-mask\", valid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input and target sent to CUDA\n",
      "Train Epoch: 1 [0/997 (0%)]\tLoss: 0.045482\n",
      "Train Epoch: 1 [10/997 (1%)]\tLoss: 0.040284\n",
      "Train Epoch: 1 [20/997 (2%)]\tLoss: 0.108915\n",
      "Train Epoch: 1 [30/997 (3%)]\tLoss: 0.386990\n",
      "Train Epoch: 1 [40/997 (4%)]\tLoss: 0.535919\n",
      "Train Epoch: 1 [50/997 (5%)]\tLoss: 0.502154\n",
      "Train Epoch: 1 [60/997 (6%)]\tLoss: 0.428798\n",
      "Train Epoch: 1 [70/997 (7%)]\tLoss: 0.342384\n",
      "Train Epoch: 1 [80/997 (8%)]\tLoss: 0.279553\n",
      "Train Epoch: 1 [90/997 (9%)]\tLoss: 0.338721\n",
      "Train Epoch: 1 [100/997 (10%)]\tLoss: 0.207496\n",
      "Train Epoch: 1 [110/997 (11%)]\tLoss: 0.104407\n",
      "Train Epoch: 1 [120/997 (12%)]\tLoss: 0.052740\n",
      "Train Epoch: 1 [130/997 (13%)]\tLoss: 0.093411\n",
      "Train Epoch: 1 [140/997 (14%)]\tLoss: 0.328593\n",
      "Train Epoch: 1 [150/997 (15%)]\tLoss: 0.057406\n",
      "Train Epoch: 1 [160/997 (16%)]\tLoss: 0.247350\n",
      "Train Epoch: 1 [170/997 (17%)]\tLoss: 0.368980\n",
      "Train Epoch: 1 [180/997 (18%)]\tLoss: 0.216839\n",
      "Train Epoch: 1 [190/997 (19%)]\tLoss: 0.076148\n",
      "Train Epoch: 1 [200/997 (20%)]\tLoss: 0.066559\n",
      "Train Epoch: 1 [210/997 (21%)]\tLoss: 0.080155\n",
      "Train Epoch: 1 [220/997 (22%)]\tLoss: 0.149601\n",
      "Train Epoch: 1 [230/997 (23%)]\tLoss: 0.275532\n",
      "Train Epoch: 1 [240/997 (24%)]\tLoss: 0.091165\n",
      "Train Epoch: 1 [250/997 (25%)]\tLoss: 0.192868\n",
      "Train Epoch: 1 [260/997 (26%)]\tLoss: 0.248300\n",
      "Train Epoch: 1 [270/997 (27%)]\tLoss: 0.109356\n",
      "Train Epoch: 1 [280/997 (28%)]\tLoss: 0.104185\n",
      "Train Epoch: 1 [290/997 (29%)]\tLoss: 0.212662\n",
      "Train Epoch: 1 [300/997 (30%)]\tLoss: 0.117739\n",
      "Train Epoch: 1 [310/997 (31%)]\tLoss: 0.063100\n",
      "Train Epoch: 1 [320/997 (32%)]\tLoss: 0.088730\n",
      "Train Epoch: 1 [330/997 (33%)]\tLoss: 0.083038\n",
      "Train Epoch: 1 [340/997 (34%)]\tLoss: 0.158974\n",
      "Train Epoch: 1 [350/997 (35%)]\tLoss: 0.181319\n",
      "Train Epoch: 1 [360/997 (36%)]\tLoss: 0.093876\n",
      "Train Epoch: 1 [370/997 (37%)]\tLoss: 0.111343\n",
      "Train Epoch: 1 [380/997 (38%)]\tLoss: 0.066091\n",
      "Train Epoch: 1 [390/997 (39%)]\tLoss: 0.086840\n",
      "Train Epoch: 1 [400/997 (40%)]\tLoss: 0.142160\n",
      "Train Epoch: 1 [410/997 (41%)]\tLoss: 0.149704\n",
      "Train Epoch: 1 [420/997 (42%)]\tLoss: 0.100268\n",
      "Train Epoch: 1 [430/997 (43%)]\tLoss: 0.169860\n",
      "Train Epoch: 1 [440/997 (44%)]\tLoss: 0.191761\n",
      "Train Epoch: 1 [450/997 (45%)]\tLoss: 0.132187\n",
      "Train Epoch: 1 [460/997 (46%)]\tLoss: 0.132330\n",
      "Train Epoch: 1 [470/997 (47%)]\tLoss: 0.073979\n",
      "Train Epoch: 1 [480/997 (48%)]\tLoss: 0.119663\n",
      "Train Epoch: 1 [490/997 (49%)]\tLoss: 0.080200\n",
      "Train Epoch: 1 [500/997 (50%)]\tLoss: 0.128514\n",
      "Train Epoch: 1 [510/997 (51%)]\tLoss: 0.114722\n",
      "Train Epoch: 1 [520/997 (52%)]\tLoss: 0.109767\n",
      "Train Epoch: 1 [530/997 (53%)]\tLoss: 0.106231\n",
      "Train Epoch: 1 [540/997 (54%)]\tLoss: 0.128016\n",
      "Train Epoch: 1 [550/997 (55%)]\tLoss: 0.075906\n",
      "Train Epoch: 1 [560/997 (56%)]\tLoss: 0.077291\n",
      "Train Epoch: 1 [570/997 (57%)]\tLoss: 0.105438\n",
      "Train Epoch: 1 [580/997 (58%)]\tLoss: 0.150372\n",
      "Train Epoch: 1 [590/997 (59%)]\tLoss: 0.119494\n",
      "Train Epoch: 1 [600/997 (60%)]\tLoss: 0.083358\n",
      "Train Epoch: 1 [610/997 (61%)]\tLoss: 0.079597\n",
      "Train Epoch: 1 [620/997 (62%)]\tLoss: 0.169866\n",
      "Train Epoch: 1 [630/997 (63%)]\tLoss: 0.169321\n",
      "Train Epoch: 1 [640/997 (64%)]\tLoss: 0.132657\n",
      "Train Epoch: 1 [650/997 (65%)]\tLoss: 0.086482\n",
      "Train Epoch: 1 [660/997 (66%)]\tLoss: 0.075783\n",
      "Train Epoch: 1 [670/997 (67%)]\tLoss: 0.061614\n",
      "Train Epoch: 1 [680/997 (68%)]\tLoss: 0.076773\n",
      "Train Epoch: 1 [690/997 (69%)]\tLoss: 0.178952\n",
      "Train Epoch: 1 [700/997 (70%)]\tLoss: 0.178935\n",
      "Train Epoch: 1 [710/997 (71%)]\tLoss: 0.181738\n",
      "Train Epoch: 1 [720/997 (72%)]\tLoss: 0.226983\n",
      "Train Epoch: 1 [730/997 (73%)]\tLoss: 0.122210\n",
      "Train Epoch: 1 [740/997 (74%)]\tLoss: 0.284340\n",
      "Train Epoch: 1 [750/997 (75%)]\tLoss: 0.246078\n",
      "Train Epoch: 1 [760/997 (76%)]\tLoss: 0.261696\n",
      "Train Epoch: 1 [770/997 (77%)]\tLoss: 0.227542\n",
      "Train Epoch: 1 [780/997 (78%)]\tLoss: 0.168967\n",
      "Train Epoch: 1 [790/997 (79%)]\tLoss: 0.157010\n",
      "Train Epoch: 1 [800/997 (80%)]\tLoss: 0.200121\n",
      "Train Epoch: 1 [810/997 (81%)]\tLoss: 0.167542\n",
      "Train Epoch: 1 [820/997 (82%)]\tLoss: 0.189853\n",
      "Train Epoch: 1 [830/997 (83%)]\tLoss: 0.154120\n",
      "Train Epoch: 1 [840/997 (84%)]\tLoss: 0.116849\n",
      "Train Epoch: 1 [850/997 (85%)]\tLoss: 0.146261\n",
      "Train Epoch: 1 [860/997 (86%)]\tLoss: 0.149794\n",
      "Train Epoch: 1 [870/997 (87%)]\tLoss: 0.101054\n",
      "Train Epoch: 1 [880/997 (88%)]\tLoss: 0.116234\n",
      "Train Epoch: 1 [890/997 (89%)]\tLoss: 0.084451\n",
      "Train Epoch: 1 [900/997 (90%)]\tLoss: 0.095311\n",
      "Train Epoch: 1 [910/997 (91%)]\tLoss: 0.117290\n",
      "Train Epoch: 1 [920/997 (92%)]\tLoss: 0.073523\n",
      "Train Epoch: 1 [930/997 (93%)]\tLoss: 0.107564\n",
      "Train Epoch: 1 [940/997 (94%)]\tLoss: 0.063144\n",
      "Train Epoch: 1 [950/997 (95%)]\tLoss: 0.072459\n",
      "Train Epoch: 1 [960/997 (96%)]\tLoss: 0.055890\n",
      "Train Epoch: 1 [970/997 (97%)]\tLoss: 0.061158\n",
      "Train Epoch: 1 [980/997 (98%)]\tLoss: 0.083958\n",
      "Train Epoch: 1 [990/997 (99%)]\tLoss: 0.055460\n",
      "input and target sent to CUDA\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1536, 0.1540, 0.1440,  ..., 0.2061, 0.2061, 0.1963], device='cuda:0')\n",
      "t tensor([0.0047, 0.0047, 0.0047,  ..., 0.3258, 0.3257, 0.3256], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1825, 0.1646, 0.1739,  ..., 0.2643, 0.2697, 0.2780], device='cuda:0')\n",
      "t tensor([0.0247, 0.0247, 0.0247,  ..., 0.0017, 0.0020, 0.0021], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1559, 0.1521, 0.1487,  ..., 0.2155, 0.2347, 0.2326], device='cuda:0')\n",
      "t tensor([0.0348, 0.0348, 0.0348,  ..., 0.4665, 0.4665, 0.4665], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1768, 0.1629, 0.1742,  ..., 0.2606, 0.2661, 0.2753], device='cuda:0')\n",
      "t tensor([0.6048, 0.6048, 0.6047,  ..., 0.9539, 0.9540, 0.9540], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1337, 0.1453, 0.1371,  ..., 0.1957, 0.1968, 0.1870], device='cuda:0')\n",
      "t tensor([0.7523, 0.7524, 0.7524,  ..., 0.9628, 0.9628, 0.9628], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1317, 0.1452, 0.1375,  ..., 0.1952, 0.1945, 0.1849], device='cuda:0')\n",
      "t tensor([0.5879, 0.5898, 0.5951,  ..., 0.9336, 0.9336, 0.9336], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1386, 0.1462, 0.1372,  ..., 0.1966, 0.1988, 0.1883], device='cuda:0')\n",
      "t tensor([0.3946, 0.3929, 0.3882,  ..., 0.9903, 0.9904, 0.9904], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1658, 0.1542, 0.1503,  ..., 0.2408, 0.2741, 0.2569], device='cuda:0')\n",
      "t tensor([0.0886, 0.0886, 0.0887,  ..., 0.8846, 0.8844, 0.8844], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1427, 0.1467, 0.1370,  ..., 0.1974, 0.2001, 0.1885], device='cuda:0')\n",
      "t tensor([0.7348, 0.7348, 0.7347,  ..., 0.5635, 0.5635, 0.5635], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1374, 0.1458, 0.1371,  ..., 0.1963, 0.1985, 0.1882], device='cuda:0')\n",
      "t tensor([0.5335, 0.5334, 0.5330,  ..., 0.6007, 0.6014, 0.6016], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1402, 0.1461, 0.1372,  ..., 0.1965, 0.1983, 0.1871], device='cuda:0')\n",
      "t tensor([0.4988, 0.4988, 0.4991,  ..., 0.9337, 0.9338, 0.9338], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1393, 0.1461, 0.1373,  ..., 0.1969, 0.1981, 0.1890], device='cuda:0')\n",
      "t tensor([0.0616, 0.0617, 0.0620,  ..., 0.6313, 0.6312, 0.6312], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1357, 0.1460, 0.1373,  ..., 0.1961, 0.1968, 0.1876], device='cuda:0')\n",
      "t tensor([0.2780, 0.2779, 0.2775,  ..., 0.3864, 0.3865, 0.3865], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1395, 0.1460, 0.1372,  ..., 0.1969, 0.1987, 0.1891], device='cuda:0')\n",
      "t tensor([0.2567, 0.2565, 0.2558,  ..., 0.2739, 0.2738, 0.2738], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1442, 0.1470, 0.1369,  ..., 0.1980, 0.2003, 0.1896], device='cuda:0')\n",
      "t tensor([0.7539, 0.7537, 0.7530,  ..., 0.9571, 0.9571, 0.9571], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1389, 0.1460, 0.1373,  ..., 0.1967, 0.1989, 0.1889], device='cuda:0')\n",
      "t tensor([0.2040, 0.2035, 0.2020,  ..., 0.4414, 0.4413, 0.4413], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1387, 0.1459, 0.1371,  ..., 0.1967, 0.1987, 0.1887], device='cuda:0')\n",
      "t tensor([0.2856, 0.2856, 0.2854,  ..., 0.8735, 0.8735, 0.8735], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1374, 0.1458, 0.1372,  ..., 0.1963, 0.1985, 0.1882], device='cuda:0')\n",
      "t tensor([0.1827, 0.1823, 0.1809,  ..., 0.9637, 0.9637, 0.9637], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1384, 0.1460, 0.1374,  ..., 0.1966, 0.1982, 0.1886], device='cuda:0')\n",
      "t tensor([0.3747, 0.3743, 0.3730,  ..., 0.6949, 0.6949, 0.6949], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1386, 0.1459, 0.1373,  ..., 0.1966, 0.1988, 0.1887], device='cuda:0')\n",
      "t tensor([0.2302, 0.2302, 0.2301,  ..., 0.6374, 0.6371, 0.6370], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o tensor([0.1374, 0.1458, 0.1371,  ..., 0.1963, 0.1985, 0.1882], device='cuda:0')\n",
      "t tensor([0.2914, 0.2912, 0.2906,  ..., 0.3812, 0.3812, 0.3811], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1425, 0.1464, 0.1369,  ..., 0.1971, 0.1989, 0.1881], device='cuda:0')\n",
      "t tensor([0.8507, 0.8507, 0.8507,  ..., 0.1821, 0.1822, 0.1823], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1394, 0.1460, 0.1372,  ..., 0.1968, 0.1987, 0.1891], device='cuda:0')\n",
      "t tensor([0.9335, 0.9334, 0.9333,  ..., 0.0684, 0.0683, 0.0683], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1397, 0.1463, 0.1370,  ..., 0.1966, 0.1964, 0.1866], device='cuda:0')\n",
      "t tensor([0.9413, 0.9413, 0.9414,  ..., 0.7688, 0.7688, 0.7688], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1366, 0.1458, 0.1374,  ..., 0.1962, 0.1964, 0.1873], device='cuda:0')\n",
      "t tensor([0.6306, 0.6306, 0.6305,  ..., 0.6907, 0.6909, 0.6909], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1379, 0.1466, 0.1382,  ..., 0.1965, 0.2003, 0.1882], device='cuda:0')\n",
      "t tensor([0.0256, 0.0256, 0.0255,  ..., 0.9322, 0.9322, 0.9322], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1294, 0.1448, 0.1374,  ..., 0.1947, 0.1931, 0.1823], device='cuda:0')\n",
      "t tensor([0.1775, 0.1775, 0.1775,  ..., 0.9676, 0.9676, 0.9676], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1413, 0.1463, 0.1373,  ..., 0.1976, 0.1990, 0.1900], device='cuda:0')\n",
      "t tensor([0.5727, 0.5722, 0.5708,  ..., 0.3124, 0.3121, 0.3120], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1342, 0.1458, 0.1375,  ..., 0.1958, 0.1947, 0.1860], device='cuda:0')\n",
      "t tensor([0.2348, 0.2342, 0.2325,  ..., 0.4525, 0.4524, 0.4524], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1394, 0.1461, 0.1373,  ..., 0.1969, 0.1982, 0.1890], device='cuda:0')\n",
      "t tensor([0.7909, 0.7908, 0.7906,  ..., 0.8627, 0.8628, 0.8628], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1384, 0.1460, 0.1374,  ..., 0.1966, 0.1982, 0.1886], device='cuda:0')\n",
      "t tensor([0.4285, 0.4283, 0.4279,  ..., 0.8922, 0.8919, 0.8919], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1395, 0.1460, 0.1372,  ..., 0.1969, 0.1987, 0.1891], device='cuda:0')\n",
      "t tensor([0.1388, 0.1376, 0.1339,  ..., 0.4811, 0.4811, 0.4811], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1374, 0.1458, 0.1371,  ..., 0.1963, 0.1985, 0.1882], device='cuda:0')\n",
      "t tensor([0.1970, 0.1972, 0.1978,  ..., 0.3019, 0.3019, 0.3019], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1384, 0.1460, 0.1374,  ..., 0.1966, 0.1982, 0.1886], device='cuda:0')\n",
      "t tensor([0.4834, 0.4836, 0.4842,  ..., 0.2392, 0.2392, 0.2392], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1666, 0.1529, 0.1483,  ..., 0.2137, 0.2340, 0.2208], device='cuda:0')\n",
      "t tensor([0.2179, 0.2179, 0.2179,  ..., 0.2706, 0.2705, 0.2703], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1804, 0.1582, 0.1603,  ..., 0.2512, 0.2741, 0.2702], device='cuda:0')\n",
      "t tensor([0.0409, 0.0409, 0.0409,  ..., 0.7987, 0.7987, 0.7987], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1300, 0.1453, 0.1370,  ..., 0.1948, 0.1936, 0.1834], device='cuda:0')\n",
      "t tensor([0.0342, 0.0342, 0.0342,  ..., 0.9659, 0.9659, 0.9659], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1355, 0.1458, 0.1373,  ..., 0.1959, 0.1955, 0.1861], device='cuda:0')\n",
      "t tensor([0.3493, 0.3492, 0.3487,  ..., 0.2131, 0.2133, 0.2134], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1360, 0.1459, 0.1373,  ..., 0.1961, 0.1974, 0.1877], device='cuda:0')\n",
      "t tensor([0.5021, 0.5020, 0.5020,  ..., 0.3519, 0.3519, 0.3519], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1385, 0.1460, 0.1374,  ..., 0.1967, 0.1983, 0.1887], device='cuda:0')\n",
      "t tensor([0.5981, 0.5980, 0.5979,  ..., 0.7961, 0.7961, 0.7961], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1420, 0.1463, 0.1371,  ..., 0.1973, 0.2006, 0.1893], device='cuda:0')\n",
      "t tensor([0.7488, 0.7488, 0.7488,  ..., 0.8285, 0.8285, 0.8285], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1365, 0.1456, 0.1369,  ..., 0.1958, 0.1976, 0.1868], device='cuda:0')\n",
      "t tensor([0.0456, 0.0458, 0.0463,  ..., 0.0027, 0.0027, 0.0027], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1352, 0.1456, 0.1411,  ..., 0.1949, 0.1936, 0.1864], device='cuda:0')\n",
      "t tensor([0.5312, 0.5311, 0.5311,  ..., 0.1408, 0.1408, 0.1408], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1872, 0.1666, 0.1749,  ..., 0.2751, 0.2777, 0.2953], device='cuda:0')\n",
      "t tensor([0.2914, 0.2914, 0.2913,  ..., 0.9984, 0.9984, 0.9984], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1903, 0.1688, 0.1758,  ..., 0.2749, 0.2778, 0.2957], device='cuda:0')\n",
      "t tensor([0.5180, 0.5180, 0.5180,  ..., 0.9702, 0.9702, 0.9702], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1915, 0.1677, 0.1770,  ..., 0.2758, 0.2778, 0.2944], device='cuda:0')\n",
      "t tensor([0.1826, 0.1826, 0.1827,  ..., 0.9062, 0.9062, 0.9063], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1651, 0.1554, 0.1679,  ..., 0.2399, 0.2714, 0.2688], device='cuda:0')\n",
      "t tensor([0.1136, 0.1137, 0.1137,  ..., 0.9989, 0.9989, 0.9989], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1320, 0.1454, 0.1370,  ..., 0.1954, 0.1952, 0.1859], device='cuda:0')\n",
      "t tensor([0.4704, 0.4702, 0.4699,  ..., 0.0199, 0.0199, 0.0199], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1386, 0.1459, 0.1373,  ..., 0.1966, 0.1988, 0.1887], device='cuda:0')\n",
      "t tensor([0.9487, 0.9486, 0.9482,  ..., 0.5739, 0.5738, 0.5738], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1395, 0.1460, 0.1372,  ..., 0.1969, 0.1987, 0.1891], device='cuda:0')\n",
      "t tensor([0.2798, 0.2796, 0.2791,  ..., 0.7839, 0.7840, 0.7840], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1384, 0.1460, 0.1374,  ..., 0.1966, 0.1982, 0.1886], device='cuda:0')\n",
      "t tensor([0.1696, 0.1689, 0.1668,  ..., 0.8651, 0.8650, 0.8650], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1394, 0.1461, 0.1373,  ..., 0.1969, 0.1983, 0.1891], device='cuda:0')\n",
      "t tensor([0.7236, 0.7236, 0.7235,  ..., 0.6560, 0.6561, 0.6562], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1449, 0.1467, 0.1368,  ..., 0.1978, 0.2011, 0.1889], device='cuda:0')\n",
      "t tensor([0.6936, 0.6936, 0.6935,  ..., 0.7810, 0.7810, 0.7810], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1874, 0.1677, 0.1745,  ..., 0.2748, 0.2778, 0.2955], device='cuda:0')\n",
      "t tensor([0.0778, 0.0778, 0.0777,  ..., 0.9969, 0.9969, 0.9969], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1389, 0.1460, 0.1373,  ..., 0.1967, 0.1984, 0.1887], device='cuda:0')\n",
      "t tensor([0.9165, 0.9165, 0.9165,  ..., 0.3030, 0.3032, 0.3032], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1332, 0.1454, 0.1371,  ..., 0.1957, 0.1959, 0.1868], device='cuda:0')\n",
      "t tensor([0.4204, 0.4204, 0.4205,  ..., 0.3603, 0.3601, 0.3601], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1428, 0.1466, 0.1369,  ..., 0.1972, 0.1993, 0.1883], device='cuda:0')\n",
      "t tensor([0.6500, 0.6501, 0.6502,  ..., 0.7238, 0.7240, 0.7240], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1900, 0.1679, 0.1737,  ..., 0.2759, 0.2778, 0.2916], device='cuda:0')\n",
      "t tensor([0.2068, 0.2069, 0.2069,  ..., 0.8895, 0.8895, 0.8894], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1405, 0.1468, 0.1374,  ..., 0.1973, 0.1983, 0.1887], device='cuda:0')\n",
      "t tensor([0.0011, 0.0011, 0.0011,  ..., 0.9777, 0.9777, 0.9777], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1320, 0.1454, 0.1370,  ..., 0.1954, 0.1952, 0.1859], device='cuda:0')\n",
      "t tensor([0.0587, 0.0587, 0.0587,  ..., 0.9461, 0.9463, 0.9464], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1358, 0.1460, 0.1373,  ..., 0.1961, 0.1968, 0.1876], device='cuda:0')\n",
      "t tensor([0.1505, 0.1516, 0.1547,  ..., 0.3629, 0.3629, 0.3630], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1385, 0.1460, 0.1374,  ..., 0.1967, 0.1983, 0.1887], device='cuda:0')\n",
      "t tensor([0.2263, 0.2268, 0.2284,  ..., 0.6039, 0.6040, 0.6041], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1408, 0.1463, 0.1371,  ..., 0.1968, 0.1983, 0.1880], device='cuda:0')\n",
      "t tensor([0.6393, 0.6394, 0.6397,  ..., 0.9386, 0.9385, 0.9385], device='cuda:0')\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1373, 0.1460, 0.1374,  ..., 0.1964, 0.1980, 0.1884], device='cuda:0')\n",
      "t tensor([0.6337, 0.6337, 0.6334,  ..., 0.2633, 0.2631, 0.2630], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1384, 0.1458, 0.1376,  ..., 0.1963, 0.1968, 0.1874], device='cuda:0')\n",
      "t tensor([0.7495, 0.7496, 0.7497,  ..., 0.1181, 0.1182, 0.1182], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1363, 0.1465, 0.1370,  ..., 0.1962, 0.1959, 0.1865], device='cuda:0')\n",
      "t tensor([0.8907, 0.8907, 0.8908,  ..., 0.0620, 0.0618, 0.0618], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1384, 0.1460, 0.1374,  ..., 0.1966, 0.1982, 0.1886], device='cuda:0')\n",
      "t tensor([0.2725, 0.2721, 0.2709,  ..., 0.5479, 0.5479, 0.5479], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1422, 0.1466, 0.1367,  ..., 0.1967, 0.1966, 0.1862], device='cuda:0')\n",
      "t tensor([0.8225, 0.8225, 0.8223,  ..., 0.2745, 0.2745, 0.2745], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1365, 0.1463, 0.1365,  ..., 0.1958, 0.1962, 0.1837], device='cuda:0')\n",
      "t tensor([0.4347, 0.4347, 0.4347,  ..., 0.5438, 0.5438, 0.5439], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1369, 0.1456, 0.1383,  ..., 0.1954, 0.1956, 0.1874], device='cuda:0')\n",
      "t tensor([0.1683, 0.1683, 0.1683,  ..., 0.0104, 0.0104, 0.0104], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1625, 0.1524, 0.1449,  ..., 0.2140, 0.2290, 0.2225], device='cuda:0')\n",
      "t tensor([0.0155, 0.0155, 0.0155,  ..., 0.2109, 0.2109, 0.2109], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1361, 0.1461, 0.1375,  ..., 0.1959, 0.1947, 0.1851], device='cuda:0')\n",
      "t tensor([0.0041, 0.0041, 0.0041,  ..., 0.1236, 0.1236, 0.1236], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1392, 0.1460, 0.1373,  ..., 0.1972, 0.1980, 0.1901], device='cuda:0')\n",
      "t tensor([0.2529, 0.2529, 0.2529,  ..., 0.0449, 0.0448, 0.0447], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1454, 0.1521, 0.1460,  ..., 0.2080, 0.2194, 0.2182], device='cuda:0')\n",
      "t tensor([7.9046e-05, 7.8999e-05, 7.8867e-05,  ..., 4.1099e-01, 4.1101e-01,\n",
      "        4.1101e-01], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1691, 0.1581, 0.1599,  ..., 0.2411, 0.2480, 0.2513], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1413, 0.1463, 0.1372,  ..., 0.1976, 0.1989, 0.1900], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1391, 0.1462, 0.1374,  ..., 0.1968, 0.1980, 0.1890], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1372, 0.1458, 0.1376,  ..., 0.1961, 0.1964, 0.1872], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1334, 0.1453, 0.1370,  ..., 0.1956, 0.1968, 0.1865], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1310, 0.1452, 0.1375,  ..., 0.1951, 0.1939, 0.1841], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1474, 0.1482, 0.1392,  ..., 0.1990, 0.2033, 0.1958], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1816, 0.1591, 0.1606,  ..., 0.2504, 0.2640, 0.2666], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1801, 0.1591, 0.1608,  ..., 0.2482, 0.2543, 0.2691], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1582, 0.1556, 0.1470,  ..., 0.2117, 0.2208, 0.2112], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1442, 0.1615, 0.1616,  ..., 0.2170, 0.2051, 0.2076], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1783, 0.1658, 0.1797,  ..., 0.2678, 0.2697, 0.2803], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1753, 0.1574, 0.1587,  ..., 0.2399, 0.2533, 0.2603], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1541, 0.1651, 0.1707,  ..., 0.2375, 0.2197, 0.2232], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1739, 0.1594, 0.1655,  ..., 0.2469, 0.2607, 0.2707], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1513, 0.1577, 0.1575,  ..., 0.2244, 0.2329, 0.2325], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1443, 0.1604, 0.1609,  ..., 0.2224, 0.2110, 0.2120], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1789, 0.1600, 0.1669,  ..., 0.2555, 0.2674, 0.2746], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1665, 0.1660, 0.1661,  ..., 0.2300, 0.2127, 0.2128], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1832, 0.1640, 0.1747,  ..., 0.2660, 0.2717, 0.2829], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1372, 0.1572, 0.1502,  ..., 0.2017, 0.1963, 0.1896], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1683, 0.1563, 0.1555,  ..., 0.2250, 0.2440, 0.2440], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1675, 0.1627, 0.1597,  ..., 0.2273, 0.2258, 0.2169], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1641, 0.1565, 0.1520,  ..., 0.2261, 0.2387, 0.2324], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1791, 0.1674, 0.1741,  ..., 0.2613, 0.2590, 0.2542], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1465, 0.1530, 0.1484,  ..., 0.2153, 0.2260, 0.2213], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "[0.3525697973107502, 0.20350569114462663, 0.19294637319264762, tensor(0.1915, device='cuda:0'), tensor(0.1936, device='cuda:0'), tensor(0.1925, device='cuda:0'), tensor(0.1946, device='cuda:0')]\n",
      "\n",
      "Test set: Avg. loss: 0.1946, Accuracy: 0/997 (0%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXmYFNXV/7+nu2cfdoYdHFAQARFwxAV3TcQlaKKJENfEJZuJRt83P4y7JnlNTEzcDW4xxjWJC4qKGyqKgCA7yL7KNgzMDMzay/39UXWrb1VXdVfPdE9X0efzPPNMd1VN9Znqqu8999xzzyUhBBiGYZiDi0CuDWAYhmEyD4s7wzDMQQiLO8MwzEEIizvDMMxBCIs7wzDMQQiLO8MwzEEIizvDJIGIPiaiq3NtB8OkC4s7kzOI6IDyEyOiJuX9JUR0JxGF9fe1RDSHiI63Oc9g/e8ftdkniOgw/fWd+vvvK/tD+rbKbP6vDNPRsLgzOUMIUS5/AGwB8B1l2/P6YS/r+3sCmAXg3zanuhzAPgCTiagoxcfuBXA3EQUz9G8wjCdhcWd8gRAiAuB5AP2JqMKy+3IAtwIIA/hOilO9C6AVwKXp2kBEASK6lYg2E9FuIvonEXXR9xUT0b+IqEbvZXxJRL31fVcS0QYi2k9EG4noEuWcPyaiVUS0j4hmEtEh+nYior/qn1NHREuJaFS6NjP5C4s74wuIqBCaiNdA89Ll9pMADADwEoBX9GOSIQDcBuAOIipI04wr9Z/TAAwBUA7gYX3fFQC6ABgIoAeAnwJoIqIyAA8COFsI0QnACQAW67ZfAOC3AL4HoALAbAAv6uf7NoCTAQwD0BXAxfr/zjCuYHFnvM4PiKgWQBOAawBcpHvxkisAvCOE2AfgBQBnE1GvZCcUQkwHUA0g3YHSSwDcL4TYIIQ4AOBmaKGgELReQw8AhwkhokKIhUKIev3vYgBGEVGJEGKHEGKFvv0nAP5PCLFK/5/+AGCM7r2HAXQCMBwA6cfsSNNeJo9hcWe8zitCiK4AegNYDuBouYOISgB8H1q4BkKIL6DF7n/o4ry3ArgFQHEatvQDsFl5vxlASLftOQAzAbxERNuJ6E9EVCCEaIDmdf8UwA4imkFEw/W/PwTAA3oYpxbaeAAB6C+E+Ahar+ARALuIaBoRdU7DVibPYXFnfIEQYg80T/dOIuqrb/4ugM4AHiWinUS0E0B/pA7NQAjxPoB1AH6ehhnboQmyZBCACIBdQoiwEOIuIcQIaKGX86QdQoiZQohvAegL4GsAT+h/vxXAT4QQXZWfEiHEHP3vHhRCHA1gJLTwzP+mYSuT57C4M75BCPE1NO/4N/qmKwA8DeBIAGP0nwnQQhtHujjlLcq53PAigF/rqZfl0MIoLwshIkR0GhEdqWfh1EMLq0SJqDcRTdJj7y0ADgCI6ud7HMDNRDQSAIioi0zTJKJjiOhYfVygAUCz8ncMkxIWd8Zv3AfgWj0ufQaAvwkhdio/C6FlxFyR6kRCiM8BzE/js5+GFn75FMBGaIL7S31fHwD/gSbsqwB8AuBf0J6xm6B5/XsBnAK9tyCEeA3AH6GFcuqhhZ3O1s/XGZqHvw9a+KcGwJ/TsJXJc4gX62AYhjn4YM+dYRjmIITFnWEY5iCExZ1hGOYghMWdYRjmICSUqw/u2bOnqKyszNXHMwzD+JKFCxfuEUJY6yslkDNxr6ysxIIFC3L18QzDML6EiDanPorDMgzDMAclLO4MwzAHISzuDMMwByEs7gzDMAchLO4MwzAHISzuDMMwByEs7gzDMAchvhV3IQT+u3Abmlq5xDXDMIwV34r73A17cdO/l+B3M1bm2hSGYRjP4Vtxb2zV1kjeUdecY0sYhmG8h2/FPRggAEAkxouNMAzDWPG9uMdY3BmGYRLwr7iT9NxjObaEYRjGe/hW3KW/HrV47tGYwIGWSMcbxDAM4yF8K+7hqOaxW8X9t68uw6g7ZnK4hmGYvMbH4q6Jt1XcX1m4FUDcs2cYhslHfCnukWgM1/xTW+jDmi0jhPzN8s4wTP7iS3GXXjuQ6LlLWNoZhslnfCnueqIMgCTizurOMEwe40txV4U76qDign13hmHyGH+KO1yEZVjbGYbJY/wp7opwx5w8dxZ3hmHyGH+Ku/K6IGD/L3BYhmGYfMaX4q4SCJDtdvbcGYbJZ1KKOxE9TUS7iWi5w/5LiGip/jOHiI7KvJlm1Bz2kJO4Z9sIhmEYD+PGc/8HgIlJ9m8EcIoQYjSAewBMy4BdSVGFO0BOnjvLO8Mw+Uso1QFCiE+JqDLJ/jnK27kABrTfrFQ2Ka+djsm2EQzDMB4m0zH3qwC8k+FzJqKKO2fLMAzDJJDSc3cLEZ0GTdxPTHLMtQCuBYBBgwa1+bPUTBhHEWdxZxgmj8mI505EowE8CeB8IUSN03FCiGlCiCohRFVFRUUmPto5z53VnWGYPKbd4k5EgwC8CuAyIcSa9puUGjcxdy7nzjBMPpMyLENELwI4FUBPItoG4A4ABQAghHgcwO0AegB4lLTMlYgQoipbBgNmQXeeocrqzjBM/uImW2ZKiv1XA7g6Yxa5QLhw3VnaGYbJZ3w5Q9Wd594xtjAMw3gRX4q7inOyDKs7wzD5iy/F3U1VSNZ2hmHyGX+Ku6LcsZjTMQzDMPmLL8XdjXJzzJ1hmHzGl+LuakCVfXeGYfIYX4q7ipOHzpOYGIbJZ3wp7u6W2WN1Zxgmf/GnuOshF6IkqZCs7QzD5DH+FHdduINE7KEzDMPY4E9x138HAuQYW2fNZxgmn/GnuOvKHaAki3VwtgzDMHmML8VdEiD23BmGYezwpbhL4Q4kibmztjMMk8/4UtwlWljGfh8PtDIMk8/4UtwNzz1ASapCMgzD5C/+FHdduoNEPImJYRjGBl+Ku4SIkoRlOtYWhmEYL+FLcY8PqCYrHMYwDJO/+FPc9d/BAHvuDMMwdvhT3I1JTOQ4WYknMTEMk8/4U9z134GAc2lf9twZhslnUoo7ET1NRLuJaLnDfiKiB4loHREtJaJxmTfTnqSTmFjcGYbJY9x47v8AMDHJ/rMBDNV/rgXwWPvNSo46Q9XRc+ewDMMweUxKcRdCfApgb5JDzgfwT6ExF0BXIuqbKQMdrAKg1XPXbUw8grWdYZg8JhMx9/4Atirvt+nbEiCia4loAREtqK6ubvMHqvXc1fd2xzAMw+QjmRB3stlmK61CiGlCiCohRFVFRUWbP1BNhQTsc905LMMwTD6TCXHfBmCg8n4AgO0ZOK8jUstJeu5JjmEYhslHMiHu0wFcrmfNHAegTgixIwPnTYnuuDt47gzDMPlLKNUBRPQigFMB9CSibQDuAFAAAEKIxwG8DeAcAOsANAL4UbaMlciQSyBpzJ3lnWGY/CWluAshpqTYLwD8ImMWuUAt+au+Nx3TgfYwDMN4DX/OUFUKhwHAy19uwcY9DbbHMAzD5CP+FHelnjsA3PnmSpz34OyEoxiGYfIVX4q7RMbcAaChNWrax547wzD5jC/FPZ4Kad7+4vwt8WM60B6GYRiv4Utxl8hJTJKbX11mvI45FZ1hGIbJA3wp7mrhMMdjOsgWhmEYL+JPcZd57oEk4s7qzjBMHuNLcZck0XauLcMwTF7jS3F3E5ZhbWcYJp/xp7jrv5N77gzDMPmLP8VdmGvL2B/TUdYwDMN4D3+Ku6tjWN0Zhslf/Cnuum4nS2Vnz51hmHzGl+IuSVbW167GO8MwTL7gU3HXhDuaRMBZ2hmGyWd8Ke5S05M656zuDMPkMf4Ud/13stALD6gyDJPP+FPcXeh2e0LudY1hLN1W2/YTMAzD5BhfirskmiRdpj3ifslTczHp4c/bfgKGYZgc40txl1kyycMybWf5N/Xt+GuGYZjc409xl7+T5rlzzJ1hmPzFn+LuIlsmE9LODQTDMH7FlbgT0UQiWk1E64hoqs3+QUQ0i4gWEdFSIjon86bGkZkwyTJiMqHLvJgTwzB+JaW4E1EQwCMAzgYwAsAUIhphOexWAK8IIcYCmAzg0UwbakfyAdX2KzN77gzD+BU3nvt4AOuEEBuEEK0AXgJwvuUYAaCz/roLgO2ZM9EGN7Vl9N9bahrRHI626WPYc2cYxq+4Eff+ALYq77fp21TuBHApEW0D8DaAX9qdiIiuJaIFRLSgurq6DeZqxAdUk4dlItEYTr5vFn754qI2fg6rO8Mw/sSNuNsVTbeq3hQA/xBCDABwDoDniCjh3EKIaUKIKiFEVUVFRfrWGuexN8JsoEBEd70/WdO2hoSjMgzD+BU34r4NwEDl/QAkhl2uAvAKAAghvgBQDKBnJgy0wxhQTVHyV+5PsmBT8s9hcWcYxqe4EfcvAQwlosFEVAhtwHS65ZgtAM4AACI6Apq4tz3ukgIpupOO6ud8DOKTnJIttZoMLhvMMIxfSSnuQogIgOsAzASwClpWzAoiupuIJumH3QTgGiJaAuBFAFeKDkg1GXdIV2y691wc2b9Lwr4N1Qdw5TPzAQDURt+dpZ1hGL8ScnOQEOJtaAOl6rbbldcrAUzIrGlJ7LG8t/PM//bB2qT73cCeO8MwfsWnM1Sl6GqqTW1V75Sfk5XTMgzDZB1/irv+W2p6KmkPtFH8eRITwzB+xZfiDksWTCCFdnO2DMMw+YY/xV1HhmPa6pmngmPuDMP4FV+Ku3XmaCptb6v2s7QzDONX/CnulrBMqgHVtg64sufOMIxf8be465qdKubuxKY9DfhifU2SD2rbeRmGYXKNqzx3rxFPhCTTbyecHPdT//wxAGDTvefa7ueqkAzD+BVfeu4Sw3NP8V+0OVuGXXeGYXyKL8Xdmn+e2nNPT97l4ey5MwzjV/wp7pb3KbNl2vo5GRpQbQ5HUTl1Bp6dsykj52MYhkmFP8U9YUC1bTF3x+Mtn9Ne6pvDAICHPlqb4kiGYZjM4Etxl767MaCaUryTH7Bu9wHc9eYKxCxxmEyJu2x8OMzDMExH4Utxz7Tnfs0/F+CZzzdh895G0/ZM5bnLj+e8eYZhOgpfirvEbZ57KsdextatMfZMSbH02K09A4ZhmGzhS3FPlMj2ee5RKe7G8TKMkhkxjhmNR0ZOxzAMkxJ/irtRfkAWDkt+fKpUyVgsft7VO/cjGsusGEtx57AMwzAdhT/FXQ6oynru7SwcFlPCMmf97dP452RIjGVjwVEZhmE6Cn+Ke0I99xRhmRTnMzx16+ekbZk9smfAnjvDMB2FL8Vd4j5bJkVYRg54WsSXY+4Mw/gVX4p7gka2c60O4SC+mRLjKMfcGYbpYPwp7pYFstub5+4kvhnz3GMs7gzDdCyuxJ2IJhLRaiJaR0RTHY75ARGtJKIVRPRCZs10skv7nTJbJpW46+I76+vdpu2Zy5Yx/2YYhsk2KcWdiIIAHgFwNoARAKYQ0QjLMUMB3AxgghBiJIAbsmCrQcJKTCmO37q3Cc/P2+y4X3rWf35vTfuNsyHKqs4wTAfjxnMfD2CdEGKDEKIVwEsAzrcccw2AR4QQ+wBACLEbWSSeCmkflrHz5G95bbnj+Zy0N9MDqgzDMB2FG3HvD2Cr8n6bvk1lGIBhRPQ5Ec0lool2JyKia4loAREtqK6ubpvF9uc1vQ8F0xtKcBLfTE9iYhiG6SjcqKBd1MOqViEAQwGcCmAKgCeJqGvCHwkxTQhRJYSoqqioSNdW5Txmw6wx9VCai6o6iW+mRJnDMgzDdDRuxH0bgIHK+wEAttsc84YQIiyE2AhgNTSxzwqpFsgOpinuTuKbucJhLO4Mw3QsbsT9SwBDiWgwERUCmAxguuWY1wGcBgBE1BNamGZDJg1VSbVAdvqeu8PnZCzmnpHTMAzDuCaluAshIgCuAzATwCoArwghVhDR3UQ0ST9sJoAaIloJYBaA/xVC1GTL6P/59xIAzgtkpxtzdyJjk5hY3RmG6WBCbg4SQrwN4G3LttuV1wLAjfpPh5MwoJqm5+5EpjSZ67gzDNPR+G6GaiQaS9hmlfJUM1YlqWLzHJZhGMav+E7cD7REjNdOhcPcDqgGXRYUay9RHlBlGKaD8Z24729Wxd1+sQ63UZlUDr7IUL4Mh2UYJvsIIfBNbVOuzfAMvhP3uqaw8Tqe526ZoerWc08ZlknLNEc4FZKRzF5bjRP/+BGaw9Fcm+KKlkjUNhTqRV6YvwUT7v0Iy7bV5doUT+A7cTd77ubfkszF3NMyzRHOlmEkv5+xCtv2NWFDdUOuTXHF4be+i4unzc21Ga5YulUT9WXfsLgDvhT3cMI2a56727BM19KCpPuzUVvGDx7bV1v24Xdvrcy1Ga6ZuWIntu5tzLUZriguCAIAmnxwH0gWbt6XaxNc0blES/6rt9GIfMR34t6trNB47bRAtpPnbs1+6VZaaHuccXwb7LNDddzVnodXmTJtLp78bCMaW71vKwD85LmFmKisfetlSnRx90Mj7ze6lGjOmhq6zWd8J+7HVHY3XksNDwbNYu60rJ41PJJ6+b3M15bxg1dRXqR5QDUHWnNsiXsaWv0hlsUF2iPX5BN7/YS8b1ncNXwn7ipSmotDQdN2pwmqEWvsO5V4Z2FAtd4HN570gPY2eF/cMzUXoaMoKfRfWAbwR09D3gks7hq+Fnep7jKOKXEKy1g991TjnNmIudf7ICzTRR+LqGloybElqUlosD2OvFcbWrx/H6gc8IG9kah2L7RG/JHdk218Le4y5l5SYP43nMItqhA88MHalKPqmcuWib8+4ANx76p77n4Iy8gH2i/ImLsfxFLFD557OKY9aH5J3cw2vhZ3SaLnbn+cOpnorx+kXlIv0wtkA0Br1PsPSXmxfwamIjF/PchFegjRDz049b5tDnv/OsuG3m+9uWzha3GXDrqMY0qcwjLpfunZqOf+2MfrMWt1VlchbDcFeuvoh0E/v80hkPdCkw8ykcJKw+kHz1167ByW0fC3uOu/i6wDqi5j7qnI1GCdWltmza4D+NEzX2bkvFlDv3yNPnigwz4Ly8iehh8GVNWQV0vE+/aG9ee7mcUdgN/FXRdxq+fulOEoHyxVtJPNUs1Y+QGfeZeyEfSb5+6HzJn4tfW+AEV8F5bRbGz2wX3bEfhb3PXfxSHzv+Ek2PLBalUGXAqCzuKesXru3tccExEfiXtY+S79kOsuexpNYe+HZdSBST+EZeLX1vu2dgS+FndJoUXcrQOsEilaale+wLqMk0KmqkI6hYOWbK3Fdx/93HMPTlS/Pn4Iy6jX1g+ZSH7qFfnOc9d75l57nnKFr8U9XjjM7H2XOIi7fLDCSkwu1CGeu/2J7pi+Aou21GLljvrMfFCG8JPnrgrQgRY/ZPf4x7sM+8xzj7DnbsLf4q4HZqyxVkfPPSo9d1Xc45fAujxf5lZisj+P3O62imVHER/0874nrKZC+qFujwx1NPnAE1Z7Rc1+GFDVn28/NEQdga/FXQbdj+jbGWMGdgUAnD68F0oK7f8t25h7Bwyoyo+zfpS0J0NLvmYMX4UOoqrn7gNxN66t921Vw5d+CMtE9YY+HBUmBy5fcbVAtlchpfzA67+YYGz/w9urbI+XKYlqHqy16JhKxlZi0j83FAiYGhbpGHnOc5cxdz+Iu89i7nHP3Q/X1l9hmbBpjCCKAqciU3mCq/+eiCYS0WoiWkdEU5McdxERCSKqypyJ6eMUllFbdok6oGrV2A9W7kbl1BnYtq99tcJlKqR17FaGfbyWwRf1UVw46rewjJ9SIdU8dx/cC+bsHu9f32yTUtyJKAjgEQBnAxgBYAoRjbA5rhOAXwGYl2kjrchURyd/12lA1T7m7uw1z1i2AwCwtJ3LdkUVz11FevRhj02hlx5biw8eELWh9kP9+aivwjKKWPpgYlAkavbc8x03nvt4AOuEEBuEEK0AXgJwvs1x9wD4E4DmDNpni5yB6lQgzFpITLKjrhlz1u0xhUasgpsNYjax9V+/vBhrdh0A4L3iV3ZjE15FHfRr8ZEANYWjnp90FY35SyzVsIwfep3Zxo2y9QewVXm/Td9mQERjAQwUQryV7EREdC0RLSCiBdXV1WkbK5F67ORzV/Yss91+w8uL8cMn55li7skmMWWKmNB6G2pj9Nqib4zXXqtiJ71hX3TF/Sbueq8oJrzfeIZ95gmrz5EfkgGyjRtxt1M/41snogCAvwK4KdWJhBDThBBVQoiqiooK91ZakJ67U4rhqYf3wgvXHOv497WN8VK2oQ4YdIkKgQA5l0UIe2wKq/TYfCGWygPth/onpolBHo+7mwdUvW0rwGEZK26UbRuAgcr7AQC2K+87ARgF4GMi2gTgOADTszmoKmPuyULVJxza03HfjGU7jddqbrt1oW1Je3vPsZhAgCghj17iNc9dPtSRmPB81UWT5+4zAWr0+DwCJ7Fc/k0dKqfOwGdr9+TCLEcisRjKfLrSVTZwI+5fAhhKRIOJqBDAZADT5U4hRJ0QoqcQolIIUQlgLoBJQogFWbEYwJ2TRqKsMIjSIvuBU8n8356Bow/plrD9zSXxtinZgKqkvSmRMSEQDBBKC+0zT71Wf1oVdK+XTzVXLvS2rYD52no9dCDvy1CATAOq8zbuBQB8sGpXTuxyIiqAMn0dVfbcXYi7ECIC4DoAMwGsAvCKEGIFEd1NRJOybaAd3xs3ACvunpgyj7VX5+KUXfWOGFCNxrRcdnnjWfHagKo5ju3th0QNHXjdVkDLjCrU71uve5cyzbS0MGgSSzkQ7LHpGRBCGM+Yem3nbqjB23rmWz7hahKTEOJtAG9btt3ucOyp7TcrczS0JH+AOmZAVYu5lzv0NCKxGKb+dynKikK47byELNMOx0+eu7SVyB9x4WhMoLw4hL0NrZ73LmW0sLwoZDu47hTGzBXRmECpDMso4xmTp80FAGy699yc2JUrDvopXNMuOxr/e9bhjvvdeO7tjrnrYRknzz0cFXjpy6146rON7fugDBGOCsO79HqoQ/Z6ygpDJs89FhOYs95bMWFAF3f9PlBnAM9eW43P13nLXjk/o7QoZGo45fPgNc89GhMcllE46MV9aO9OOH9MP8f9pph7lm7WqD6gWu4YlvGWgEZjMWM8w+uhDhlCKi0MmhqiJ2ZvwA+fmOe5JQ1jigCpMffLnpqPS57M+vy/tJDhl9LCoKlwmByD8pi2QwgYz5jXQ14dwUEv7kBy79xNXZf2RsRjQiAQIFx14mDb/V5LhYzEBMr0wV9VMJ+ft9lzYinjwmVFIVO2zPpqbYLYzrqsz6lLi5iIh+e8LkBRpeHce6AVdY1aSWXPeu5CoLgggACx5w7kibgnW0ovHV5btA1zN9Sk/XfRmECQCGMHdcMVxx+CLiUFpv3e89yFsXShKu63vLbcc+u/ynaxpCBo6mVIAfJcxU1l0O+/X33j6SUYpbiXFYawvyWCo+5+D0Dc2XGaIZ4rtLEtQkwAD320znOpmh1NXoi7U365W2T39NcvLzEGZ9IhJuIiEwwEEh5or+WSqwNTi7bU5tia5Kje5a76FtQ3696lvt9rg35qWObTNdV48cstObbIGdlAWtcoNjz3DrYnFXI+ieSF+Zsxrw3O2MFCXoh7IMfuWywmDBtCQUrIa1eneW+vbepQ2+yICWEUX7vnrZVYtGVfji1yRs5SLikM4pvaJhx9z/s5tig5USFQrsx32HugNcnRuUUOqJZZ5mcY8z48pu5RPXFBUhAM4OI2OGMHC3kh7naeu2zgO8JnVm+6YIASPHU1LHPCvR/lfBBTzToAtIJrf31/TQ4tckZey07Fmr2yoRRx191TqCEvACgIefcRNHpFSgpvOBpDfZM2s9Z7vSLzGFpHzGHxMr5erMMtdjH34lAQTeGoY30alfaGTbSwjC7uRKaJN0DigGokKuCQWJN1hBCIibhYAsC8DTV49ovNuTEoBdK77FQUH8dYsb0O7yzXJq14S360RkcVoMJgAHM8lgIpiSnZMpIbXlpslML2WMjdmE8iaW841u/kRdNm9yUX62WBoy5mh7Zb3GNCiblTwsLb1gHVaHsT69uBtK1zcUHyAz2CHL8oVxqjcx/8zMgh99qgXzQmoE6srm1sxQ89lgIpiRnjGfFrO0OZ6emtKxufTyKxrrLmtbGtbJMX4m7ruesxZSchVf9m9/4WrN65v82frz3QeszdxhZrDD6XGRTyAehcEn+gvSaQKuosSj8QFcI0BuS1NFgV6feUFSav4eQVojHzvRq03LfWHvPBjj+eiHZiJ05FeqzTqTUvCMZj4/fNXI37Zq5u8+fLFC3Afs1W69TuXBYSi3fF47eGlxcbNgb9HEo7RD32QMf0tFiJlyMHhufu0HB6zRPWPPf4e+u1zWW4Mxfkheduh+G5O9RRKcjgYIxJ3G0aGutC1F7w3NUehrW+jBACs1bv9kSOtgx5OQ3uhT1WlM0aOvBa0TgVu5i7iievrfJ8WZ2kbF7rV79q2xyYbJK34l5kI+4qmcxiUMMydiGiRovnvnb3gZwtwSY9YdVOq+f+xuLt+NEzX+L5+bnP0bamv1nx0gQxOVit9iQ7UiDXVx/AP7/Y5Pp4eS8UOlRfzXavqKElklbvIBoTpmtrLSQXjsWwbveBrGSj3fhK2+bAZJP8FXddvNU43DvXn4TzRvcFkNlqkeokpkKbRsNa1/uSJ+fhidkbMvb56RBf79VZgL7Rc/E9kZNvmbhiJdshrr0Nra4FSB5m9i7NAhSJxvB/b69C9f6WjNkoufCxObj9jRWuxU3eC06NZ1hfzCUblUPrGsMYecfMtMKhQl/O8t0bTgIAUz0cAKje34Iz7/8Et762PKO2epW8FXe7sMwRfTvj2MHdASTPkX1+XnppgTFlEM2uBr3dog3z9QUR3FA5dQZufyMzN2zU5oG2rvVpt+B3rpC9Iid9b43G8Md3v8YNLy3K+Gc3tEQw7p738fsZq1wdH7+2wFkje2v2WYTx49XV+PunG3Db65kXoFq9Nsz+ZncrQMmv3WkSYCQawxVPz8ewW9/JiH0q763UVkt7/JP1rv8mqofohvfpjF6dihLGsvY1aBPG5m7MXvjESz3FvBX3Av2GtXp2sluXzHO/Jc2WP6oMotl1ca1hGSD97vo/M5SHLrviaqEzqwDJS3agOZLzeu9yDsHYQYkf2eRMAAAgAElEQVQrbgFanPWxj9fj9cXbbfe3h9omTSxfW7TN1fEx5do+MHksgMRrK++Fd1fsxF/ea/sgfjLqdbtTIe0tcghRRqICn2UpR18tquZ2QD9qcaKsYRnppLgpFthWbp++ImvnTpe8FXd5E0gvdEhFGYD4xIxUqzylgzqgah+WSfSk3N7QmY7NyyhBkAg/O/VQW1vkQ//sF5tx1bO5LSQmJ64cfUg3XFw1EJ0s6RDZ9KRklUS3oR9D3ImM+8saOlDF/qGP1mXCzATq0hB3IuC4wT3QuTgxzSSbIS/1OrjtaQhlQLUwFEiouinPGRPZWxv4ta++ycp520L+irsu4pGYwJLbv40ZvzxJ3+4cPmkrsRggozy24m7jubsd2c/0TRofUI1fA6u4qw3K7AxX3ovFRFoNljpY3bWsICGEpOaRv7diJzKJFEm334ERliEyQknWRb07ovREveuwjCaWgQDhxm8NS9ivjhcc+4cPMjpOoDYcbhujqDL+UhgMJIQ7ZYXTrXubcMK9H2bIUjPq/JBck7fiLgUhGhPoUlpg1PuQou9m4Wy3qBkd9p57oncZdpmJkMp7ag5HcfWzC7Bu9wFX51MHVGVoqtXS0GRzBu31Ly/G0Fvcx3DVa1sQCNikv8Wv47XPLcS63W2fjGZFio5rz90Swy4IBBJWurKKfTZwH5aBUvAu8b5VQ4e76lsyVus/GhOYroTR3Pc0lGsbooRekepE7arP/IA1APTvWpKV87aFvBV3GVu3el1yoNWNfq3aUY+HP1qb8jhTWMZ2QDXRk3LruacSlrkbavDBql2402UsUB1QNTx3iwBlszv+5pLtiKThvavZMqFgYlE269hFptqlaExg9tpqAO5DP0avSHEgrJ66teeRKUbe/q7xOp2wTMAIUyY6O9Zr7ZQPny7/mLMJK3fUG+/d2Gsd5C8IBhIaSrvEhUwhP7fW5bXtCPJX3PXfTuLupqv9zrId+PN7a1LeNKoA2Xnu9gOqlsGgSAyn/fljPPaxOXsgVW0c+X+4LXus5rnLiUx2tqjIVY/ay4JN8Qwht6sUqWEZu1BatmbX3v/+ajw/T8vzd7sYTMwyhyAUoATPPWGA1abhbwsNyj0qa96nQk0EcHNtDzRHMhJW2rq30fTeTU8j3nCqA6rOnnsm+ev7a4wkg70N3inhnDfi/vU9E1FcEDDqlEuxtYYYpPfhplqkDFfsa0z+hZrCMjYPid1HWb3j+uYwNu5pwB/f/dpyXPwB+8Pbq0w9icbWCD5fp6V9ua2Qp4ZlSA/NJIQOLO/P+Msnrs6diose/8J4LdP2UhFVekV23qW1B5SpSUOLt8YXMXFb+lZeW0oiQFZxHHH7zPaYaYss2ZsKNYXXLixjvbZTX12GX77Q/pRTa+5/qucLMGciAXrM3XJtrTPBM8UDH8afuf3NkZxNQLTiStyJaCIRrSaidUQ01Wb/jUS0koiWEtGHRHRI5k1tH8UFQXx9z9n4uZ4BIrNirDdoqc3yck5IzyVVa63Wmbbz3JOdW+IUplF7GNM+3YA/vxevu37jy0vw9OcbAbj3Lq0zVO0EqCPSH92Ke8xUlM3Gu7ROEsrQrEr1GrRGY65CM9ZrG7JrOG1i7pkWC9dhGaXHWWBz/9j1bt9buat9xgEIR+LnLQoFEjx5O4zxDOU5S/DcLb2gR2aty+i17VJSgGhMuNKOjiCl0hBREMAjAM4GMALAFCIaYTlsEYAqIcRoAP8B8KdMG5oppKjLm8DqocuwjJv4nBTgVJ6FOXbpTtwTPU77GyZZ/HuhsoJSgIDl39QZEzmciCqeu7S3I8Td6rHOcznRJCqUhsim4bR66pMe/hwvZaBsgnWQ2S60ZkXNlgG0xigxWybx2rZ3jMMqYK7DMkqP03ZANUvlB9R7fVD3UmyqSS3uapYXoPXirJfN6rnfN3N1RjN85NrIDS2ZCaW1FzdKMx7AOiHEBiFEK4CXAJyvHiCEmCWEkN/AXAADMmtm5pBdYsNzTxgU0lKZrCPtdrj13NW4sNOEELtzz9tQY6yApN7wqhAmGxtQ9zW0RHHeQ5/hgkc/T/q5Rp57IB7qaE4Iy2S+e2vNZb7rzZWu/k6tlW/nXdp51NM+bX9pB+sg8/wNqWcUW7NlQsHEjI4Gmxh7extT62SefQ2txmBwMqJKj9MueyxbhbjUQeVDepRiiwtxV+cQAO5ngmcyN0CK+y9e+MqYA5FL3ChNfwBblffb9G1OXAXANpeNiK4logVEtKC6OvXNlQ2kaHUvLcRxQ7rjbxePMe2XYZlUnvuybXV4cb52WVKFENTYpduwDABcPG2uEc9TPdCGFnez91RhkzMJN6d4UKweUGEwkCAudmLT3gqRdhNVht3yDmpTjWekHFBNtCsTz7P1ul/9zwUpu/hxAdLehwKU4Lk3tiTed+3t5lsbjDnra3DZU/PxxfrkvSOhlNC1GyvK1mC1et5enYux50Bq79paE8k2K81uPolN72P+xr1G/aRUqM+CFPe5G/bi/vezM7s4HdwojV2w1vYuJqJLAVQBuM9uvxBimhCiSghRVVFR4d7KDCL/GSLgpWuPx4TDepr2l7iMuX/n4c+M1ylj7iL5TZcKIYQplXH22mrMWb8Hm2sacMPLix3/ri3deWtYxq4xsrs21hS+b2qb8MznG13Hd/croYJDepQa50w1SUpNM7ULy9g9vG4Gy60IIbChOl6t0y7l7f4U68zajWdYr5tdeMeuMa050JI0zj93Qw0+W7sH4WgM1/5zge0xqQq/qZOCZLjSut8N+xpa05pApjbIPcoKUdPQmvI+MoqyJWno7QZUrfeyEAI/+PsXOF95vpOh3rdS3AFt9rbbBiJbuFGabQAGKu8HAEgo1EFEZwK4BcAkIUR2ZghkgHNH90VJQRAXHzPQdn+JzU2cipTZMjFh5Da7LSWsPjafrduDL5Ra0de/tBiXPzUfp9z3MZZuq7P9+9lrqx2zA5J52dZ0vaJQ4vWwE/eWSAx1TWFDcJ6avRF3vbkSbyw2T8eOxoRt3F/13Pt1iU8EWbG9PuFYq71SgOxCXnbCaKftL3+5JWlK553TV+D0v3yCFdvrsXRbrW2s9onZGxLGJ0y2WhrOolAgQSAbbeK11jBYbWMrjv7dB7jzTee5C5OnzcWlT83DjtpmfLUlntlz6uFxp2rbvhTirlxbu+fCrfNwyZPzcO1zC3HAZSxa/c56lBUCAMbd835yWy157m6qrwLaAPYdbyw3igFKR23PAXcpjS99GQ9qWGen7vCBuH8JYCgRDSaiQgCTAUxXDyCisQD+Dk3YMzNNLUsM6FaKVfdMxGG9Otnub0vZgb0NrZixdAfG//4D266qKSzj8vyqAC3/JlHgUj1Yby5xLpR1IEnutHXQr6jAnQfU0BLBUXe9h7vfWolwNIbFW7XBXOtDcv/7qzH2nvcTBF59SPops/zqmto3nmHXFReWjqcQAv/vv8uSpnS+oV/P7bVNmOcQX28Ox5J6a1bP3c4bbrC5ttYGaqbuBf9rbuqB4dao+XzfGzcAlx+vJbMt+8beMZDIErpAvEer4hSWeXPJdlROnWF8x3JCkpt89elLtpuKkXUt1cQ9GhNJQ3TWVEhbzz1sM54RjeHZLzYbxQC36Jk5yZLLWiJRxGICzeGoqSRx5xLzusNuG7NskVJphBARANcBmAlgFYBXhBAriOhuIpqkH3YfgHIA/yaixUQ03eF0vuCkoT1x/w+Ocn18bWMYv31tGXbvb0FtYxg76prwwAdrjS68KaXMZVkDNZxQ4yLmqBKLiYTshkP1wmhA8kJMhneZpDGym1ErvcAZS3fglteWGd6iVcTfXa4J0466Zjw7ZxNaIzHUNYYxc3m8237i0B7G61TjGVFlyrmdWNoOosWANbv2476ZX0MI4Sr3vUwfaK9tDGNTTUPC/p+cMgQAsLOu2fEc1nQ9uxmddpOWWiIxVE6dgTv0ss57G+LXxC40ooZrWiPm/UWhAO4+fxQuPW4Q5qzfkzRuHlUGq+1sdQrL/O0DLTy1s958LZzuu4aWCK574Susrz6AL9bXmIq/qZ9x1bP24SXAZkA1lPic2TklexvMz5ZsnLvrPQYrTa1RjLnrfXz30c8TemnWReXd5OdnE1dupBDibSHEMCHEoUKI3+vbbhdCTNdfnymE6C2EGKP/TEp+Rm/z3FXH4nvjkif8HDWgi/F6b0Or8ZC0RKL45QuL8NcP1uBrfVHtqFKtzu1i06q3lu6st5ZILCFzRPVkknlQVu/SznO38y437dEEr1tZIV5fFO817GtsRSwmjPQwacerX23DHdNX4OFZ67B+zwFT7LlP5xIsuf3bOLJ/l9SD1UrIy94TtheUCx+dg0dmrcf+lojps60PbHM4inA0ZizAvbexFXsOtJjiq5vuPReTjxkEIIW4WwZU1XVqJXYCVKN//8/qZZ3VMM2iLftww0uLTPdLteIM7LekPcrGekTfLmhsjSZNBVRL6NpdW6eGwemcTimYK7bX462lO3DhY3PQGomZPOBz9cVzAGDh5n12f67Zaulx2jklzTbXdv1uc0N9QG+AnOaF1Da1oikcxZJtdQk9qt6di03v1UY4F+TNDNW2cOd3RuDiKvvY/BlH9DZe1zS0GA9lU2vU8FBkaEUtaOQW9SHfo4j7EMUDd6IpHEXQMqFHvVkXbNrrmNlhHVC1i7nb5fFu1L3Z7qWFJrF8a+kO3PL6coy8Yyaaw1FD3GUbt3Dz3oSMkcIQoUtpAXp3LsYXG2rwcZKCVCnDMjYPtBAC+/X/oSUcwzRlQYjaRm0msBSi4be9i0uenGd8xr6GVrREYuhkKYHbp3MxiIA3lzqHw6wLodgJpl3MffVOLaxhN8HuZ89/hdcXbzcJ3z5FVHZZhFZe/96diwAAS7bWOt4L6mLedjF3pzEdWXXS2lBaGxqJdF5qG8NojcZM8fLigiAW3nomAODw3vahVCD+rCVdFMcmRLd1X6PtMXadklhMmO4na4qp+oQHSLtXcjlblcU9CVdOGIzLjrefbKuKrFphrrE1anhoUsDUXOy2oIZlyl0s394UjibcyN8/Ot4Tue2NFbjXUsZAYh1QtV1cxOahlrMIu5QWJOx7UZ80VL2/xQhLHdBT/jZWNyQMGMoHc2e91kW+8pkvHTM71EE/W8/dJrVQfXC37mvEg0rd9H2NrTjtzx/ju4/E5wPM37jXaLD2NrSiJRxL+B5KCoOYMn4QPl5djd319t571BIXtgt12PWK1uzSBnr7dC7Gqh31pswT6T2qDa56Pa22yHz1Xp00L/Nnz3+F5+ZutrXXug6BtYRFqun8LZEYFm6Oj084hWVqlNBIaySacM/1KC/C5GMGYvWu/Y4T0NwMqNrZq/YMl22rM+ZX2InyyDtm4jsPxbNoZA9J3tMxIfDFzafjw5tOQbfSQjw8ax0uf3q+rb0dAYt7Cpzy0g+tKLfd/sTsDVirl9eV3dZITDjWdnETpalRBiXdVN6rbWxN6AJfcUIl3rn+JOP93z/ZYBtCkE53sgFV+8/UPi9ZDZstexsNYZWx+OoDLQnZN1Lcu5cVGduWKLVcVGLtHFC1Do5KD3h9tbm7LkW0rimMlkg0Ib4KAJceqzkCTut+xiyhA7dVFGXjXhgK4OfPf2WyTaYIquEn9XrK79gqmL06x6/tO8vs0xSjMXOPM91MsuZwFBc+Fq8X5FRHfq9yf7dEYrbPnAx5TH11mW0GlF2aqRW7LC+Z0hogmOo22Y3DNIWjpsZ3l95wSqciJgT6dinBoRXlhj2ZXu8gHVjcU+CUPWONr0neWrrDeC1vwuZw1NarBNw9MKpn48Zz31HbjP3NEZN4EBH6dzPXmj73wdkJfytXtJdRHbczat3UNr/kyXlGwS3ZFQ9HRUL3XXpCf/3BUXjmymMAaB6mXbaEOkXe6RpbUU20DqjdqqxdWjl1hvFafpdN4ShaIjGU26xMNKJfZ0wc2Qf/XrjN6K3Yfa6RXuhS3KUAERE27kkczAW0xkj2nlQRk2EZ60B+D2XA0KlMbUyZxAQAxWmW9LWuIVBd34z65nDC912jhB1rG8O299yhveLO1Kodidljwugtyx6nu65ynXJt1clSdU3hlBU5ZcMZF/f4PnUGc65CMyzuKbDLbnnxmuNQ7MKjfeBDLWOmJRJzFEk34q56EaqAnTS0p93h+Ka2Cfubwwkj/p2LC/Dfnx2PwT21kFJNQyt27zd774bnnuaMWpkZkCzPW0VtsKwDcLJB7VFeZMrLtltwRM1EcvOdyL8x7HCZzywzPxpaImgORx2/t5OGad/Jza8uS9gXL7+svXfrCcup7Mnk6sEP1+KkP83C5poGTPs0PoawS3rulu8xFAxgwa1n4rzRfbFm135bIYspiQBA+vXaN1gaolmrqzH6zvdwxl8+wf3vrcaOOi3Upjbau+qbbe+54X3i8fa1NveBtcfpNqVZJhcQEpMdHvpoHW58eTE+W7sHT85OLFmxS392SmzKhKtOTq4KibG4p8DuRutUHEoYaLQT2tlr9xhfbJHDg6yK9a/OGJrSHvUGvP8HY2yP2V7bhP3NEZN3Jjn6kO4Y1T+e6TP+9+blxqx1se0GVO2QYRa3q/yooQVrbrj6YKr/7/a65oQJWKrn7tZWNYSheo1dShJDLVYaW6NJG+vzx2iVOexW5ImJtoVlZMPpZkbot+7/1CjzDMQFyO4+7llehO+N649oTGDJ1sSc92hMmK5/umEZtdHuVBQyMqq+qW3Cgx+tw22vr8D+5rApXCPFfXifTvjeuHiVk0MryvHtEVoSg5xDoRIfK9LeuxV32bAQJTaej328Hq8u+gaXPjUPv5uxKuFv9+zX/rbMpjetirvbQm2ZhsU9BXZlZInM2SdXnlCJ28+zFsrUkA+mo+euPODXnDQ4pT3yYwd1L0XPcvtc3JqGVlvPXWKNv6rdRmueu9uwjIxF7m1Dbu9ufUDaaXHy6/VG71cvLsLflNrZgLm4lbrASDLULAc1zbSyZ+pMpIbWiCbuDr2E8qIQpowfZFtczZotU2KTCmnHPt1zd7PYhLWcgYwLHzdEmzvQs7zItH/swG4AgEUOgqne527DXhJV3HuUFxoZSpIPVu3CuHvexydrquPJB0K7P9+94WST8xIMEKZdXgVAm7y10jJzWV5bovR6nDIkFY6KtBfzkGGcqWcPx6XHDcJFStLCId1Ljdfjf/8hKqfO6JD1cVVY3FMQTDHi2b9rCe6cNNI0q1JFdvvdxNwDLkZX5RFXnzTYMWe+rimM/c0R04Ckys3nDDe9Vx+6hHzhNAqdAeY0PLfI0JBsSKw5xnKCEAD8Z8FW0z4hzJlI6dqrins3m0wfK02tUbSEoygKBfHk5VX48KZTEo6pKC/EngOteMViq3UWZbqecLriU1IQNBqy355zBN694SQc1sucCNCtrBCVPUqx/Ju6hNiwuhITkH5YRs3U6VFufy+GowLRmDD1dJx6uSrnPDjblCJr7RVZHQSnR0v9l53GM5yQ4t6zvBC/u+BI0zP+r6uPxffGmusr/nvBtrTO315Y3FNQVhREp+KQSXCkCL/9q5Pw5i9PBOD8oMobwE3MvbQwiIurBpq6o1ZI8VKd2NfQisbWKLqX2YtVz/Ii/PSUQ433o+98D6/o0/8TJjG5DHVI2jLlWqaSypoy1gZVneyzva4Zz32xyXiv5rkD6ZePUENC3UrtezoqDS3xsMyZI3rbZk1VdNKE7Df/WYqbXlkSH2yOJg/LWHtU3xrR2/Q+3drjfbvEB/2LC4IY3qez7XEDu5fi7WU7Mfjmt3HcH+JhunBUmGZ6ptsYbVeysVJdW7WeULISHf9vYtwxufKZL7Fm137dVq0Rk7WbCi0zVN32QNMh/mwnXpd+XUtw1qg+pm3qYH1HwOKeglAwgGV3noUfVMW7XFJ7RvTrbIQ+AgHCEJtu/QY9tuzkuX9nTD8A2sAtEeGPF43GcYN72B5rsiuJuMtSANJzt/Na+nc1Z/u8smArItGYMSAq7S0vSv5Af3nLma5z+Ccd1c92u4yzPnf1sfjdBaNsc+VVbntjhTHFvsmSiZRMgC46egCG9bZPYQXMMffxg7sn7C8vChnZMsnEQq1b9N+vtmGGnkElPW8p6tZsGWvN9Ccur0JXF70JJ/oq33EywVQzsNSSAY2tUZQUxPcly5Y5e1Qfx9AkkNrr76famuTaXnvyENP7O95YYdiqfo519q/1+Xv+6mMdP+OogV1N7/904eiEYwqDAaNuktO9MHZQ14Rt7S2NnQ4s7i4xDfI55C289asT8dFNp2DiyHiLffdb2qQIuxtgwa1nYswA7QYoVlp/u4URjqnshsE9y4yus3UsoLOemldSEDQeUDmgesmxgxLOd9JQc8nlBZv3YfRd7xneoXxI7FL+AODMI3ph+nUTUNGpyLVHZyeYAAyx7N+1BJceZz9p7GenHmp6L4tRNbVGTSJpV1MEAC477hDcff5ITBmfeC0kaljsN2cdnrBfFdpkoYMjldIUALBNnwUpZzeWGAJkEXdT7zBxWzLsZm+q6brJBNM6uUjGhptazem0yb7nob3KHcd4kv2tzEZTw5rJGk5rj1WOfUhxl59jnUFcbPGuJxzWE0N7JTb0vz1nON74xQTTtr5dE9OeKzoVGSFMp2vbq1MxbvrWMNO2ZHWdMg2Lu0tuVL4kp/hdaWEIQyrK8eCUsQn77MSgKBQw0vfUATq7mh1XnFCJWf9zKqoqNYG0xk6H6OEBNS7bvawQX98zEXdPGpVwvsqeZXj80nEYO6irMcjW2BrFQ/psTeMhKbL3HE8eVoHResNkN6vSDikUasqinAafKvzzm7MOx/u/Phm3nnsEAG25vKc+24jG1ihKlWsbdSgCds8Fo1BaGDJlNljTXNVVkdTCa7KR7KRMXEomQOVFIfzpwtGYecPJOKxXOVbv3I+Pvt6FbXoISF5bq+BJB+KnpxyKr+85G4D70rNTzx6esK27HgoJUPIw3q3nHWFKOV2z8wDmrNuje+5xG53q4D9xeRV+dcZQ07W9YIy5l6b+7e8uiN+PlT203m6vTkXxZyFFCGXmDScbrwuDWtlka8NpvW/luY8f0gP/+enxAMylB2Sv225ymt3cEjXklaxXZC078vbyHTjlvlkpc+gzAYu7S7qWFhqVFVP5Una58cU2N2xxQdBo9VVxUx9ouV8+OJccOwizf3Oa0XW854JR+PtlRxsTlAYroaHD+3RCcUHQsa7NxFF98drPJ6BXJ/NgV3FBwPgbJ8+9wmaATM3E+PWZw/DmdSca7685abAhmN9VBprOHqUVhkoVKiciDO3dyeTZ3/PWSjSFoybv0m4SlSoW8kEd3qcTHlIa4QHdSkz/U0s4ijL9vKcN7wXAXHIilQD94JiBOLxPJxw3pDs+/Ho3fvyPBXhQz/SRIQNrWEaGxHqUFSZ4g3dNGml6f+3JQ4zKpdecNNi4PyqU77Kb3iiligQM79MZ//jRePzjR9qEse88/Bl++OQ87GtsNdlot3g3AJx2eAVCwQDK9BBe97JC/PDY+Pc0tFc5TlRShQuDAeP+lTWazj6yr9F4phoUP7xPJ5xzpNY7fm/lLhz627eNeQVOPU557x05oIvhIMnB5injBxpZbQOVLBdA6xGrvQA58HvskHgvNJm9P54w2NTw3vzqMmyuaTTCtdnEXS4WAwD4+2VVeHbOJsfSAxI1i6UgSAhHha3nXhAMGDfGiH7xwS7Vy3rmymPw4ardOFYPaRCR6Qa8TBe7r/Rp898d299YEWhAN/sMHiunDa8wwhyAOV7pNCPW2nMAgNd/cQJO/OMsAMD1Z2rpi09cXoWiUAAnD6tAcziKP3//KJw/pp+xRKHMuXe7qo/d2IWaUqiep1Bf6Ujtaclr27dLsfGdjB/cHa/85Hhs3duIBz5cCyJgZP8uuOjoAXj2i8245NhBGNyzDFPGDzLi51YRcGLyMYNMddeJ4l6kNS4sG6ahNuMCU8YPwh36alyLbvuWIdyyemljawSXHjcI1550KE6+T/sO+tmEE5IhRU8SjgpTw+k0QU0Kp7xXOheHjMyjnuWFeP/GUxCJxrBoSy2IgO+O649JY/qhJRJDcUEAV5xwCHp3LjY+y857tvLA5LHYuneOUZNe9lhlT6OsyL7hHNE3cVD512cOM9YTGKR/r2ce0RsfrNqFL/WiZRNH9sG5o/virJF9sHt/MzZUN+CRWdpksWTjCSWFQVx14mDc+465llOqaqeZgMU9DQ7rVY57LkgMcdix6d5zAQAXPjYHCzfvM4VaQgEyHuS+XUrwzJXHoKqym7H/RxMq0dSqeaQnHNojYSlAO646cTAWb63F5GMG4sKjB2B3fbPr8sK/PnMYLjuuEjf9ezE+X1djulmtscv+XUvwTW0TDumROHg8oFui4KkZH8UFQSMX+KEpY1HbFEalvqReOlkuK+46CyPvmGm8V+1Vr/Pxh/bAJ2uqccbw3gn7D+lRhtH9u6CsMGjERQd2LzW+NwC47bwROP2I3hg7qBvGDop/PwAcM0+sjOrfBQ//cCyue2ERAE185PdiDcuMHdQVczfstT236h3aDbKWFobwuwuOBADMmXo6dtY3p50hUl4Uwt3nj8Ttb8RXeFJtVEN+Q3qWYcOeBlPcWjasnUsKDC/8CF1MQ8EA7lR6HwXBeEPdV8+UkVP2h/Vxrv4Y//sAHr/saEy49yPTdtnTsIb5Lq4aiL+8vwajLeMhgNbjnHBoT3y2bo8R+3/i8qMRUxYrefyyo43jB3QrNer7A6nLeBcEA/jXVcfi0qfmGdt2ORSXyyQs7lnm9OG9sHDzPlMq2Ac3nmKaSi+7/ZKiUBC/tgzEpKJX52K8/JPjjfd2MySdCAUD6NOlGIN7liWIu9Vzf/XnJ2D97gMmsXnsknFG13/mDSe7ysf+jp45IwdwzziiV7LDTZQVhbD49m9hzN3a0uJUrKYAAAjoSURBVGtq6ED13HuUF+KDG08xebBnjeyDm741DD8+UQtlrLh7ouPnhIIBnDLMPPD8uwtG4V9zNxtjBW44b3Q/zFi6A+8s32m6ttZyCY/8cBzmb9yLPko8976LRhvT+J+6ogqrdtSnFJN+XUvQr2uJ4a1OOCx19pXkgrH9zeKuhmWUafSVPcvw6KXjTKGskf264Pwx/XD9GUPRp0sxHvnhOJzowjEx/r5/F2yva05a2lelf9cSTBk/yFTHxyn+fd3ph2HSmH4mp+TZH4/H/I01CAQIj192NHbVNxtiTkRIVp6mW1khDutVbkqeSIbqvAHA7jTTWtsCi3uW+fmph+LcI/uaZj9W9ixzNRuyo5EPVX1TfLBHemDDepfjvouOQu/OxQlF084+Mr6gwuEuvC6Vik5FeP/XJ9v2BJLRVWksVcH8ftVA/GPOJvxoQiV+dfpQI3whKQgG8EsXZR6cuPS4QxwzepIh4/XqmKQq0v/92fHoUV5kupaA9v9Izjiit2kdgVQUFwTxzvUnuQ7PAVpIZPldZ2GU3jNSQ0d/vHA0Hv5oLU49vBdOG94roVxDYSiABybHxzHUhTbc8JcfHIXFW2pNjVsq/u97R+KM4b1wtb4IuF3Dd88Fo0BECffYKcMqjMa7vCiE8hThVisf3Jg4gc2J4oIgZv/mNPzqpUVYtKU27QlpbYHFPcsQkSeF3A7ZhVZznQtDAcz+zWno1bko7QlNbhnq0lOzImPqqrjfdt4I3PTtYabMFi9wTGV3AOtNtWwAzTMf1b+Lce0zTVvOW14UQr8uxdhe12xKDjisVzn+NjkxEyxTdC4uwMmWnpIbnByKD286BZ2KQujlUMG1oxnYvRSv/XwChBCuQ6btgbNlGIMxAxMnXQDaTZktYW8PduGGYIA8J+xAvLaLle9XDcyasLeHG7+t5fn37eLe688VTj2TQyvKPSPsKh0h7ABAuao1XFVVJRYscF7wlskNc9bvQefiAlPlSK9S29iKhz5ah//59uGua6PnkoWb90EIkZCV4lV21zejolNRh4lRe6hvDqOhJeKLxqi9ENFCIURVyuNY3BmGYfyDW3HnsAzDMMxBiCtxJ6KJRLSaiNYR0VSb/UVE9LK+fx4RVWbaUIZhGMY9KcWdiIIAHgFwNoARAKYQkbX821UA9gkhDgPwVwB/zLShDMMwjHvceO7jAawTQmwQQrQCeAnA+ZZjzgfwrP76PwDOID+MwjAMwxykuBH3/gDUJWW26dtsjxFCRADUAUjI/SKia4loAREtqK6ubpvFDMMwTErciLudB25NsXFzDIQQ04QQVUKIqoqK9CcrMAzDMO5wI+7bAAxU3g8AsN3pGCIKAegCYG8mDGQYhmHSx424fwlgKBENJqJCAJMBTLccMx3AFfrriwB8JHKVQM8wDMO4m8REROcA+BuAIICnhRC/J6K7ASwQQkwnomIAzwEYC81jnyyE2JDinNUANrfR7p4A9rTxb3MB25s9/GQr4C97/WQr4C9722PrIUKIlHHtnM1QbQ9EtMDNDC2vwPZmDz/ZCvjLXj/ZCvjL3o6wlWeoMgzDHISwuDMMwxyE+FXcp+XagDRhe7OHn2wF/GWvn2wF/GVv1m31ZcydYRiGSY5fPXeGYRgmCSzuDMMwByG+E/dU5YdzARE9TUS7iWi5sq07Eb1PRGv139307URED+r2LyWicR1s60AimkVEq4hoBRFd71V7iaiYiOYT0RLd1rv07YP10tJr9VLThfp2T5SeJqIgES0iore8bi8RbSKiZUS0mIgW6Ns8dy/on9+ViP5DRF/r9+/xHrb1cP2ayp96IrqhQ+0VQvjmB9okqvUAhgAoBLAEwAgP2HUygHEAlivb/gRgqv56KoA/6q/PAfAOtHo8xwGY18G29gUwTn/dCcAaaKWcPWev/pnl+usCAPN0G16BNlEOAB4H8DP99c8BPK6/ngzg5RzdDzcCeAHAW/p7z9oLYBOAnpZtnrsX9M9/FsDV+utCAF29aqvF7iCAnQAO6Uh7c/LPtuMiHQ9gpvL+ZgA359ou3ZZKi7ivBtBXf90XwGr99d8BTLE7Lkd2vwHgW163F0ApgK8AHAttZl/Iek8AmAngeP11SD+OOtjOAQA+BHA6gLf0h9XL9tqJu+fuBQCdAWy0Xh8v2mpj+7cBfN7R9votLOOm/LBX6C2E2AEA+u9e+nbP/A96GGAsNI/Yk/bqIY7FAHYDeB9az61WaKWlrfa4Kj2dZf4G4DcAYvr7HvC2vQLAe0S0kIiu1bd58V4YAqAawDN6yOtJIirzqK1WJgN4UX/dYfb6TdxdlRb2OJ74H4ioHMB/AdwghKhPdqjNtg6zVwgRFUKMgeYRjwdwRBJ7cmorEZ0HYLcQYqG62eZQT9irM0EIMQ7aSmu/IKKTkxybS3tD0EKfjwkhxgJogBbWcMIL1xb6+MokAP9OdajNtnbZ6zdxd1N+2CvsIqK+AKD/3q1vz/n/QEQF0IT9eSHEq/pmz9oLAEKIWgAfQ4tHdiWttLTVnlyXnp4AYBIRbYK2Ytnp0Dx5r9oLIcR2/fduAK9Ba0C9eC9sA7BNCDFPf/8faGLvRVtVzgbwlRBil/6+w+z1m7i7KT/sFdQyyFdAi23L7Zfro+PHAaiT3bSOgIgIwFMAVgkh7veyvURUQURd9dclAM4EsArALGilpe1szVnpaSHEzUKIAUKISmj35kdCiEu8ai8RlRFRJ/kaWmx4OTx4LwghdgLYSkSH65vOALDSi7ZamIJ4SEba1TH25mKAoZ2DE+dAy/BYD+CWXNuj2/QigB0AwtBa4KugxU4/BLBW/91dP5agLTi+HsAyAFUdbOuJ0Lp7SwEs1n/O8aK9AEYDWKTbuhzA7fr2IQDmA1gHrbtbpG8v1t+v0/cPyeE9cSri2TKetFe3a4n+s0I+T168F/TPHwNggX4/vA6gm1dt1W0oBVADoIuyrcPs5fIDDMMwByF+C8swDMMwLmBxZxiGOQhhcWcYhjkIYXFnGIY5CGFxZxiGOQhhcWcYhjkIYXFnGIY5CPn/2QdSMxLiCq4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input and target sent to CUDA\n",
      "Train Epoch: 2 [0/997 (0%)]\tLoss: 0.046808\n",
      "Train Epoch: 2 [10/997 (1%)]\tLoss: 0.041776\n",
      "Train Epoch: 2 [20/997 (2%)]\tLoss: 0.111372\n",
      "Train Epoch: 2 [30/997 (3%)]\tLoss: 0.398673\n",
      "Train Epoch: 2 [40/997 (4%)]\tLoss: 0.554127\n",
      "Train Epoch: 2 [50/997 (5%)]\tLoss: 0.486398\n",
      "Train Epoch: 2 [60/997 (6%)]\tLoss: 0.422419\n",
      "Train Epoch: 2 [70/997 (7%)]\tLoss: 0.338737\n",
      "Train Epoch: 2 [80/997 (8%)]\tLoss: 0.268588\n",
      "Train Epoch: 2 [90/997 (9%)]\tLoss: 0.358855\n",
      "Train Epoch: 2 [100/997 (10%)]\tLoss: 0.185204\n",
      "Train Epoch: 2 [110/997 (11%)]\tLoss: 0.099064\n",
      "Train Epoch: 2 [120/997 (12%)]\tLoss: 0.053698\n",
      "Train Epoch: 2 [130/997 (13%)]\tLoss: 0.093940\n",
      "Train Epoch: 2 [140/997 (14%)]\tLoss: 0.311855\n",
      "Train Epoch: 2 [150/997 (15%)]\tLoss: 0.057570\n",
      "Train Epoch: 2 [160/997 (16%)]\tLoss: 0.249835\n",
      "Train Epoch: 2 [170/997 (17%)]\tLoss: 0.352981\n",
      "Train Epoch: 2 [180/997 (18%)]\tLoss: 0.202833\n",
      "Train Epoch: 2 [190/997 (19%)]\tLoss: 0.077381\n",
      "Train Epoch: 2 [200/997 (20%)]\tLoss: 0.064421\n",
      "Train Epoch: 2 [210/997 (21%)]\tLoss: 0.080428\n",
      "Train Epoch: 2 [220/997 (22%)]\tLoss: 0.155860\n",
      "Train Epoch: 2 [230/997 (23%)]\tLoss: 0.279472\n",
      "Train Epoch: 2 [240/997 (24%)]\tLoss: 0.094303\n",
      "Train Epoch: 2 [250/997 (25%)]\tLoss: 0.185883\n",
      "Train Epoch: 2 [260/997 (26%)]\tLoss: 0.272420\n",
      "Train Epoch: 2 [270/997 (27%)]\tLoss: 0.108766\n",
      "Train Epoch: 2 [280/997 (28%)]\tLoss: 0.107831\n",
      "Train Epoch: 2 [290/997 (29%)]\tLoss: 0.195566\n",
      "Train Epoch: 2 [300/997 (30%)]\tLoss: 0.120163\n",
      "Train Epoch: 2 [310/997 (31%)]\tLoss: 0.062307\n",
      "Train Epoch: 2 [320/997 (32%)]\tLoss: 0.085363\n",
      "Train Epoch: 2 [330/997 (33%)]\tLoss: 0.085294\n",
      "Train Epoch: 2 [340/997 (34%)]\tLoss: 0.152597\n",
      "Train Epoch: 2 [350/997 (35%)]\tLoss: 0.171219\n",
      "Train Epoch: 2 [360/997 (36%)]\tLoss: 0.093763\n",
      "Train Epoch: 2 [370/997 (37%)]\tLoss: 0.105283\n",
      "Train Epoch: 2 [380/997 (38%)]\tLoss: 0.063925\n",
      "Train Epoch: 2 [390/997 (39%)]\tLoss: 0.073903\n",
      "Train Epoch: 2 [400/997 (40%)]\tLoss: 0.143814\n",
      "Train Epoch: 2 [410/997 (41%)]\tLoss: 0.138065\n",
      "Train Epoch: 2 [420/997 (42%)]\tLoss: 0.092889\n",
      "Train Epoch: 2 [430/997 (43%)]\tLoss: 0.173662\n",
      "Train Epoch: 2 [440/997 (44%)]\tLoss: 0.200724\n",
      "Train Epoch: 2 [450/997 (45%)]\tLoss: 0.131193\n",
      "Train Epoch: 2 [460/997 (46%)]\tLoss: 0.123390\n",
      "Train Epoch: 2 [470/997 (47%)]\tLoss: 0.080033\n",
      "Train Epoch: 2 [480/997 (48%)]\tLoss: 0.127560\n",
      "Train Epoch: 2 [490/997 (49%)]\tLoss: 0.083054\n",
      "Train Epoch: 2 [500/997 (50%)]\tLoss: 0.146854\n",
      "Train Epoch: 2 [510/997 (51%)]\tLoss: 0.101993\n",
      "Train Epoch: 2 [520/997 (52%)]\tLoss: 0.082132\n",
      "Train Epoch: 2 [530/997 (53%)]\tLoss: 0.127839\n",
      "Train Epoch: 2 [540/997 (54%)]\tLoss: 0.132615\n",
      "Train Epoch: 2 [550/997 (55%)]\tLoss: 0.076345\n",
      "Train Epoch: 2 [560/997 (56%)]\tLoss: 0.072053\n",
      "Train Epoch: 2 [570/997 (57%)]\tLoss: 0.103800\n",
      "Train Epoch: 2 [580/997 (58%)]\tLoss: 0.118331\n",
      "Train Epoch: 2 [590/997 (59%)]\tLoss: 0.125248\n",
      "Train Epoch: 2 [600/997 (60%)]\tLoss: 0.096809\n",
      "Train Epoch: 2 [610/997 (61%)]\tLoss: 0.086563\n",
      "Train Epoch: 2 [620/997 (62%)]\tLoss: 0.158875\n",
      "Train Epoch: 2 [630/997 (63%)]\tLoss: 0.149462\n",
      "Train Epoch: 2 [640/997 (64%)]\tLoss: 0.125639\n",
      "Train Epoch: 2 [650/997 (65%)]\tLoss: 0.107869\n",
      "Train Epoch: 2 [660/997 (66%)]\tLoss: 0.078952\n",
      "Train Epoch: 2 [670/997 (67%)]\tLoss: 0.058394\n",
      "Train Epoch: 2 [680/997 (68%)]\tLoss: 0.076377\n",
      "Train Epoch: 2 [690/997 (69%)]\tLoss: 0.192092\n",
      "Train Epoch: 2 [700/997 (70%)]\tLoss: 0.159946\n",
      "Train Epoch: 2 [710/997 (71%)]\tLoss: 0.177422\n",
      "Train Epoch: 2 [720/997 (72%)]\tLoss: 0.245453\n",
      "Train Epoch: 2 [730/997 (73%)]\tLoss: 0.132230\n",
      "Train Epoch: 2 [740/997 (74%)]\tLoss: 0.264405\n",
      "Train Epoch: 2 [750/997 (75%)]\tLoss: 0.245831\n",
      "Train Epoch: 2 [760/997 (76%)]\tLoss: 0.240119\n",
      "Train Epoch: 2 [770/997 (77%)]\tLoss: 0.226179\n",
      "Train Epoch: 2 [780/997 (78%)]\tLoss: 0.160328\n",
      "Train Epoch: 2 [790/997 (79%)]\tLoss: 0.168489\n",
      "Train Epoch: 2 [800/997 (80%)]\tLoss: 0.190456\n",
      "Train Epoch: 2 [810/997 (81%)]\tLoss: 0.188767\n",
      "Train Epoch: 2 [820/997 (82%)]\tLoss: 0.188376\n",
      "Train Epoch: 2 [830/997 (83%)]\tLoss: 0.140310\n",
      "Train Epoch: 2 [840/997 (84%)]\tLoss: 0.115196\n",
      "Train Epoch: 2 [850/997 (85%)]\tLoss: 0.153160\n",
      "Train Epoch: 2 [860/997 (86%)]\tLoss: 0.155340\n",
      "Train Epoch: 2 [870/997 (87%)]\tLoss: 0.105440\n",
      "Train Epoch: 2 [880/997 (88%)]\tLoss: 0.131224\n",
      "Train Epoch: 2 [890/997 (89%)]\tLoss: 0.082599\n",
      "Train Epoch: 2 [900/997 (90%)]\tLoss: 0.095103\n",
      "Train Epoch: 2 [910/997 (91%)]\tLoss: 0.113005\n",
      "Train Epoch: 2 [920/997 (92%)]\tLoss: 0.068053\n",
      "Train Epoch: 2 [930/997 (93%)]\tLoss: 0.115648\n",
      "Train Epoch: 2 [940/997 (94%)]\tLoss: 0.062181\n",
      "Train Epoch: 2 [950/997 (95%)]\tLoss: 0.065186\n",
      "Train Epoch: 2 [960/997 (96%)]\tLoss: 0.054259\n",
      "Train Epoch: 2 [970/997 (97%)]\tLoss: 0.065095\n",
      "Train Epoch: 2 [980/997 (98%)]\tLoss: 0.087214\n",
      "Train Epoch: 2 [990/997 (99%)]\tLoss: 0.057010\n",
      "input and target sent to CUDA\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1804, 0.1615, 0.1396,  ..., 0.1961, 0.2310, 0.1989], device='cuda:0')\n",
      "t tensor([0.0047, 0.0047, 0.0047,  ..., 0.3258, 0.3257, 0.3256], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1716, 0.1564, 0.1851,  ..., 0.2782, 0.2787, 0.2907], device='cuda:0')\n",
      "t tensor([0.0247, 0.0247, 0.0247,  ..., 0.0017, 0.0020, 0.0021], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1759, 0.1572, 0.1508,  ..., 0.2101, 0.2676, 0.2431], device='cuda:0')\n",
      "t tensor([0.0348, 0.0348, 0.0348,  ..., 0.4665, 0.4665, 0.4665], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1648, 0.1568, 0.1840,  ..., 0.2768, 0.2717, 0.2862], device='cuda:0')\n",
      "t tensor([0.6048, 0.6048, 0.6047,  ..., 0.9539, 0.9540, 0.9540], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1772, 0.1604, 0.1271,  ..., 0.1886, 0.2545, 0.1949], device='cuda:0')\n",
      "t tensor([0.7523, 0.7524, 0.7524,  ..., 0.9628, 0.9628, 0.9628], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1749, 0.1603, 0.1261,  ..., 0.1881, 0.2385, 0.1899], device='cuda:0')\n",
      "t tensor([0.5879, 0.5898, 0.5951,  ..., 0.9336, 0.9336, 0.9336], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1814, 0.1608, 0.1264,  ..., 0.1887, 0.2487, 0.1944], device='cuda:0')\n",
      "t tensor([0.3946, 0.3929, 0.3882,  ..., 0.9903, 0.9904, 0.9904], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1783, 0.1588, 0.1607,  ..., 0.2780, 0.2878, 0.2981], device='cuda:0')\n",
      "t tensor([0.0886, 0.0886, 0.0887,  ..., 0.8846, 0.8844, 0.8844], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1786, 0.1600, 0.1280,  ..., 0.1900, 0.2620, 0.2064], device='cuda:0')\n",
      "t tensor([0.7348, 0.7348, 0.7347,  ..., 0.5635, 0.5635, 0.5635], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1737, 0.1605, 0.1264,  ..., 0.1887, 0.2617, 0.1986], device='cuda:0')\n",
      "t tensor([0.5335, 0.5334, 0.5330,  ..., 0.6007, 0.6014, 0.6016], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1816, 0.1614, 0.1271,  ..., 0.1874, 0.2176, 0.1828], device='cuda:0')\n",
      "t tensor([0.4988, 0.4988, 0.4991,  ..., 0.9337, 0.9338, 0.9338], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1803, 0.1607, 0.1263,  ..., 0.1889, 0.2506, 0.1952], device='cuda:0')\n",
      "t tensor([0.0616, 0.0617, 0.0620,  ..., 0.6313, 0.6312, 0.6312], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1733, 0.1610, 0.1251,  ..., 0.1886, 0.2504, 0.1953], device='cuda:0')\n",
      "t tensor([0.2780, 0.2779, 0.2775,  ..., 0.3864, 0.3865, 0.3865], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1777, 0.1606, 0.1260,  ..., 0.1888, 0.2560, 0.1974], device='cuda:0')\n",
      "t tensor([0.2567, 0.2565, 0.2558,  ..., 0.2739, 0.2738, 0.2738], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1815, 0.1615, 0.1275,  ..., 0.1875, 0.2168, 0.1872], device='cuda:0')\n",
      "t tensor([0.7539, 0.7537, 0.7530,  ..., 0.9571, 0.9571, 0.9571], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1726, 0.1603, 0.1263,  ..., 0.1891, 0.2643, 0.1998], device='cuda:0')\n",
      "t tensor([0.2040, 0.2035, 0.2020,  ..., 0.4414, 0.4413, 0.4413], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o tensor([0.1739, 0.1600, 0.1272,  ..., 0.1896, 0.2651, 0.2016], device='cuda:0')\n",
      "t tensor([0.2856, 0.2856, 0.2854,  ..., 0.8735, 0.8735, 0.8735], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1724, 0.1603, 0.1262,  ..., 0.1891, 0.2643, 0.1998], device='cuda:0')\n",
      "t tensor([0.1827, 0.1823, 0.1809,  ..., 0.9637, 0.9637, 0.9637], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1786, 0.1604, 0.1267,  ..., 0.1889, 0.2592, 0.1971], device='cuda:0')\n",
      "t tensor([0.3747, 0.3743, 0.3730,  ..., 0.6949, 0.6949, 0.6949], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1744, 0.1601, 0.1264,  ..., 0.1891, 0.2660, 0.1992], device='cuda:0')\n",
      "t tensor([0.2302, 0.2302, 0.2301,  ..., 0.6374, 0.6371, 0.6370], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1678, 0.1607, 0.1254,  ..., 0.1892, 0.2613, 0.1982], device='cuda:0')\n",
      "t tensor([0.2914, 0.2912, 0.2906,  ..., 0.3812, 0.3812, 0.3811], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1795, 0.1602, 0.1273,  ..., 0.1894, 0.2620, 0.2024], device='cuda:0')\n",
      "t tensor([0.8507, 0.8507, 0.8507,  ..., 0.1821, 0.1822, 0.1823], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1783, 0.1603, 0.1267,  ..., 0.1891, 0.2615, 0.1981], device='cuda:0')\n",
      "t tensor([0.9335, 0.9334, 0.9333,  ..., 0.0684, 0.0683, 0.0683], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1822, 0.1613, 0.1271,  ..., 0.1885, 0.2354, 0.1916], device='cuda:0')\n",
      "t tensor([0.9413, 0.9413, 0.9414,  ..., 0.7688, 0.7688, 0.7688], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1791, 0.1603, 0.1270,  ..., 0.1893, 0.2607, 0.2007], device='cuda:0')\n",
      "t tensor([0.6306, 0.6306, 0.6305,  ..., 0.6907, 0.6909, 0.6909], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1791, 0.1608, 0.1283,  ..., 0.1906, 0.2643, 0.2049], device='cuda:0')\n",
      "t tensor([0.0256, 0.0256, 0.0255,  ..., 0.9322, 0.9322, 0.9322], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1559, 0.1613, 0.1242,  ..., 0.1871, 0.2125, 0.1843], device='cuda:0')\n",
      "t tensor([0.1775, 0.1775, 0.1775,  ..., 0.9676, 0.9676, 0.9676], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1829, 0.1598, 0.1305,  ..., 0.1903, 0.2565, 0.2116], device='cuda:0')\n",
      "t tensor([0.5727, 0.5722, 0.5708,  ..., 0.3124, 0.3121, 0.3120], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1768, 0.1607, 0.1254,  ..., 0.1889, 0.2558, 0.1975], device='cuda:0')\n",
      "t tensor([0.2348, 0.2342, 0.2325,  ..., 0.4525, 0.4524, 0.4524], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1783, 0.1604, 0.1270,  ..., 0.1893, 0.2601, 0.2018], device='cuda:0')\n",
      "t tensor([0.7909, 0.7908, 0.7906,  ..., 0.8627, 0.8628, 0.8628], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1780, 0.1603, 0.1266,  ..., 0.1891, 0.2614, 0.1981], device='cuda:0')\n",
      "t tensor([0.4285, 0.4283, 0.4279,  ..., 0.8922, 0.8919, 0.8919], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1756, 0.1603, 0.1264,  ..., 0.1891, 0.2647, 0.2018], device='cuda:0')\n",
      "t tensor([0.1388, 0.1376, 0.1339,  ..., 0.4811, 0.4811, 0.4811], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1777, 0.1602, 0.1270,  ..., 0.1889, 0.2577, 0.1965], device='cuda:0')\n",
      "t tensor([0.1970, 0.1972, 0.1978,  ..., 0.3019, 0.3019, 0.3019], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1792, 0.1607, 0.1257,  ..., 0.1891, 0.2534, 0.1960], device='cuda:0')\n",
      "t tensor([0.4834, 0.4836, 0.4842,  ..., 0.2392, 0.2392, 0.2392], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1874, 0.1576, 0.1521,  ..., 0.2025, 0.2807, 0.2477], device='cuda:0')\n",
      "t tensor([0.2179, 0.2179, 0.2179,  ..., 0.2706, 0.2705, 0.2703], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1822, 0.1572, 0.1632,  ..., 0.2632, 0.2875, 0.2908], device='cuda:0')\n",
      "t tensor([0.0409, 0.0409, 0.0409,  ..., 0.7987, 0.7987, 0.7987], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1666, 0.1608, 0.1248,  ..., 0.1886, 0.2406, 0.1944], device='cuda:0')\n",
      "t tensor([0.0342, 0.0342, 0.0342,  ..., 0.9659, 0.9659, 0.9659], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1770, 0.1609, 0.1248,  ..., 0.1881, 0.2417, 0.1918], device='cuda:0')\n",
      "t tensor([0.3493, 0.3492, 0.3487,  ..., 0.2131, 0.2133, 0.2134], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1693, 0.1606, 0.1254,  ..., 0.1889, 0.2567, 0.1975], device='cuda:0')\n",
      "t tensor([0.5021, 0.5020, 0.5020,  ..., 0.3519, 0.3519, 0.3519], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1812, 0.1605, 0.1264,  ..., 0.1891, 0.2529, 0.1961], device='cuda:0')\n",
      "t tensor([0.5981, 0.5980, 0.5979,  ..., 0.7961, 0.7961, 0.7961], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1812, 0.1605, 0.1265,  ..., 0.1891, 0.2529, 0.1961], device='cuda:0')\n",
      "t tensor([0.7488, 0.7488, 0.7488,  ..., 0.8285, 0.8285, 0.8285], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1753, 0.1603, 0.1252,  ..., 0.1876, 0.2536, 0.1960], device='cuda:0')\n",
      "t tensor([0.0456, 0.0458, 0.0463,  ..., 0.0027, 0.0027, 0.0027], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1781, 0.1616, 0.1304,  ..., 0.1894, 0.2320, 0.1983], device='cuda:0')\n",
      "t tensor([0.5312, 0.5311, 0.5311,  ..., 0.1408, 0.1408, 0.1408], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1560, 0.1562, 0.1903,  ..., 0.2928, 0.2898, 0.3213], device='cuda:0')\n",
      "t tensor([0.2914, 0.2914, 0.2913,  ..., 0.9984, 0.9984, 0.9984], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1593, 0.1561, 0.1932,  ..., 0.2924, 0.2899, 0.3211], device='cuda:0')\n",
      "t tensor([0.5180, 0.5180, 0.5180,  ..., 0.9702, 0.9702, 0.9702], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1584, 0.1558, 0.1929,  ..., 0.2928, 0.2898, 0.3198], device='cuda:0')\n",
      "t tensor([0.1826, 0.1826, 0.1827,  ..., 0.9062, 0.9062, 0.9063], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1515, 0.1567, 0.1691,  ..., 0.2824, 0.2891, 0.3126], device='cuda:0')\n",
      "t tensor([0.1136, 0.1137, 0.1137,  ..., 0.9989, 0.9989, 0.9989], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1737, 0.1612, 0.1240,  ..., 0.1875, 0.2270, 0.1873], device='cuda:0')\n",
      "t tensor([0.4704, 0.4702, 0.4699,  ..., 0.0199, 0.0199, 0.0199], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1758, 0.1602, 0.1265,  ..., 0.1891, 0.2655, 0.2002], device='cuda:0')\n",
      "t tensor([0.9487, 0.9486, 0.9482,  ..., 0.5739, 0.5738, 0.5738], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1743, 0.1603, 0.1264,  ..., 0.1891, 0.2640, 0.2014], device='cuda:0')\n",
      "t tensor([0.2798, 0.2796, 0.2791,  ..., 0.7839, 0.7840, 0.7840], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1811, 0.1606, 0.1263,  ..., 0.1889, 0.2530, 0.1957], device='cuda:0')\n",
      "t tensor([0.1696, 0.1689, 0.1668,  ..., 0.8651, 0.8650, 0.8650], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1800, 0.1607, 0.1262,  ..., 0.1888, 0.2505, 0.1948], device='cuda:0')\n",
      "t tensor([0.7236, 0.7236, 0.7235,  ..., 0.6560, 0.6561, 0.6562], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1822, 0.1613, 0.1272,  ..., 0.1884, 0.2343, 0.1910], device='cuda:0')\n",
      "t tensor([0.6936, 0.6936, 0.6935,  ..., 0.7810, 0.7810, 0.7810], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1550, 0.1557, 0.1917,  ..., 0.2926, 0.2899, 0.3212], device='cuda:0')\n",
      "t tensor([0.0778, 0.0778, 0.0777,  ..., 0.9969, 0.9969, 0.9969], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1804, 0.1604, 0.1266,  ..., 0.1890, 0.2556, 0.1961], device='cuda:0')\n",
      "t tensor([0.9165, 0.9165, 0.9165,  ..., 0.3030, 0.3032, 0.3032], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1744, 0.1607, 0.1259,  ..., 0.1887, 0.2495, 0.1951], device='cuda:0')\n",
      "t tensor([0.4204, 0.4204, 0.4205,  ..., 0.3603, 0.3601, 0.3601], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o tensor([0.1796, 0.1602, 0.1274,  ..., 0.1896, 0.2608, 0.2021], device='cuda:0')\n",
      "t tensor([0.6500, 0.6501, 0.6502,  ..., 0.7238, 0.7240, 0.7240], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1520, 0.1557, 0.1898,  ..., 0.2928, 0.2899, 0.3208], device='cuda:0')\n",
      "t tensor([0.2068, 0.2069, 0.2069,  ..., 0.8895, 0.8895, 0.8894], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1718, 0.1602, 0.1271,  ..., 0.1903, 0.2561, 0.2024], device='cuda:0')\n",
      "t tensor([0.0011, 0.0011, 0.0011,  ..., 0.9777, 0.9777, 0.9777], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1667, 0.1605, 0.1253,  ..., 0.1886, 0.2406, 0.1936], device='cuda:0')\n",
      "t tensor([0.0587, 0.0587, 0.0587,  ..., 0.9461, 0.9463, 0.9464], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1733, 0.1610, 0.1246,  ..., 0.1882, 0.2418, 0.1913], device='cuda:0')\n",
      "t tensor([0.1505, 0.1516, 0.1547,  ..., 0.3629, 0.3629, 0.3630], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1812, 0.1605, 0.1264,  ..., 0.1891, 0.2529, 0.1961], device='cuda:0')\n",
      "t tensor([0.2263, 0.2268, 0.2284,  ..., 0.6039, 0.6040, 0.6041], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1801, 0.1604, 0.1266,  ..., 0.1891, 0.2572, 0.1974], device='cuda:0')\n",
      "t tensor([0.6393, 0.6394, 0.6397,  ..., 0.9386, 0.9385, 0.9385], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1794, 0.1605, 0.1263,  ..., 0.1888, 0.2565, 0.1990], device='cuda:0')\n",
      "t tensor([0.6337, 0.6337, 0.6334,  ..., 0.2633, 0.2631, 0.2630], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1809, 0.1609, 0.1275,  ..., 0.1884, 0.2401, 0.1901], device='cuda:0')\n",
      "t tensor([0.7495, 0.7496, 0.7497,  ..., 0.1181, 0.1182, 0.1182], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1801, 0.1615, 0.1252,  ..., 0.1871, 0.2147, 0.1824], device='cuda:0')\n",
      "t tensor([0.8907, 0.8907, 0.8908,  ..., 0.0620, 0.0618, 0.0618], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1745, 0.1603, 0.1265,  ..., 0.1890, 0.2643, 0.2007], device='cuda:0')\n",
      "t tensor([0.2725, 0.2721, 0.2709,  ..., 0.5479, 0.5479, 0.5479], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1821, 0.1619, 0.1255,  ..., 0.1869, 0.2043, 0.1802], device='cuda:0')\n",
      "t tensor([0.8225, 0.8225, 0.8223,  ..., 0.2745, 0.2745, 0.2745], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1768, 0.1611, 0.1245,  ..., 0.1877, 0.2407, 0.1894], device='cuda:0')\n",
      "t tensor([0.4347, 0.4347, 0.4347,  ..., 0.5438, 0.5438, 0.5439], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1764, 0.1603, 0.1312,  ..., 0.1911, 0.2599, 0.2100], device='cuda:0')\n",
      "t tensor([0.1683, 0.1683, 0.1683,  ..., 0.0104, 0.0104, 0.0104], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1823, 0.1572, 0.1536,  ..., 0.2138, 0.2771, 0.2457], device='cuda:0')\n",
      "t tensor([0.0155, 0.0155, 0.0155,  ..., 0.2109, 0.2109, 0.2109], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1815, 0.1611, 0.1267,  ..., 0.1877, 0.2224, 0.1838], device='cuda:0')\n",
      "t tensor([0.0041, 0.0041, 0.0041,  ..., 0.1236, 0.1236, 0.1236], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1819, 0.1606, 0.1268,  ..., 0.1881, 0.2373, 0.1990], device='cuda:0')\n",
      "t tensor([0.2529, 0.2529, 0.2529,  ..., 0.0449, 0.0448, 0.0447], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1657, 0.1593, 0.1414,  ..., 0.2000, 0.2382, 0.2216], device='cuda:0')\n",
      "t tensor([7.9046e-05, 7.8999e-05, 7.8867e-05,  ..., 4.1099e-01, 4.1101e-01,\n",
      "        4.1101e-01], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1734, 0.1565, 0.1657,  ..., 0.2439, 0.2687, 0.2663], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1827, 0.1602, 0.1284,  ..., 0.1895, 0.2478, 0.2045], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1804, 0.1603, 0.1273,  ..., 0.1890, 0.2515, 0.1942], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1782, 0.1609, 0.1267,  ..., 0.1886, 0.2439, 0.1928], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1770, 0.1604, 0.1270,  ..., 0.1885, 0.2551, 0.1951], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1781, 0.1609, 0.1248,  ..., 0.1875, 0.2275, 0.1861], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1821, 0.1599, 0.1314,  ..., 0.1909, 0.2565, 0.2112], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1792, 0.1561, 0.1751,  ..., 0.2568, 0.2829, 0.2806], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1765, 0.1557, 0.1782,  ..., 0.2699, 0.2836, 0.2853], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1799, 0.1604, 0.1466,  ..., 0.2011, 0.2454, 0.2175], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1427, 0.1618, 0.1491,  ..., 0.2120, 0.1926, 0.1926], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1596, 0.1572, 0.1906,  ..., 0.2837, 0.2623, 0.2862], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1782, 0.1561, 0.1734,  ..., 0.2511, 0.2788, 0.2723], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1421, 0.1617, 0.1611,  ..., 0.2342, 0.1939, 0.2016], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1716, 0.1563, 0.1759,  ..., 0.2551, 0.2689, 0.2755], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1542, 0.1596, 0.1510,  ..., 0.2177, 0.2226, 0.2222], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1399, 0.1616, 0.1477,  ..., 0.2137, 0.1945, 0.1966], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1702, 0.1563, 0.1758,  ..., 0.2620, 0.2790, 0.2777], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1647, 0.1620, 0.1589,  ..., 0.2190, 0.1971, 0.1981], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1693, 0.1561, 0.1871,  ..., 0.2806, 0.2792, 0.2936], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1527, 0.1622, 0.1371,  ..., 0.1919, 0.1918, 0.1804], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1811, 0.1573, 0.1628,  ..., 0.2196, 0.2673, 0.2513], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1695, 0.1614, 0.1602,  ..., 0.2263, 0.2245, 0.2190], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1737, 0.1589, 0.1499,  ..., 0.2146, 0.2590, 0.2364], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1617, 0.1601, 0.1791,  ..., 0.2724, 0.2397, 0.2545], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1594, 0.1585, 0.1422,  ..., 0.2071, 0.2458, 0.2253], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "[0.3525697973107502, 0.20350569114462663, 0.19294637319264762, tensor(0.1915, device='cuda:0'), tensor(0.1936, device='cuda:0'), tensor(0.1925, device='cuda:0'), tensor(0.1946, device='cuda:0'), tensor(0.1940, device='cuda:0')]\n",
      "\n",
      "Test set: Avg. loss: 0.1940, Accuracy: 0/997 (0%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input and target sent to CUDA\n",
      "Train Epoch: 3 [0/997 (0%)]\tLoss: 0.047355\n",
      "Train Epoch: 3 [10/997 (1%)]\tLoss: 0.042477\n",
      "Train Epoch: 3 [20/997 (2%)]\tLoss: 0.111148\n",
      "Train Epoch: 3 [30/997 (3%)]\tLoss: 0.411402\n",
      "Train Epoch: 3 [40/997 (4%)]\tLoss: 0.524188\n",
      "Train Epoch: 3 [50/997 (5%)]\tLoss: 0.476661\n",
      "Train Epoch: 3 [60/997 (6%)]\tLoss: 0.484674\n",
      "Train Epoch: 3 [70/997 (7%)]\tLoss: 0.326203\n",
      "Train Epoch: 3 [80/997 (8%)]\tLoss: 0.288811\n",
      "Train Epoch: 3 [90/997 (9%)]\tLoss: 0.310285\n",
      "Train Epoch: 3 [100/997 (10%)]\tLoss: 0.226459\n",
      "Train Epoch: 3 [110/997 (11%)]\tLoss: 0.097029\n",
      "Train Epoch: 3 [120/997 (12%)]\tLoss: 0.051276\n",
      "Train Epoch: 3 [130/997 (13%)]\tLoss: 0.091516\n",
      "Train Epoch: 3 [140/997 (14%)]\tLoss: 0.384715\n",
      "Train Epoch: 3 [150/997 (15%)]\tLoss: 0.056904\n",
      "Train Epoch: 3 [160/997 (16%)]\tLoss: 0.221418\n",
      "Train Epoch: 3 [170/997 (17%)]\tLoss: 0.375565\n",
      "Train Epoch: 3 [180/997 (18%)]\tLoss: 0.229129\n",
      "Train Epoch: 3 [190/997 (19%)]\tLoss: 0.078448\n",
      "Train Epoch: 3 [200/997 (20%)]\tLoss: 0.064010\n",
      "Train Epoch: 3 [210/997 (21%)]\tLoss: 0.080273\n",
      "Train Epoch: 3 [220/997 (22%)]\tLoss: 0.150115\n",
      "Train Epoch: 3 [230/997 (23%)]\tLoss: 0.266971\n",
      "Train Epoch: 3 [240/997 (24%)]\tLoss: 0.091117\n",
      "Train Epoch: 3 [250/997 (25%)]\tLoss: 0.179694\n",
      "Train Epoch: 3 [260/997 (26%)]\tLoss: 0.264185\n",
      "Train Epoch: 3 [270/997 (27%)]\tLoss: 0.105108\n",
      "Train Epoch: 3 [280/997 (28%)]\tLoss: 0.107310\n",
      "Train Epoch: 3 [290/997 (29%)]\tLoss: 0.196027\n",
      "Train Epoch: 3 [300/997 (30%)]\tLoss: 0.116847\n",
      "Train Epoch: 3 [310/997 (31%)]\tLoss: 0.063425\n",
      "Train Epoch: 3 [320/997 (32%)]\tLoss: 0.085705\n",
      "Train Epoch: 3 [330/997 (33%)]\tLoss: 0.079943\n",
      "Train Epoch: 3 [340/997 (34%)]\tLoss: 0.145974\n",
      "Train Epoch: 3 [350/997 (35%)]\tLoss: 0.173093\n",
      "Train Epoch: 3 [360/997 (36%)]\tLoss: 0.089820\n",
      "Train Epoch: 3 [370/997 (37%)]\tLoss: 0.115649\n",
      "Train Epoch: 3 [380/997 (38%)]\tLoss: 0.070034\n",
      "Train Epoch: 3 [390/997 (39%)]\tLoss: 0.087233\n",
      "Train Epoch: 3 [400/997 (40%)]\tLoss: 0.147356\n",
      "Train Epoch: 3 [410/997 (41%)]\tLoss: 0.136253\n",
      "Train Epoch: 3 [420/997 (42%)]\tLoss: 0.089493\n",
      "Train Epoch: 3 [430/997 (43%)]\tLoss: 0.172123\n",
      "Train Epoch: 3 [440/997 (44%)]\tLoss: 0.197620\n",
      "Train Epoch: 3 [450/997 (45%)]\tLoss: 0.173717\n",
      "Train Epoch: 3 [460/997 (46%)]\tLoss: 0.123255\n",
      "Train Epoch: 3 [470/997 (47%)]\tLoss: 0.077884\n",
      "Train Epoch: 3 [480/997 (48%)]\tLoss: 0.137285\n",
      "Train Epoch: 3 [490/997 (49%)]\tLoss: 0.082376\n",
      "Train Epoch: 3 [500/997 (50%)]\tLoss: 0.129526\n",
      "Train Epoch: 3 [510/997 (51%)]\tLoss: 0.148620\n",
      "Train Epoch: 3 [520/997 (52%)]\tLoss: 0.123648\n",
      "Train Epoch: 3 [530/997 (53%)]\tLoss: 0.113179\n",
      "Train Epoch: 3 [540/997 (54%)]\tLoss: 0.129406\n",
      "Train Epoch: 3 [550/997 (55%)]\tLoss: 0.080163\n",
      "Train Epoch: 3 [560/997 (56%)]\tLoss: 0.075565\n",
      "Train Epoch: 3 [570/997 (57%)]\tLoss: 0.104871\n",
      "Train Epoch: 3 [580/997 (58%)]\tLoss: 0.120709\n",
      "Train Epoch: 3 [590/997 (59%)]\tLoss: 0.123572\n",
      "Train Epoch: 3 [600/997 (60%)]\tLoss: 0.089123\n",
      "Train Epoch: 3 [610/997 (61%)]\tLoss: 0.081432\n",
      "Train Epoch: 3 [620/997 (62%)]\tLoss: 0.160568\n",
      "Train Epoch: 3 [630/997 (63%)]\tLoss: 0.147054\n",
      "Train Epoch: 3 [640/997 (64%)]\tLoss: 0.138273\n",
      "Train Epoch: 3 [650/997 (65%)]\tLoss: 0.092975\n",
      "Train Epoch: 3 [660/997 (66%)]\tLoss: 0.078818\n",
      "Train Epoch: 3 [670/997 (67%)]\tLoss: 0.062018\n",
      "Train Epoch: 3 [680/997 (68%)]\tLoss: 0.077384\n",
      "Train Epoch: 3 [690/997 (69%)]\tLoss: 0.208893\n",
      "Train Epoch: 3 [700/997 (70%)]\tLoss: 0.147575\n",
      "Train Epoch: 3 [710/997 (71%)]\tLoss: 0.186039\n",
      "Train Epoch: 3 [720/997 (72%)]\tLoss: 0.226827\n",
      "Train Epoch: 3 [730/997 (73%)]\tLoss: 0.133024\n",
      "Train Epoch: 3 [740/997 (74%)]\tLoss: 0.239752\n",
      "Train Epoch: 3 [750/997 (75%)]\tLoss: 0.211842\n",
      "Train Epoch: 3 [760/997 (76%)]\tLoss: 0.253518\n",
      "Train Epoch: 3 [770/997 (77%)]\tLoss: 0.249829\n",
      "Train Epoch: 3 [780/997 (78%)]\tLoss: 0.173575\n",
      "Train Epoch: 3 [790/997 (79%)]\tLoss: 0.158556\n",
      "Train Epoch: 3 [800/997 (80%)]\tLoss: 0.176614\n",
      "Train Epoch: 3 [810/997 (81%)]\tLoss: 0.179801\n",
      "Train Epoch: 3 [820/997 (82%)]\tLoss: 0.182855\n",
      "Train Epoch: 3 [830/997 (83%)]\tLoss: 0.139566\n",
      "Train Epoch: 3 [840/997 (84%)]\tLoss: 0.113204\n",
      "Train Epoch: 3 [850/997 (85%)]\tLoss: 0.156762\n",
      "Train Epoch: 3 [860/997 (86%)]\tLoss: 0.143933\n",
      "Train Epoch: 3 [870/997 (87%)]\tLoss: 0.100020\n",
      "Train Epoch: 3 [880/997 (88%)]\tLoss: 0.102925\n",
      "Train Epoch: 3 [890/997 (89%)]\tLoss: 0.074129\n",
      "Train Epoch: 3 [900/997 (90%)]\tLoss: 0.091610\n",
      "Train Epoch: 3 [910/997 (91%)]\tLoss: 0.103760\n",
      "Train Epoch: 3 [920/997 (92%)]\tLoss: 0.064999\n",
      "Train Epoch: 3 [930/997 (93%)]\tLoss: 0.123410\n",
      "Train Epoch: 3 [940/997 (94%)]\tLoss: 0.063907\n",
      "Train Epoch: 3 [950/997 (95%)]\tLoss: 0.056518\n",
      "Train Epoch: 3 [960/997 (96%)]\tLoss: 0.061838\n",
      "Train Epoch: 3 [970/997 (97%)]\tLoss: 0.068456\n",
      "Train Epoch: 3 [980/997 (98%)]\tLoss: 0.098407\n",
      "Train Epoch: 3 [990/997 (99%)]\tLoss: 0.053178\n",
      "input and target sent to CUDA\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1737, 0.1450, 0.1502,  ..., 0.1994, 0.2100, 0.2041], device='cuda:0')\n",
      "t tensor([0.0047, 0.0047, 0.0047,  ..., 0.3258, 0.3257, 0.3256], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1835, 0.1821, 0.1716,  ..., 0.2931, 0.2783, 0.2905], device='cuda:0')\n",
      "t tensor([0.0247, 0.0247, 0.0247,  ..., 0.0017, 0.0020, 0.0021], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1589, 0.1713, 0.1470,  ..., 0.2176, 0.2512, 0.2214], device='cuda:0')\n",
      "t tensor([0.0348, 0.0348, 0.0348,  ..., 0.4665, 0.4665, 0.4665], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1780, 0.1817, 0.1722,  ..., 0.2901, 0.2757, 0.2871], device='cuda:0')\n",
      "t tensor([0.6048, 0.6048, 0.6047,  ..., 0.9539, 0.9540, 0.9540], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1407, 0.1451, 0.1411,  ..., 0.1871, 0.2010, 0.1904], device='cuda:0')\n",
      "t tensor([0.7523, 0.7524, 0.7524,  ..., 0.9628, 0.9628, 0.9628], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1413, 0.1443, 0.1414,  ..., 0.1870, 0.2000, 0.1905], device='cuda:0')\n",
      "t tensor([0.5879, 0.5898, 0.5951,  ..., 0.9336, 0.9336, 0.9336], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1557, 0.1443, 0.1421,  ..., 0.1879, 0.2010, 0.1920], device='cuda:0')\n",
      "t tensor([0.3946, 0.3929, 0.3882,  ..., 0.9903, 0.9904, 0.9904], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1772, 0.1548, 0.1557,  ..., 0.2772, 0.2747, 0.2859], device='cuda:0')\n",
      "t tensor([0.0886, 0.0886, 0.0887,  ..., 0.8846, 0.8844, 0.8844], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1598, 0.1462, 0.1419,  ..., 0.1904, 0.2078, 0.2006], device='cuda:0')\n",
      "t tensor([0.7348, 0.7348, 0.7347,  ..., 0.5635, 0.5635, 0.5635], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1568, 0.1448, 0.1417,  ..., 0.1884, 0.2015, 0.1929], device='cuda:0')\n",
      "t tensor([0.5335, 0.5334, 0.5330,  ..., 0.6007, 0.6014, 0.6016], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1581, 0.1462, 0.1424,  ..., 0.1895, 0.2046, 0.1957], device='cuda:0')\n",
      "t tensor([0.4988, 0.4988, 0.4991,  ..., 0.9337, 0.9338, 0.9338], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1606, 0.1477, 0.1422,  ..., 0.1915, 0.2057, 0.1990], device='cuda:0')\n",
      "t tensor([0.0616, 0.0617, 0.0620,  ..., 0.6313, 0.6312, 0.6312], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1551, 0.1445, 0.1421,  ..., 0.1877, 0.2014, 0.1916], device='cuda:0')\n",
      "t tensor([0.2780, 0.2779, 0.2775,  ..., 0.3864, 0.3865, 0.3865], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1590, 0.1488, 0.1420,  ..., 0.1927, 0.2095, 0.2005], device='cuda:0')\n",
      "t tensor([0.2567, 0.2565, 0.2558,  ..., 0.2739, 0.2738, 0.2738], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1670, 0.1443, 0.1430,  ..., 0.1899, 0.2036, 0.1961], device='cuda:0')\n",
      "t tensor([0.7539, 0.7537, 0.7530,  ..., 0.9571, 0.9571, 0.9571], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1593, 0.1474, 0.1420,  ..., 0.1910, 0.2058, 0.1982], device='cuda:0')\n",
      "t tensor([0.2040, 0.2035, 0.2020,  ..., 0.4414, 0.4413, 0.4413], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1513, 0.1500, 0.1411,  ..., 0.1929, 0.2094, 0.1992], device='cuda:0')\n",
      "t tensor([0.2856, 0.2856, 0.2854,  ..., 0.8735, 0.8735, 0.8735], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1579, 0.1470, 0.1419,  ..., 0.1904, 0.2049, 0.1973], device='cuda:0')\n",
      "t tensor([0.1827, 0.1823, 0.1809,  ..., 0.9637, 0.9637, 0.9637], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o tensor([0.1603, 0.1481, 0.1420,  ..., 0.1913, 0.2060, 0.1986], device='cuda:0')\n",
      "t tensor([0.3747, 0.3743, 0.3730,  ..., 0.6949, 0.6949, 0.6949], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1596, 0.1474, 0.1421,  ..., 0.1911, 0.2058, 0.1984], device='cuda:0')\n",
      "t tensor([0.2302, 0.2302, 0.2301,  ..., 0.6374, 0.6371, 0.6370], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1518, 0.1443, 0.1415,  ..., 0.1886, 0.2027, 0.1936], device='cuda:0')\n",
      "t tensor([0.2914, 0.2912, 0.2906,  ..., 0.3812, 0.3812, 0.3811], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1608, 0.1477, 0.1422,  ..., 0.1915, 0.2057, 0.1990], device='cuda:0')\n",
      "t tensor([0.8507, 0.8507, 0.8507,  ..., 0.1821, 0.1822, 0.1823], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1590, 0.1488, 0.1420,  ..., 0.1927, 0.2095, 0.2005], device='cuda:0')\n",
      "t tensor([0.9335, 0.9334, 0.9333,  ..., 0.0684, 0.0683, 0.0683], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1638, 0.1445, 0.1448,  ..., 0.1903, 0.2037, 0.1949], device='cuda:0')\n",
      "t tensor([0.9413, 0.9413, 0.9414,  ..., 0.7688, 0.7688, 0.7688], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1599, 0.1472, 0.1421,  ..., 0.1910, 0.2055, 0.1982], device='cuda:0')\n",
      "t tensor([0.6306, 0.6306, 0.6305,  ..., 0.6907, 0.6909, 0.6909], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1563, 0.1458, 0.1427,  ..., 0.1918, 0.2113, 0.2044], device='cuda:0')\n",
      "t tensor([0.0256, 0.0256, 0.0255,  ..., 0.9322, 0.9322, 0.9322], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1304, 0.1415, 0.1404,  ..., 0.1860, 0.1982, 0.1873], device='cuda:0')\n",
      "t tensor([0.1775, 0.1775, 0.1775,  ..., 0.9676, 0.9676, 0.9676], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1632, 0.1494, 0.1425,  ..., 0.1931, 0.2082, 0.2007], device='cuda:0')\n",
      "t tensor([0.5727, 0.5722, 0.5708,  ..., 0.3124, 0.3121, 0.3120], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1596, 0.1464, 0.1420,  ..., 0.1896, 0.2031, 0.1957], device='cuda:0')\n",
      "t tensor([0.2348, 0.2342, 0.2325,  ..., 0.4525, 0.4524, 0.4524], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1606, 0.1478, 0.1422,  ..., 0.1915, 0.2058, 0.1990], device='cuda:0')\n",
      "t tensor([0.7909, 0.7908, 0.7906,  ..., 0.8627, 0.8628, 0.8628], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1603, 0.1476, 0.1421,  ..., 0.1914, 0.2058, 0.1988], device='cuda:0')\n",
      "t tensor([0.4285, 0.4283, 0.4279,  ..., 0.8922, 0.8919, 0.8919], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1606, 0.1477, 0.1422,  ..., 0.1915, 0.2057, 0.1990], device='cuda:0')\n",
      "t tensor([0.1388, 0.1376, 0.1339,  ..., 0.4811, 0.4811, 0.4811], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1548, 0.1457, 0.1422,  ..., 0.1893, 0.2052, 0.1954], device='cuda:0')\n",
      "t tensor([0.1970, 0.1972, 0.1978,  ..., 0.3019, 0.3019, 0.3019], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1596, 0.1474, 0.1421,  ..., 0.1911, 0.2058, 0.1984], device='cuda:0')\n",
      "t tensor([0.4834, 0.4836, 0.4842,  ..., 0.2392, 0.2392, 0.2392], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1705, 0.1724, 0.1438,  ..., 0.2265, 0.2618, 0.2270], device='cuda:0')\n",
      "t tensor([0.2179, 0.2179, 0.2179,  ..., 0.2706, 0.2705, 0.2703], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1778, 0.1686, 0.1508,  ..., 0.2661, 0.2717, 0.2737], device='cuda:0')\n",
      "t tensor([0.0409, 0.0409, 0.0409,  ..., 0.7987, 0.7987, 0.7987], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1438, 0.1443, 0.1419,  ..., 0.1883, 0.2039, 0.1960], device='cuda:0')\n",
      "t tensor([0.0342, 0.0342, 0.0342,  ..., 0.9659, 0.9659, 0.9659], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1586, 0.1453, 0.1420,  ..., 0.1891, 0.2026, 0.1946], device='cuda:0')\n",
      "t tensor([0.3493, 0.3492, 0.3487,  ..., 0.2131, 0.2133, 0.2134], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1575, 0.1461, 0.1419,  ..., 0.1888, 0.2026, 0.1939], device='cuda:0')\n",
      "t tensor([0.5021, 0.5020, 0.5020,  ..., 0.3519, 0.3519, 0.3519], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1606, 0.1477, 0.1422,  ..., 0.1915, 0.2057, 0.1989], device='cuda:0')\n",
      "t tensor([0.5981, 0.5980, 0.5979,  ..., 0.7961, 0.7961, 0.7961], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1585, 0.1473, 0.1425,  ..., 0.1911, 0.2073, 0.1985], device='cuda:0')\n",
      "t tensor([0.7488, 0.7488, 0.7488,  ..., 0.8285, 0.8285, 0.8285], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1536, 0.1453, 0.1413,  ..., 0.1887, 0.2061, 0.1960], device='cuda:0')\n",
      "t tensor([0.0456, 0.0458, 0.0463,  ..., 0.0027, 0.0027, 0.0027], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1383, 0.1431, 0.1423,  ..., 0.1869, 0.2007, 0.1888], device='cuda:0')\n",
      "t tensor([0.5312, 0.5311, 0.5311,  ..., 0.1408, 0.1408, 0.1408], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1835, 0.1871, 0.1746,  ..., 0.3036, 0.2863, 0.3136], device='cuda:0')\n",
      "t tensor([0.2914, 0.2914, 0.2913,  ..., 0.9984, 0.9984, 0.9984], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1830, 0.1851, 0.1747,  ..., 0.3037, 0.2863, 0.3131], device='cuda:0')\n",
      "t tensor([0.5180, 0.5180, 0.5180,  ..., 0.9702, 0.9702, 0.9702], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1766, 0.1849, 0.1732,  ..., 0.3038, 0.2863, 0.3134], device='cuda:0')\n",
      "t tensor([0.1826, 0.1826, 0.1827,  ..., 0.9062, 0.9062, 0.9063], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1752, 0.1569, 0.1527,  ..., 0.2414, 0.2579, 0.2605], device='cuda:0')\n",
      "t tensor([0.1136, 0.1137, 0.1137,  ..., 0.9989, 0.9989, 0.9989], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1497, 0.1431, 0.1417,  ..., 0.1871, 0.2004, 0.1897], device='cuda:0')\n",
      "t tensor([0.4704, 0.4702, 0.4699,  ..., 0.0199, 0.0199, 0.0199], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1596, 0.1474, 0.1421,  ..., 0.1911, 0.2058, 0.1984], device='cuda:0')\n",
      "t tensor([0.9487, 0.9486, 0.9482,  ..., 0.5739, 0.5738, 0.5738], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1558, 0.1467, 0.1419,  ..., 0.1894, 0.2047, 0.1963], device='cuda:0')\n",
      "t tensor([0.2798, 0.2796, 0.2791,  ..., 0.7839, 0.7840, 0.7840], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1593, 0.1464, 0.1421,  ..., 0.1901, 0.2051, 0.1968], device='cuda:0')\n",
      "t tensor([0.1696, 0.1689, 0.1668,  ..., 0.8651, 0.8650, 0.8650], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1599, 0.1493, 0.1421,  ..., 0.1933, 0.2095, 0.2013], device='cuda:0')\n",
      "t tensor([0.7236, 0.7236, 0.7235,  ..., 0.6560, 0.6561, 0.6562], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1647, 0.1464, 0.1425,  ..., 0.1911, 0.2039, 0.1982], device='cuda:0')\n",
      "t tensor([0.6936, 0.6936, 0.6935,  ..., 0.7810, 0.7810, 0.7810], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1776, 0.1884, 0.1753,  ..., 0.3036, 0.2865, 0.3132], device='cuda:0')\n",
      "t tensor([0.0778, 0.0778, 0.0777,  ..., 0.9969, 0.9969, 0.9969], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1594, 0.1472, 0.1422,  ..., 0.1906, 0.2057, 0.1975], device='cuda:0')\n",
      "t tensor([0.9165, 0.9165, 0.9165,  ..., 0.3030, 0.3032, 0.3032], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1549, 0.1446, 0.1420,  ..., 0.1877, 0.2015, 0.1914], device='cuda:0')\n",
      "t tensor([0.4204, 0.4204, 0.4205,  ..., 0.3603, 0.3601, 0.3601], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1616, 0.1489, 0.1421,  ..., 0.1931, 0.2078, 0.2010], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t tensor([0.6500, 0.6501, 0.6502,  ..., 0.7238, 0.7240, 0.7240], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1803, 0.1877, 0.1708,  ..., 0.3037, 0.2865, 0.3132], device='cuda:0')\n",
      "t tensor([0.2068, 0.2069, 0.2069,  ..., 0.8895, 0.8895, 0.8894], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1578, 0.1449, 0.1425,  ..., 0.1885, 0.2022, 0.1939], device='cuda:0')\n",
      "t tensor([0.0011, 0.0011, 0.0011,  ..., 0.9777, 0.9777, 0.9777], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1413, 0.1443, 0.1414,  ..., 0.1870, 0.2000, 0.1905], device='cuda:0')\n",
      "t tensor([0.0587, 0.0587, 0.0587,  ..., 0.9461, 0.9463, 0.9464], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1493, 0.1428, 0.1416,  ..., 0.1871, 0.2000, 0.1898], device='cuda:0')\n",
      "t tensor([0.1505, 0.1516, 0.1547,  ..., 0.3629, 0.3629, 0.3630], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1591, 0.1478, 0.1423,  ..., 0.1916, 0.2080, 0.1992], device='cuda:0')\n",
      "t tensor([0.2263, 0.2268, 0.2284,  ..., 0.6039, 0.6040, 0.6041], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1606, 0.1478, 0.1422,  ..., 0.1915, 0.2058, 0.1990], device='cuda:0')\n",
      "t tensor([0.6393, 0.6394, 0.6397,  ..., 0.9386, 0.9385, 0.9385], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1574, 0.1461, 0.1418,  ..., 0.1897, 0.2052, 0.1987], device='cuda:0')\n",
      "t tensor([0.6337, 0.6337, 0.6334,  ..., 0.2633, 0.2631, 0.2630], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1589, 0.1472, 0.1420,  ..., 0.1903, 0.2055, 0.1971], device='cuda:0')\n",
      "t tensor([0.7495, 0.7496, 0.7497,  ..., 0.1181, 0.1182, 0.1182], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1557, 0.1443, 0.1421,  ..., 0.1879, 0.2010, 0.1920], device='cuda:0')\n",
      "t tensor([0.8907, 0.8907, 0.8908,  ..., 0.0620, 0.0618, 0.0618], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1591, 0.1468, 0.1419,  ..., 0.1899, 0.2044, 0.1965], device='cuda:0')\n",
      "t tensor([0.2725, 0.2721, 0.2709,  ..., 0.5479, 0.5479, 0.5479], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1637, 0.1446, 0.1436,  ..., 0.1896, 0.2030, 0.1951], device='cuda:0')\n",
      "t tensor([0.8225, 0.8225, 0.8223,  ..., 0.2745, 0.2745, 0.2745], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1603, 0.1449, 0.1437,  ..., 0.1893, 0.2026, 0.1956], device='cuda:0')\n",
      "t tensor([0.4347, 0.4347, 0.4347,  ..., 0.5438, 0.5438, 0.5439], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1543, 0.1443, 0.1426,  ..., 0.1878, 0.2027, 0.1945], device='cuda:0')\n",
      "t tensor([0.1683, 0.1683, 0.1683,  ..., 0.0104, 0.0104, 0.0104], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1717, 0.1745, 0.1476,  ..., 0.2493, 0.2644, 0.2397], device='cuda:0')\n",
      "t tensor([0.0155, 0.0155, 0.0155,  ..., 0.2109, 0.2109, 0.2109], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1604, 0.1434, 0.1449,  ..., 0.1888, 0.2016, 0.1923], device='cuda:0')\n",
      "t tensor([0.0041, 0.0041, 0.0041,  ..., 0.1236, 0.1236, 0.1236], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1665, 0.1437, 0.1428,  ..., 0.1893, 0.2034, 0.1954], device='cuda:0')\n",
      "t tensor([0.2529, 0.2529, 0.2529,  ..., 0.0449, 0.0448, 0.0447], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1511, 0.1563, 0.1466,  ..., 0.1996, 0.2212, 0.2069], device='cuda:0')\n",
      "t tensor([7.9046e-05, 7.8999e-05, 7.8867e-05,  ..., 4.1099e-01, 4.1101e-01,\n",
      "        4.1101e-01], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1683, 0.1766, 0.1546,  ..., 0.2496, 0.2513, 0.2435], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1649, 0.1504, 0.1422,  ..., 0.1951, 0.2110, 0.2032], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1553, 0.1463, 0.1424,  ..., 0.1901, 0.2062, 0.1975], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1522, 0.1458, 0.1421,  ..., 0.1884, 0.2044, 0.1938], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1434, 0.1438, 0.1415,  ..., 0.1871, 0.2004, 0.1905], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1419, 0.1436, 0.1410,  ..., 0.1866, 0.1991, 0.1891], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1622, 0.1505, 0.1423,  ..., 0.1915, 0.2104, 0.1998], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1756, 0.1813, 0.1590,  ..., 0.2753, 0.2774, 0.2695], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1658, 0.1850, 0.1594,  ..., 0.2821, 0.2814, 0.2710], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1737, 0.1530, 0.1510,  ..., 0.2121, 0.2279, 0.2187], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1411, 0.1425, 0.1605,  ..., 0.1913, 0.1991, 0.1954], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1814, 0.1822, 0.1772,  ..., 0.2981, 0.2779, 0.2971], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1647, 0.1822, 0.1571,  ..., 0.2652, 0.2765, 0.2570], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1466, 0.1440, 0.1693,  ..., 0.2013, 0.2015, 0.2058], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1654, 0.1805, 0.1620,  ..., 0.2647, 0.2686, 0.2553], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1486, 0.1546, 0.1534,  ..., 0.2059, 0.2166, 0.2105], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1398, 0.1458, 0.1618,  ..., 0.1978, 0.2037, 0.2035], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1693, 0.1818, 0.1634,  ..., 0.2807, 0.2768, 0.2730], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1641, 0.1421, 0.1644,  ..., 0.1964, 0.2004, 0.2037], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1797, 0.1841, 0.1721,  ..., 0.2952, 0.2791, 0.2917], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1461, 0.1405, 0.1525,  ..., 0.1875, 0.1978, 0.1907], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1713, 0.1714, 0.1517,  ..., 0.2370, 0.2557, 0.2352], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1766, 0.1481, 0.1661,  ..., 0.2287, 0.2263, 0.2358], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1730, 0.1631, 0.1529,  ..., 0.2352, 0.2447, 0.2368], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1756, 0.1563, 0.1737,  ..., 0.2665, 0.2435, 0.2637], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "correct 0\n",
      "--------------------------\n",
      "o tensor([0.1506, 0.1601, 0.1469,  ..., 0.2037, 0.2226, 0.2089], device='cuda:0')\n",
      "t tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
      "-----------------------\n",
      "[0.3525697973107502, 0.20350569114462663, 0.19294637319264762, tensor(0.1915, device='cuda:0'), tensor(0.1936, device='cuda:0'), tensor(0.1925, device='cuda:0'), tensor(0.1946, device='cuda:0'), tensor(0.1940, device='cuda:0'), tensor(0.1942, device='cuda:0')]\n",
      "\n",
      "Test set: Avg. loss: 0.1942, Accuracy: 0/997 (0%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# CE E IN JOS DE AICI ERA TESTARE MAI VECHE, O PASTREZ JUST IN CASE\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    mask = train(epoch)\n",
    "    test()  \n",
    "#     split_audios_epoch(epoch, mask)\n",
    "    \n",
    "    plt.plot(train_losses)\n",
    "    plt.title('TRAIN losses')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(test_losses)\n",
    "    plt.title('TEST losses')\n",
    "    plt.show()\n",
    "\n",
    "print(\"DONE\")\n",
    "# print(len(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    mask = []\n",
    "    network.train()\n",
    "    \n",
    "    # input is an AF, target is the mask for the PF of the current AF\n",
    "    for index, (input, target) in enumerate(train_set.items()):\n",
    "        \n",
    "        assert(input.shape == (4, 1, 513, 11))\n",
    "        assert(target.shape == (513, 11))\n",
    "\n",
    "        target_view = target.view(-1, target.shape[0] * target.shape[1])\n",
    "        \n",
    "        # if cuda is available, send (input, target) to gpu\n",
    "        if torch.cuda.is_available():\n",
    "            input, target_view = input.cuda(), target_view.cuda()\n",
    "            if index == 0:\n",
    "                print(\"input and target sent to CUDA\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1. forward propagation\n",
    "        output = network(input)\n",
    "        \n",
    "        # 2. loss calculation\n",
    "        loss = loss_function(output[len(output)-1], target_view[0])       \n",
    "        \n",
    "        # 3. backward propagation\n",
    "        loss.backward() \n",
    "        \n",
    "        # 4. weight optimization\n",
    "        optimizer.step()\n",
    "            \n",
    "        \n",
    "        current_mask = output[len(output)-1].view(target.shape[0], target.shape[1])\n",
    "        mask.append(output[len(output)-1])\n",
    "\n",
    "        if index % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, index, len(train_set),\n",
    "            100. * index / len(train_set), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (index) + ((epoch-1)*len(train_set.items())))\n",
    "#             torch.save(network.state_dict(), './results/model.pth')\n",
    "#             torch.save(optimizer.state_dict(), './results/optimizer.pth')\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for index, (input, target) in enumerate(test_set.items()):\n",
    "            \n",
    "            # if cuda is available, send (input, target) to gpu\n",
    "            target_view = target.view(-1, target.shape[0] * target.shape[1])\n",
    "            if torch.cuda.is_available():\n",
    "                input, target_view = input.cuda(), target_view.cuda()\n",
    "                if index == 0:\n",
    "                    print(\"input and target sent to CUDA\")\n",
    "            \n",
    "            output = network(input)\n",
    "            \n",
    "            test_loss += loss_function(output[len(output)-1], target_view[0])\n",
    "            \n",
    "            correct += (int)(torch.eq(output[len(output)-1], target_view[0]).sum())\n",
    "            \n",
    "#             print(\"test loss\",test_loss)\n",
    "            if index % 100 == 0:\n",
    "                print(\"correct\",correct)\n",
    "                print(\"--------------------------\")\n",
    "                print(\"o\",output[len(output)-1])\n",
    "                print(\"t\",target_view[0])\n",
    "                print(\"-----------------------\")\n",
    "\n",
    "#             break\n",
    "            \n",
    "    test_loss /= len(test_set)\n",
    "    test_losses.append(test_loss)\n",
    "    print(test_losses)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_set),\n",
    "    100. * correct / len(test_set)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
