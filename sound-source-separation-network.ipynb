{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa, librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import time\n",
    "\n",
    "n_fft = 1024\n",
    "hop_length = 8\n",
    "sr = 16000\n",
    "\n",
    "# cate bucati PF iau in considerare pentru AF -> cu overlapping => 15 frame-uri \n",
    "# ultimul frame din AF fiind PF-ul curent\n",
    "nb_of_frames_in_AF = 4\n",
    "# exp_w_avg_beta = 0.98\n",
    "# marja_eroare = 0.01\n",
    "\n",
    "# frame_length_ms = 5\n",
    "# total_length_ms = 5000.0\n",
    "# nb_of_samples = 80000\n",
    "samples_per_frame_10ms = 160 #(int)(160000 * 10 / 10000.0)\n",
    "samples_per_frame_5ms = 80 #(int)(80000 * 5 / 5000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliz(mix):\n",
    "    mu = np.mean(mix, axis=0)\n",
    "    var = np.var(mix, axis=0)\n",
    "\n",
    "    mix_norm = (mix - mu) / np.sqrt(var + 1e-8)\n",
    "    \n",
    "    return mix_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(stft_1, stft_2):\n",
    "    # small epsilon to avoid dividing by zero\n",
    "    eps = np.finfo(np.float).eps\n",
    "\n",
    "    # compute model as the sum of spectrograms\n",
    "    mix = eps + np.abs(stft_1) + np.abs(stft_2)    \n",
    "    mask = np.divide(np.abs(stft_1), mix)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STFT_AF_frames(AF_array, PF_size):\n",
    "    frames = []\n",
    "    half = (int)(PF_size/2)\n",
    "    frames_added = 0\n",
    "    \n",
    "    i = AF_array.shape[0]-1\n",
    "    \n",
    "    # use AF as 4 times bigger than PF's size -> PF = 10 ms => AF = 40 ms\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 1. add PF[i]\n",
    "    frame = librosa.stft(librosa.to_mono(AF_array[i]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "    frame = normaliz(frame)\n",
    "    \n",
    "    tensor = []\n",
    "    tensor.append(frame)        \n",
    "    frames.append(tensor)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 2. add PF[i-1, i] -> overlap\n",
    "    overlap_frame = AF_array[i-1][half:]\n",
    "    overlap_frame = np.concatenate((overlap_frame, AF_array[i][:half]))\n",
    "    \n",
    "    frame = librosa.stft(librosa.to_mono(overlap_frame), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "    frame = normaliz(frame)\n",
    "\n",
    "    tensor = []\n",
    "    tensor.append(frame)\n",
    "    frames.append(tensor)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 3. add PF[i - 1]\n",
    "    frame = librosa.stft(librosa.to_mono(AF_array[i - 1]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "    frame = normaliz(frame)\n",
    "    \n",
    "    tensor = []\n",
    "    tensor.append(frame)        \n",
    "    frames.append(tensor)\n",
    "    \n",
    "    #---------------------------------------------------------------------------\n",
    "    # 4. add PF[i-1, i] -> overlap\n",
    "    overlap_frame = AF_array[i-1][half:]\n",
    "    overlap_frame = np.concatenate((overlap_frame, AF_array[i][:half]))\n",
    "    \n",
    "    frame = librosa.stft(librosa.to_mono(overlap_frame), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "    frame = np.abs(frame)\n",
    "    frame = normaliz(frame)\n",
    "\n",
    "    tensor = []\n",
    "    tensor.append(frame)\n",
    "    frames.append(tensor)\n",
    "        \n",
    "    \n",
    "    return np.asarray(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_set_for_mix(mix, voice_1, voice_2, samples_per_frame):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    mix_frames = np.array([mix[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    voice_1_frames = np.array([voice_1[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    voice_2_frames = np.array([voice_2[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    \n",
    "    for index in range(mix_frames.shape[0]):\n",
    "        \n",
    "        if index < 2:\n",
    "            continue\n",
    "        \n",
    "        # 1. create train set input for AF with current PF and previous frames\n",
    "        AF_frames = np.array([mix_frames[i] for i in range(index - 2, index)])      \n",
    "        AF_STFT_frames = get_STFT_AF_frames(AF_array=AF_frames, PF_size= samples_per_frame)\n",
    "        inputs.append(AF_STFT_frames)\n",
    "        \n",
    "        # 2. create train set target for that AF, containing the mask for the current PF\n",
    "        stft_voice_1 = librosa.stft(librosa.to_mono(voice_1_frames[index]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        stft_voice_2 = librosa.stft(librosa.to_mono(voice_2_frames[index]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        mask = my_utils.compute_mask(stft_voice_1, stft_voice_2)\n",
    "        \n",
    "        targets.append(mask)\n",
    "    \n",
    "    # train set for only one audio\n",
    "    train_set_input = torch.from_numpy(np.array(inputs))\n",
    "    \n",
    "    # target contains the calculated masks for each PF from mix\n",
    "    train_set_target = torch.from_numpy(np.array(targets))\n",
    "    return np.array(inputs), np.array(targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    height = 0\n",
    "    width = 0\n",
    "    \n",
    "    # default values for height and width are given for a processing frame of 10 ms\n",
    "    # 5 ms  = [513, 11]\n",
    "    def __init__(self, height = 513, width = 21):\n",
    "        super(Network, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # layer 1\n",
    "        self.fc1 = nn.Linear(1 * height * width, 250)\n",
    "        self.fc1_batch = nn.BatchNorm1d(250)\n",
    "        \n",
    "        # layer 2\n",
    "        self.fc2 = nn.Linear(250, 250)\n",
    "        self.fc2_batch = nn.BatchNorm1d(250)\n",
    "        \n",
    "        # layer 3\n",
    "        self.fc3 = nn.Linear(250, height * width)\n",
    "        self.fc3_batch = nn.BatchNorm1d(height * width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # layer 1\n",
    "        # firstly, transform the matrix into an array for the FC\n",
    "        x = x.view(-1, self.height * self.width)        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 3\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc3_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = x.view(nb_of_frames_in_AF, self.height, self.width) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkConv(nn.Module):\n",
    "    height = 0\n",
    "    width = 0\n",
    "    \n",
    "    # default values for height and width are given for a processing frame of 10 ms\n",
    "    # 5 ms  = [513, 11]\n",
    "    def __init__(self, height = 513, width = 21):\n",
    "        super(NetworkConv, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, padding=2)\n",
    "        self.conv1_batch = nn.BatchNorm2d(20)\n",
    "        \n",
    "        # layer 2\n",
    "        self.conv2 = nn.Conv2d(20, 20, kernel_size=5, padding=2)\n",
    "        self.conv2_batch = nn.BatchNorm2d(20)\n",
    "    \n",
    "        # layer 3\n",
    "        self.fc1 = nn.Linear(20 * height * width, 250)\n",
    "        self.fc1_batch = nn.BatchNorm1d(250)\n",
    "        \n",
    "        # layer 4\n",
    "        self.fc2 = nn.Linear(250, height * width)\n",
    "        self.fc2_batch = nn.BatchNorm1d(height * width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # firstly, transform the matrix into an array for the FC\n",
    "        \n",
    "        \n",
    "        # layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv1_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 3\n",
    "        x = x.view(-1, self.height * self.width * 20) \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # layer 4\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2_batch(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = x.view(nb_of_frames_in_AF, self.height, self.width) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mix_of_10_seconds(voice):\n",
    "    nb_sample_for_10_seconds = 160000\n",
    "    \n",
    "    if len(voice) < nb_sample_for_10_seconds:\n",
    "        voice = np.pad(voice, (0,(nb_sample_for_10_seconds - len(voice) )), 'constant', constant_values=(0))\n",
    "    elif len(voice) > nb_sample_for_10_seconds:\n",
    "        voice = voice[0:nb_sample_for_10_seconds]\n",
    "    \n",
    "    return voice        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mix_of_5_seconds(voice):\n",
    "    nb_sample_for_5_seconds = 80000\n",
    "    \n",
    "    if len(voice) < nb_sample_for_5_seconds:\n",
    "        voice = np.pad(voice, (0,(nb_sample_for_5_seconds - len(voice) )), 'constant', constant_values=(0))\n",
    "    elif len(voice) > nb_sample_for_5_seconds:\n",
    "        voice = voice[0:nb_sample_for_5_seconds]\n",
    "    \n",
    "    return voice   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_from_voices_lists(voice1_file_names_list, voice2_file_names_list, \n",
    "                                 samples_per_frame, mix_5_seconds):\n",
    "    inputs_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    for voice1_file_name in voice1_file_names_list:\n",
    "        for voice2_file_name in voice2_file_names_list:\n",
    "            \n",
    "            voice1, sr = librosa.load(voice1_file_name, sr=16000) \n",
    "            voice1 = np.append(voice1, voice1)\n",
    "            \n",
    "            voice2, sr = librosa.load(voice2_file_name, sr=16000)\n",
    "            voice2 = np.append(voice2, voice2)\n",
    "\n",
    "            # pad smaller array with zeros if it's the case or delete last entries\n",
    "            if mix_5_seconds == True:\n",
    "                voice1 = make_mix_of_5_seconds(voice1)\n",
    "                voice2 = make_mix_of_5_seconds(voice2)\n",
    "            else:\n",
    "                voice1 = make_mix_of_10_seconds(voice1)\n",
    "                voice2 = make_mix_of_10_seconds(voice2)\n",
    "\n",
    "            # load the mixed audio \n",
    "            mix = voice1 + voice2\n",
    "\n",
    "            voice1 = np.array(voice1)\n",
    "            voice2 = np.array(voice2)\n",
    "            mix = np.array(mix)\n",
    "\n",
    "            inputs, targets = get_train_set_for_mix(mix, voice1, voice2, samples_per_frame)\n",
    "            inputs_list.extend(inputs)\n",
    "            targets_list.extend(targets)\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    \n",
    "    print(\"dims for set: \", inputs_list.shape, targets_list.shape)\n",
    "    \n",
    "    network_set = dict(zip(inputs_list, targets_list))\n",
    "    \n",
    "    return network_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_with_noise_from_voices_lists(voice1_file_names_list, voice2_file_names_list, \n",
    "                                            samples_per_frame, mix_5_seconds, noise_filename):\n",
    "    inputs_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    for voice1_file_name in voice1_file_names_list:\n",
    "        for voice2_file_name in voice2_file_names_list:\n",
    "            \n",
    "            voice1, sr = librosa.load(voice1_file_name, sr=16000) \n",
    "            voice1 = np.append(voice1, voice1)\n",
    "            \n",
    "            voice2, sr = librosa.load(voice2_file_name, sr=16000)\n",
    "            voice2 = np.append(voice2, voice2)\n",
    "\n",
    "            noise, sr = librosa.load(noise_filename, sr=16000) \n",
    "            noise = np.append(noise, noise)\n",
    "\n",
    "            # pad smaller array with zeros if it's the case or delete last entries\n",
    "            if mix_5_seconds == True:\n",
    "                voice1 = make_mix_of_5_seconds(voice1)\n",
    "                voice2 = make_mix_of_5_seconds(voice2)\n",
    "                noise = make_mix_of_5_seconds(noise)\n",
    "            else:\n",
    "                voice1 = make_mix_of_10_seconds(voice1)\n",
    "                voice2 = make_mix_of_10_seconds(voice2)\n",
    "                noise = make_mix_of_10_seconds(noise)\n",
    "\n",
    "            # load the mixed audio \n",
    "            mix = voice1 + voice2 + noise\n",
    "\n",
    "            voice1 = np.array(voice1)\n",
    "            voice2 = np.array(voice2)\n",
    "            noise = np.array(noise)\n",
    "            mix = np.array(mix)\n",
    "\n",
    "            inputs, targets = get_train_set_for_mix(mix, voice1, voice2, samples_per_frame)\n",
    "            inputs_list.extend(inputs)\n",
    "            targets_list.extend(targets)\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    \n",
    "    print(\"dims for noise set: \", inputs_list.shape, targets_list.shape)\n",
    "    \n",
    "    network_set = dict(zip(inputs_list, targets_list))\n",
    "    \n",
    "    return network_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_from_voices_lists_80_10_10(voice1_file_names_list, voice2_file_names_list, \n",
    "                                          samples_per_frame, mix_5_seconds):\n",
    "    inputs_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    for voice1_file_name in voice1_file_names_list:\n",
    "        for voice2_file_name in voice2_file_names_list:\n",
    "            \n",
    "            voice1, sr = librosa.load(voice1_file_name, sr=16000) \n",
    "            voice1 = np.append(voice1, voice1)\n",
    "            \n",
    "            voice2, sr = librosa.load(voice2_file_name, sr=16000) \n",
    "            voice2 = np.append(voice2, voice2)\n",
    "\n",
    "            # pad smaller array with zeros if it's the case or delete last entries\n",
    "            if mix_5_seconds == True:\n",
    "                voice1 = make_mix_of_5_seconds(voice1)\n",
    "                voice2 = make_mix_of_5_seconds(voice2)\n",
    "            else:\n",
    "                voice1 = make_mix_of_10_seconds(voice1)\n",
    "                voice2 = make_mix_of_10_seconds(voice2)\n",
    "\n",
    "            # load the mixed audio \n",
    "            mix = voice1 + voice2\n",
    "\n",
    "            voice1 = np.array(voice1)\n",
    "            voice2 = np.array(voice2)\n",
    "            mix = np.array(mix)\n",
    "\n",
    "            inputs, targets = get_train_set_for_mix(mix, voice1, voice2, samples_per_frame)\n",
    "            inputs_list.extend(inputs)\n",
    "            targets_list.extend(targets)\n",
    "    \n",
    "    inputs_list = torch.from_numpy(np.array(inputs_list))\n",
    "    targets_list = torch.from_numpy(np.array(targets_list))\n",
    "    \n",
    "#     print(\"dims for 80-10-10 set: \", inputs_list.shape, targets_list.shape)\n",
    "    \n",
    "    train_len = (int)(80/100 * len(inputs_list))\n",
    "    m = (int)( (len(inputs_list) - train_len) / 2)\n",
    "\n",
    "    train_set = dict(zip(inputs_list[:train_len, :, :], targets_list[:train_len, :, :]))\n",
    "    valid_set = dict(zip(inputs_list[train_len:(train_len + m), :, :], targets_list[train_len:(train_len + m), :, :]))\n",
    "    test_set = dict(zip(inputs_list[(train_len + m):len(inputs_list), :, :], targets_list[(train_len + m):len(inputs_list), :, :]))\n",
    "\n",
    "    print(\"80-10-10 sets dim:\", len(train_set), len(valid_set), len(test_set))\n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80-10-10 sets dim: 398 50 50\n",
      "dims for set:  torch.Size([498, 4, 1, 513, 21]) torch.Size([498, 513, 21])\n",
      "dims for noise set:  torch.Size([498, 4, 1, 513, 21]) torch.Size([498, 513, 21])\n",
      "dims for set:  torch.Size([498, 4, 1, 513, 21]) torch.Size([498, 513, 21])\n",
      "dims for noise set:  torch.Size([498, 4, 1, 513, 21]) torch.Size([498, 513, 21])\n"
     ]
    }
   ],
   "source": [
    "noise_filename = 'recordings/noises/piano5sec.wav'\n",
    "noise2_filename = 'recordings/noises/engine.wav'\n",
    "noise3_filename = 'recordings/noises/beeps.wav'\n",
    "\n",
    "voice1_file_names_list = ['recordings/voice1/arctic_a0007.wav']\n",
    "voice2_file_names_list = ['recordings/voice2/arctic_a0032.wav']\n",
    "\n",
    "voice1_file_names_list2 = ['recordings/voice1/arctic_a0407.wav']\n",
    "voice2_file_names_list2 = ['recordings/voice2/arctic_a0032.wav']\n",
    "\n",
    "\n",
    "train_set_80, valid_set_10, test_set_10 = create_set_from_voices_lists_80_10_10(voice1_file_names_list, \n",
    "                                                                                      voice2_file_names_list,\n",
    "                                                                                     samples_per_frame=samples_per_frame_10ms,\n",
    "                                                                                     mix_5_seconds=True)\n",
    "\n",
    "# train_set_80_2, valid_set_10_2, test_set_10_2 = create_set_from_voices_lists_80_10_10(voice1_file_names_list2, \n",
    "#                                                                                       voice2_file_names_list2,\n",
    "#                                                                                      samples_per_frame=samples_per_frame_10ms,\n",
    "#                                                                                      mix_5_seconds=True)\n",
    "\n",
    "train_set = create_set_from_voices_lists(voice1_file_names_list, voice2_file_names_list, \n",
    "                                         samples_per_frame=samples_per_frame_10ms,\n",
    "                                        mix_5_seconds= True)\n",
    "\n",
    "valid_set = create_set_with_noise_from_voices_lists(voice1_file_names_list, voice2_file_names_list,\n",
    "                                                    samples_per_frame=samples_per_frame_10ms,\n",
    "                                                    mix_5_seconds= True, \n",
    "                                                    noise_filename=noise_filename)\n",
    "\n",
    "test_set = create_set_from_voices_lists(voice1_file_names_list, voice2_file_names_list,\n",
    "                                                    samples_per_frame=samples_per_frame_10ms,\n",
    "                                                    mix_5_seconds= True)\n",
    "\n",
    "test_set_with_noise = create_set_with_noise_from_voices_lists(voice1_file_names_list, voice2_file_names_list,\n",
    "                                                    samples_per_frame=samples_per_frame_10ms,\n",
    "                                                    mix_5_seconds= True, \n",
    "                                                    noise_filename=noise2_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_network(nr_epochs, train_set, valid_set, model_name, optimizer, network):\n",
    "\n",
    "    # plot train/valid loss contains losses on each epoch, so we can see after each epoch what happens with the error\n",
    "    plot_train_losses = []\n",
    "    plot_valid_losses = []\n",
    "    plot_train_accuracy = []\n",
    "    plot_valid_accuracy = []\n",
    "\n",
    "\n",
    "    epoch = 1\n",
    "    while epoch <= nr_epochs:\n",
    "        \n",
    "        correct_train = 0\n",
    "        correct_test = 0\n",
    "        train_losses, valid_losses = [], []\n",
    "        loss = 0\n",
    "\n",
    "        # training part \n",
    "        network.train()\n",
    "        for index, (input, target) in enumerate(train_set.items()):\n",
    "\n",
    "            # if cuda is available, send (input, target) to gpu\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1. forward propagation\n",
    "            output = network(input)\n",
    "\n",
    "\n",
    "            # 2. loss calculation\n",
    "            loss = loss_function(target, output[len(output)-1])  \n",
    "\n",
    "\n",
    "            # 3. backward propagation\n",
    "            loss.backward() \n",
    "\n",
    "\n",
    "            # 4. weight optimization\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # 5. save the loss for this PF\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            \n",
    "            # 6. check how many items where predicted correctly\n",
    "            correct_train += 1 if loss.item() <= marja_eroare else 0 \n",
    "            \n",
    "\n",
    "        # add the mean loss for this training epoch for ploting\n",
    "        train_mean_loss_for_epoch = np.mean(train_losses)\n",
    "        plot_train_losses.append(train_mean_loss_for_epoch)\n",
    "        \n",
    "        current_accuracy = 100. * correct_train / len(train_set)\n",
    "        plot_train_accuracy.append(current_accuracy)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(\"--------------------------------------Epoch\", str(epoch) ,\"------------------------------\")\n",
    "            print(\"Train set accuracy: {}/{} ({:.0f}%)\".format(correct_train, len(train_set), current_accuracy))\n",
    "        \n",
    "        #-----------------------------------------------------------------------\n",
    "        # evaluation part \n",
    "        network.eval()\n",
    "        if epoch % 1 == 0:\n",
    "#             \n",
    "            with torch.no_grad():\n",
    "                for index, (input, target) in enumerate(valid_set.items()):\n",
    "\n",
    "                    # if cuda is available, send (input, target) to gpu\n",
    "                    input, target = input.cuda(), target.cuda()\n",
    "\n",
    "                    # 1. forward propagation\n",
    "                    output = network.forward(input)\n",
    "\n",
    "                    # 2. loss calculation\n",
    "                    loss = loss_function(target, output[len(output)-1]).detach().item()\n",
    "                    \n",
    "                    # 3. save loss for current PF\n",
    "                    valid_losses.append(loss)\n",
    "\n",
    "                    # 4. check how many items where predicted correctly\n",
    "                    correct_test += 1 if loss <= marja_eroare else 0\n",
    "\n",
    "                 # add the mean loss for this training epoch for ploting\n",
    "                valid_mean_loss_for_epoch = np.mean(valid_losses)\n",
    "                plot_valid_losses.append(valid_mean_loss_for_epoch)\n",
    "                \n",
    "                current_accuracy = 100. * correct_test / len(valid_set)\n",
    "                plot_valid_accuracy.append(current_accuracy)\n",
    "\n",
    "\n",
    "                if epoch % 10 == 0 or epoch == 1:\n",
    "                    print(\"Valid set accuracy: {}/{} ({:.0f}%)\".format(correct_test, len(valid_set),current_accuracy))\n",
    "                \n",
    "                if current_accuracy > max(plot_valid_accuracy) :\n",
    "                    print(\"--------------------------------------EVAL epoch\", str(epoch) ,\"------------------------------\")\n",
    "                    torch.save(network, str(epoch) + \"valid\"+ str(current_accuracy) + model_name)\n",
    "                    network_test = torch.load(str(epoch) + \"valid\"+ str(current_accuracy) + model_name)\n",
    "                    eval(network=network_test, test_set=test_set)\n",
    "                    \n",
    "        epoch += 1\n",
    "\n",
    "    torch.save(network, model_name)\n",
    "    \n",
    "    plt.plot(plot_train_losses, label='Training loss')\n",
    "    plt.plot(plot_valid_losses, label='Validation loss')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(plot_train_accuracy, label='Training accuracy')\n",
    "    plt.plot(plot_valid_accuracy, label='Validation accuracy')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECONSTRUIRE SUNET PE FRAME-URI, FOLOSIND TARGETURILE GENERATE PT SETUL DE ANTRENARE\n",
    "def split_audios_epoch(name, mask, mix, samples_per_frame):\n",
    "    \n",
    "    frame_pos = (int)(nb_of_frames_in_AF/2)\n",
    "\n",
    "    mask_stack = torch.stack(mask)\n",
    "    cpu_mask = mask_stack.cpu()\n",
    "    n_mask = cpu_mask.detach().numpy()\n",
    "\n",
    "    mix_frames = np.array([mix[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    sound1 = np.empty([0,])\n",
    "    sound2 = np.empty([0,])\n",
    "\n",
    "\n",
    "    for i in range (0, len(mix_frames)):\n",
    "        stft_mix = librosa.stft(librosa.to_mono(mix_frames[i]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        # reteaua nu a invatat mastile pentru primele frame_pos bucati [PF] din mix\n",
    "        # deci las matricea STFT asa cum e in mix\n",
    "        if i < frame_pos :\n",
    "            y_frame_1_stft_with_mask = stft_mix\n",
    "            y_frame_2_stft_with_mask = stft_mix\n",
    "\n",
    "            inverse_sound1_stft = librosa.istft(y_frame_1_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "            inverse_sound2_stft = librosa.istft(y_frame_2_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "\n",
    "            sound1 = np.concatenate((sound1, inverse_sound1_stft))\n",
    "            sound2 = np.concatenate((sound2, inverse_sound2_stft))\n",
    "            \n",
    "        else:\n",
    "            # n_mask[i-7] pentru ca reteaua invata pt AF-uri facute pe cate 8 PF-uri -> 15 items pt AF\n",
    "            # abia de cand ajunge la primul AF care s a putut compune, folosesc masca invata de retea\n",
    "            \n",
    "            y_frame_1_stft_with_mask = np.multiply(n_mask[i-frame_pos], stft_mix)\n",
    "            y_frame_2_stft_with_mask = np.multiply((1 -  n_mask[i-frame_pos]), stft_mix)\n",
    "\n",
    "            inverse_sound1_stft = librosa.istft(y_frame_1_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "            inverse_sound2_stft = librosa.istft(y_frame_2_stft_with_mask, hop_length=hop_length, window='hann')\n",
    "\n",
    "            sound1 = np.concatenate((sound1, inverse_sound1_stft))\n",
    "            sound2 = np.concatenate((sound2, inverse_sound2_stft))\n",
    "\n",
    "\n",
    "    librosa.output.write_wav(\"recordings/voice1-\"+ str(name) + \".wav\", sound1, sr = 16000)\n",
    "    librosa.output.write_wav(\"recordings/voice2-\"+ str(name) + \".wav\", sound2, sr = 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(network, test_set):\n",
    "\n",
    "#     network.eval()\n",
    "                \n",
    "    test_loss, test_mask, test_accuracy = [], [], []\n",
    "    correct_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, (input, target) in enumerate(test_set.items()):\n",
    "\n",
    "            # if cuda is available, send (input, target) to gpu\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # 1. forward propagation\n",
    "            output = network.forward(input)\n",
    "\n",
    "            # 2. loss calculation\n",
    "            loss = loss_function(target, output[len(output)-1]).detach().item()\n",
    "\n",
    "            # 3. save the mask for the current PF, meaning the last entry in the output\n",
    "            current_mask = output[len(output)-1]\n",
    "            test_mask.append(current_mask)\n",
    "\n",
    "            test_loss.append(loss)\n",
    "\n",
    "            # 4. check how many items where predicted correctly\n",
    "            correct_test += 1 if loss <= marja_eroare else 0\n",
    "\n",
    "\n",
    "        current_accuracy = 100. * correct_test / len(test_set)\n",
    "        print(\"--------------------------------------------- Evaluation ------------------------------------\")\n",
    "        print(\"Test mix loss:\", np.mean(test_loss))\n",
    "        print(\"Mix accuracy: \", (int)(current_accuracy), '%')\n",
    "\n",
    "        plt.plot(test_loss, label='Test loss')\n",
    "        plt.plot(test_accuracy, label='Test accuracy')\n",
    "        plt.legend(frameon=False)\n",
    "        plt.show()\n",
    "\n",
    "        return test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAdam(nr_epochs, train_set, valid_set, test_set, mixnb):\n",
    "\n",
    "    print(\"__________________________________________ADAM FC_________________________________\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model_name = \"200epoci-\" + mixnb + \".pth\"\n",
    "    \n",
    "    network = Network()\n",
    "    \n",
    "    print(\"network sent to CUDA\")\n",
    "    network.cuda()\n",
    "    \n",
    "    # set optimizer -> article : Adam, lr = 0.01, b1 = 0.9, b2 = 0.999\n",
    "    optimizer = optim.Adam(network.parameters(), lr = 0.001, betas = (0.9, 0.999))\n",
    "        \n",
    "    start_time = time.time()\n",
    "    train_network(nr_epochs=nr_epochs, \n",
    "                train_set=train_set, valid_set=valid_set, test_set = test_set,\n",
    "                model_name=model_name,\n",
    "                optimizer=optimizer,\n",
    "                network=network)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"Elapsed time for \" + model_name + \" : {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    \n",
    "    return model_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAdamConv(nr_epochs, train_set, valid_set, test_set, mixnb):\n",
    "    \n",
    "    print(\"__________________________________________ADAM CONV + FC_________________________________\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model_name = \"200epoci-\"+ mixnb + \".pth\"\n",
    "    \n",
    "    networkConv = NetworkConv()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"networkConv sent to CUDA\")\n",
    "        networkConv.cuda()\n",
    "        \n",
    "    # set optimizer -> article : Adam, lr = 0.001, b1 = 0.9, b2 = 0.999\n",
    "    optimizer = optim.Adam(networkConv.parameters(), lr = 0.001, betas = (0.9, 0.999))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_network(nr_epochs=nr_epochs, \n",
    "                train_set=train_set, valid_set=valid_set, \n",
    "                model_name=model_name,\n",
    "                optimizer=optimizer,\n",
    "                network=networkConv)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print(\"Elapsed time for \" + model_name + \" : {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    \n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mix_from_voices(voice1_filename, voice2_filename, mix_5_sec):\n",
    "    \n",
    "    voice1, sr = librosa.load(voice1_filename, sr=16000) \n",
    "    voice1 = np.append(voice1, voice1)\n",
    "    \n",
    "    voice2, sr = librosa.load(voice2_filename, sr=16000)\n",
    "    voice2 = np.append(voice2, voice2)\n",
    "    \n",
    "    if mix_5_seconds == True:\n",
    "        voice1 = make_mix_of_5_seconds(voice1)\n",
    "        voice2 = make_mix_of_5_seconds(voice2)\n",
    "    else:\n",
    "        voice1 = make_mix_of_10_seconds(voice1)\n",
    "        voice2 = make_mix_of_10_seconds(voice2)\n",
    "        \n",
    "    mix = voice1 + voice2\n",
    "    \n",
    "    librosa.output.write_wav(\"recordings/mix-withOUT-noise.wav\", mix, 16000)\n",
    "    \n",
    "    return mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mix_from_voices_with_noise(voice1_filename, voice2_filename, noise_filename):\n",
    "    \n",
    "    voice1, sr = librosa.load(voice1_filename, sr=16000) \n",
    "    voice1 = np.append(voice1, voice1)\n",
    "    \n",
    "    voice2, sr = librosa.load(voice2_filename, sr=16000)\n",
    "    voice2 = np.append(voice2, voice2)\n",
    "    \n",
    "    noise, sr = librosa.load(noise_filename, sr=16000)\n",
    "    noise = np.append(noise, noise)\n",
    "    \n",
    "    if mix_5_seconds == True:\n",
    "        voice1 = make_mix_of_5_seconds(voice1)\n",
    "        voice2 = make_mix_of_5_seconds(voice2)\n",
    "    else:\n",
    "        voice1 = make_mix_of_10_seconds(voice1)\n",
    "        voice2 = make_mix_of_10_seconds(voice2)\n",
    "    \n",
    "    mix = voice1 + voice2 + noise\n",
    "    \n",
    "    librosa.output.write_wav(\"recordings/mix-with-noise.wav\", mix, 16000)\n",
    "    return mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________ADAM CONV + FC_________________________________\n",
      "networkConv sent to CUDA\n",
      "--------------------------------------Epoch 1 ------------------------------\n",
      "Train set accuracy: 105/498 (21%)\n",
      "Valid set accuracy: 0/498 (0%)\n",
      "--------------------------------------Epoch 10 ------------------------------\n",
      "Train set accuracy: 144/498 (29%)\n",
      "Valid set accuracy: 44/498 (9%)\n",
      "--------------------------------------Epoch 20 ------------------------------\n",
      "Train set accuracy: 157/498 (32%)\n",
      "Valid set accuracy: 30/498 (6%)\n",
      "--------------------------------------Epoch 30 ------------------------------\n",
      "Train set accuracy: 175/498 (35%)\n",
      "Valid set accuracy: 25/498 (5%)\n",
      "--------------------------------------Epoch 40 ------------------------------\n",
      "Train set accuracy: 235/498 (47%)\n",
      "Valid set accuracy: 11/498 (2%)\n",
      "--------------------------------------Epoch 50 ------------------------------\n",
      "Train set accuracy: 334/498 (67%)\n",
      "Valid set accuracy: 4/498 (1%)\n",
      "--------------------------------------Epoch 60 ------------------------------\n",
      "Train set accuracy: 425/498 (85%)\n",
      "Valid set accuracy: 8/498 (2%)\n",
      "--------------------------------------Epoch 70 ------------------------------\n",
      "Train set accuracy: 449/498 (90%)\n",
      "Valid set accuracy: 3/498 (1%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-dcb033f2b4a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# model_name = runAdamConv(200, train_set_80_1, valid_set_10_1, test_set_10_1, \"v1(7)-v2(1)\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunAdamConv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"v1(7)-v2(1)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mmix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mix_from_voices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvoice1_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoice2_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-cb846eda0920>\u001b[0m in \u001b[0;36mrunAdamConv\u001b[1;34m(nr_epochs, train_set, valid_set, test_set, mixnb)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 network=networkConv)\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mhours\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-ec486b8745f4>\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(nr_epochs, train_set, valid_set, model_name, optimizer, network)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;31m# 5. save the loss for this PF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import winsound\n",
    "import os\n",
    "\n",
    "voice1 = 'recordings/voice1/arctic_a0007.wav'\n",
    "voice2 = 'recordings/voice2/arctic_a0032.wav'\n",
    "    \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model_name = runAdam(200, train_set, valid_set, test_set_with_noise, \"FC-mix-intreg-v1(7)-v2(32)\")\n",
    "\n",
    "# test first what it learned from train\n",
    "mix = get_mix_from_voices(voice1, voice2)\n",
    "network = torch.load(model_name)\n",
    "test_mask = eval(network=network, test_set=test_set)\n",
    "split_audios_epoch(\"FC-mix-intreg-v1(7)-v2(32)-test\", test_mask, mix)\n",
    "\n",
    "# test with noise\n",
    "mix = get_mix_from_voices_with_noise(voice1, voice2, 'recordings/noises/piano5sec.wav')\n",
    "network = torch.load(model_name)\n",
    "test_mask = eval(network=network, test_set=test_set_with_noise)\n",
    "split_audios_epoch(\"FC-mix-intreg-v1(7)-v2(32)-test-noise\", test_mask, mix)\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model_name = runAdamConv(200, train_set, valid_set, test_set_with_noise, \"CONV-mix-intreg-v1(7)-v2(32)\")\n",
    "\n",
    "# # test first what it learned from train\n",
    "# mix = get_mix_from_voices(voice1, voice2)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set)\n",
    "# split_audios_epoch(\"CONV-mix-intreg-v1(7)-v2(32)-test\", test_mask, mix)\n",
    "\n",
    "# # test with noise\n",
    "# mix = get_mix_from_voices_with_noise(voice1, voice2, 'recordings/noises/piano5sec.wav')\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set_with_noise)\n",
    "# split_audios_epoch(\"CONV-mix-intreg-v1(7)-v2(32)-test-noise\", test_mask, mix)\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model_name = runAdam(200, train_set_80, valid_set_10, test_set_10, \"FC-mix-80-10-10-v1(7)-v2(32)\")\n",
    "\n",
    "# # test first what it learned from train\n",
    "# mix = get_mix_from_voices(voice1, voice2)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set)\n",
    "# split_audios_epoch(\"FC-mix-80-10-10-v1(7)-v2(32)-test\", test_mask, mix)\n",
    "\n",
    "# # test with noise\n",
    "# mix = get_mix_from_voices_with_noise(voice1, voice2, 'recordings/noises/piano5sec.wav')\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set_with_noise)\n",
    "# split_audios_epoch(\"FC-mix-80-10-10-v1(7)-v2(32)-test-noise\", test_mask, mix)\n",
    "\n",
    "# #-----------------------------------------------------------------------------------\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# model_name = runAdamConv(200, train_set_80, valid_set_10, test_set_10, \"CONV-mix-80-10-10-v1(7)-v2(32)\")\n",
    "\n",
    "# # test first what it learned from train\n",
    "# mix = get_mix_from_voices(voice1, voice2)\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set)\n",
    "# split_audios_epoch(\"CONV-mix-80-10-10-v1(7)-v2(32)-test\", test_mask, mix)\n",
    "\n",
    "# # test with noise\n",
    "# mix = get_mix_from_voices_with_noise(voice1, voice2, 'recordings/noises/piano5sec.wav')\n",
    "# network = torch.load(model_name)\n",
    "# test_mask = eval(network=network, test_set=test_set_with_noise)\n",
    "# split_audios_epoch(\"CONV-mix-80-10-10-v1(7)-v2(32)-test-noise\", test_mask, mix)\n",
    "\n",
    "# #-----------------------------------------------------------------------------------\n",
    "\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
