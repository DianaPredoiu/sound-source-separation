{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DISERTATIE_LUCRU\\envs\\data-science\\lib\\site-packages\\pydub\\utils.py:165: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import librosa, librosa.display\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import my_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "# momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "n_fft = 1024\n",
    "hop_length = 8\n",
    "sr = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_fft = 1024\n",
    "hop_length = 8\n",
    "sr = 16000\n",
    "\n",
    "voice1, sr_female = librosa.load(\"../recordings/voice1/arctic_a0001_female.wav\", sr=16000) \n",
    "voice2, sr_male = librosa.load(\"../recordings/voice2/arctic_a0002_male.wav\", sr=16000) \n",
    "\n",
    "# pad smaller array with zeros, so both audio files have the same length\n",
    "voice1, voice2 = my_utils.make_wav_files_same_size(voice1, voice2)\n",
    "\n",
    "# tried to reconstruct from spectrogram - np.abs(spectrogram) is used inside 'reconstruct' function\n",
    "# we have to send the spectrogram itself and not np.abs(spectrogram) because :\n",
    "# Z = np.abs(spectrogram) * np.exp(np.angle(spectrogram) * 1j)\n",
    "spectrogram = librosa.stft(librosa.to_mono(voice1), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "voice1_rec = my_utils.reconstruct(spectrogram=spectrogram)\n",
    "librosa.output.write_wav('../recordings/REC.wav', voice1_rec, sr = 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STFT_AF_frames(AF, PF_size):\n",
    "    frames = []\n",
    "    \n",
    "    i = 0\n",
    "    start = (int)(i * (PF_size / 2))\n",
    "    stop = start + PF_size\n",
    "    \n",
    "    while start < len(AF) and stop <= len(AF):\n",
    "        frame = AF[start:stop]\n",
    "        frame = librosa.stft(librosa.to_mono(frame), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        frame = np.abs(frame)\n",
    "        \n",
    "        tensor = []\n",
    "        tensor.append(frame)\n",
    "#         print(\"tensor\", np.asarray(tensor).shape)\n",
    "        \n",
    "        frames.append(tensor)\n",
    "        \n",
    "        i += 1\n",
    "        start = (int)(i * (PF_size / 2))\n",
    "        stop = start + PF_size\n",
    "    \n",
    "    return np.asarray(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_AF_from_mix(mix_frames, current_position, number_of_PF_per_AF, PF_length):\n",
    "    AF = []\n",
    "    if current_position < (number_of_PF_per_AF-1):\n",
    "        # ex: get_AF_from_mix(mix, 2, 4, 320) -> trebuie AF din mix[0: 940]\n",
    "        # start = 0\n",
    "        # stop = (2+1) * 320 = 940 => adica am frame-urile {0, 1, 2}\n",
    "        stop = (current_position + 1) * PF_length\n",
    "        AF = mix[: stop]\n",
    "    else:\n",
    "        # ex: get_AF_from_mix(mix, 5, 4, 320) -> trebuie AF din mix[640: 1920]\n",
    "        # start = (5-4) + 1  * 320 = 640\n",
    "        # stop = (5+1) * 320 = 1920 => adica am frame-urile {2, 3, 4, 5}\n",
    "        start = (current_position - number_of_PF_per_AF + 1) * PF_length\n",
    "        stop = (current_position + 1) * PF_length\n",
    "        AF = mix[start : stop]\n",
    "        \n",
    "    return AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_target_masks_for_voices(voice1, voice2):\n",
    "    voice1_frames = np.array([female[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "    voice2_frames = np.array([male[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "\n",
    "    mask_frames = []\n",
    "    n_fft = 1024\n",
    "    hop_length = 8\n",
    "    sr = 16000\n",
    "    \n",
    "    for i in range(0, voice1_frames.shape[0]):\n",
    "        stft_1 = librosa.stft(librosa.to_mono(voice1_frames[i]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        stft_2 = librosa.stft(librosa.to_mono(voice2_frames[i]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        mask_frames.append(my_utils.compute_mask(stft_1, stft_2))\n",
    "        \n",
    "    return np.array(mask_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_stft_frames_for_audio(mix):\n",
    "    stft_frames = []\n",
    "    n_fft = 1024\n",
    "    hop_length = 8\n",
    "    sr = 16000\n",
    "    \n",
    "    for i in range(0, mix.shape[0]):\n",
    "        stft = librosa.stft(librosa.to_mono(mix[i]), window='hann', n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        stft_frames.append(stft)\n",
    "        \n",
    "    return np.array(stft_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mix total length:  80000\n",
      "mix total length (ms) :  5000.0\n",
      "nb of samples for 20 ms frame = 320 samples/array\n",
      "Mask frames shape:  (250, 513, 41)\n",
      "Mask -one- frame shape:  (513, 41)\n"
     ]
    }
   ],
   "source": [
    "male_filename = '../recordings/male1/arctic_a0002.wav'\n",
    "female_filename = '../recordings/female1/arctic_a0001.wav'\n",
    "\n",
    "male, sr = librosa.load(male_filename, sr=16000) \n",
    "female, sr = librosa.load(female_filename, sr=16000) \n",
    "\n",
    "# pad smaller array with zeros, so both audio files have the same length\n",
    "female, male = my_utils.make_wav_files_same_size(female, male)\n",
    "\n",
    "# load the mixed audio \n",
    "mix = female + male\n",
    "\n",
    "male = np.array(male)\n",
    "female = np.array(female)\n",
    "mix = np.array(mix)\n",
    "\n",
    "frame_length_ms = 20\n",
    "mix_length_ms = len(mix) / sr * 1000\n",
    "samples_per_frame = (int)(len(mix) * frame_length_ms / mix_length_ms)\n",
    "\n",
    "print(\"mix total length: \",len(mix))\n",
    "print(\"mix total length (ms) : \", mix_length_ms)\n",
    "print(\"nb of samples for\", frame_length_ms,\"ms frame =\", samples_per_frame, \"samples/array\")\n",
    "\n",
    "mix_audio_frames = np.array([mix[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "\n",
    "mask_frames = get_input_target_masks_for_voices(voice1, voice2)\n",
    "print(\"Mask frames shape: \", mask_frames.shape)\n",
    "print(\"Mask -one- frame shape: \", mask_frames[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set input shape:  torch.Size([250, 320])\n",
      "Train set input shape:  torch.Size([250, 513, 41])\n",
      "AF:  (1280,)\n",
      "AF_frames shape after concatenating 4 PFs + overlap:  (7, 1, 513, 41)\n",
      "train set PFs:  250\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# train set for only one audio\n",
    "train_set_input = torch.from_numpy(mix_audio_frames)\n",
    "# target contains the calculated masks for each PF from mix\n",
    "train_set_target = torch.from_numpy(mask_frames)\n",
    "\n",
    "# 250 ferestre a cate 320 valori fiecare / 250 PF-uri\n",
    "print(\"Train set input shape: \", train_set_input.shape)\n",
    "# targetul are aceeasi dim la shape[0] ca si inputul -> acelasi nr de frame-uri\n",
    "print(\"Train set input shape: \", train_set_target.shape)\n",
    "\n",
    "# asa ar trebui sa calculez un AF, luand cate 4 PF-uri, ultimul fiind cel curent\n",
    "AF = get_AF_from_mix(mix_frames=mix, current_position= 3, number_of_PF_per_AF= 4, PF_length=samples_per_frame)\n",
    "\n",
    "# AF = train_set_input[0, :]\n",
    "# AF = np.append(AF, train_set_input[1, :])\n",
    "# AF = np.append(AF, train_set_input[2, :])\n",
    "# AF = np.append(AF, train_set_input[3, :])\n",
    "\n",
    "print(\"AF: \", AF.shape)\n",
    "assert(AF.shape == (1280,))\n",
    "\n",
    "\n",
    "# AF_frames va intra ca \"data\" in CNN cu network(data)\n",
    "PF_size = len(train_set_input[0, :])\n",
    "AF_frames = get_STFT_AF_frames(AF, PF_size)\n",
    "\n",
    "# create the shape for the tensor how the pytorch input data needs: entries, nr of channels, height, width\n",
    "print(\"AF_frames shape after concatenating 4 PFs + overlap: \", AF_frames.shape)\n",
    "assert(AF_frames.shape == (7, 1, 513, 41))\n",
    "\n",
    "\n",
    "# train_set_input need to be bounded, so each PF from it needs to be associated with PF from voice1 + voice2\n",
    "# keys will be train_set_input\n",
    "# values will be train_set_target = each position of train_set_target will have a coresp mask\n",
    "train_set = dict(zip(train_set_input, train_set_target))\n",
    "print(\"train set PFs: \", len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv1_batch = nn.BatchNorm2d(20)\n",
    "        \n",
    "        # layer 2\n",
    "        self.conv2 = nn.Conv2d(20, 20, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2_batch = nn.BatchNorm2d(20)\n",
    "        \n",
    "        # layer 3\n",
    "        self.conv3 = nn.Conv2d(20, 1, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3_batch = nn.BatchNorm2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # layer 1\n",
    "        x = self.conv1(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "        x = nn.functional.sigmoid(x)\n",
    "        x = self.conv1_batch(x)\n",
    "#         print(\"1: \", x.shape)\n",
    "        \n",
    "        # layer 2\n",
    "        x = self.conv2(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "        x = nn.functional.sigmoid(x)\n",
    "        x = self.conv2_batch(x)\n",
    "#         print(\"2: \", x.shape)\n",
    "        \n",
    "        # layer 2\n",
    "        x = self.conv3(x)\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "        x = nn.functional.sigmoid(x)\n",
    "        x = self.conv3_batch(x)\n",
    "#         print(\"3: \", x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "network = Network()\n",
    "\n",
    "# if cuda is avaibable, send network's params to gpu\n",
    "if torch.cuda.is_available():\n",
    "    network.cuda()\n",
    "    \n",
    "# set optimizer -> article : Adam, lr = 0.001, b1 = 0.9, b2 = 0.999\n",
    "optimizer = optim.Adam(network.parameters(), lr = 0.001, betas = (0.9, 0.999))\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init arrays for train/test errors\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_set) for i in range(n_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_PF_per_AF = 4\n",
    "samples_per_frame = 320 # pentru 20ms per frame\n",
    "\n",
    "def train(epoch):\n",
    "    mask = []\n",
    "    network.train()\n",
    "\n",
    "    mix_frames = [train_set.keys()]\n",
    "#     print(type(mix_frames))\n",
    "    \n",
    "    # input is current PF => network will learn the mask for the current PF\n",
    "    for index, (input, target) in enumerate(train_set.items()):\n",
    "        \n",
    "        # asa ar trebui sa calculez un AF, luand cate N PF-uri, ultimul fiind cel curent\n",
    "        input_AF = get_AF_from_mix(mix_frames=mix_frames, current_position= index, number_of_PF_per_AF= number_of_PF_per_AF, \n",
    "                                 PF_length=samples_per_frame)\n",
    "        \n",
    "        # np.abs pe fiecare frame -> torch nu accepta numere complexe\n",
    "        input_AF_STFT_frames = get_STFT_AF_frames(AF=input_AF, PF_size=len(input))\n",
    "#         print(\"train_set_input:\", input_AF_STFT_frames.shape)\n",
    "        \n",
    "        data = torch.from_numpy(input_AF_STFT_frames)\n",
    "\n",
    "        # if cuda is available, send (data, target) to gpu\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = network(data)\n",
    "        print(\"train-output shape: \", output.shape, target.shape)\n",
    "        \n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # face update la weights\n",
    "        optimizer.step()\n",
    "            \n",
    "        mask.append(output[0][0].shape)\n",
    "\n",
    "#         print(\"Epoch\", epoch, \"[\", index * len(data), \"/\", len(data))\n",
    "        if index % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, index, len(train_set),\n",
    "            100. * index / len(train_set), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (index) + ((epoch-1)*len(train_set.items())))\n",
    "#             torch.save(network.state_dict(), './results/model.pth')\n",
    "#             torch.save(optimizer.state_dict(), './results/optimizer.pth')\n",
    "    print(mask.shape)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DISERTATIE_LUCRU\\envs\\data-science\\lib\\site-packages\\torch\\nn\\functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-output shape:  torch.Size([1, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [0/250 (0%)]\tLoss: 1.126617\n",
      "train-output shape:  torch.Size([3, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([5, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [10/250 (4%)]\tLoss: 1.869516\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [20/250 (8%)]\tLoss: 1.724864\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [30/250 (12%)]\tLoss: 0.972945\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [40/250 (16%)]\tLoss: 1.060451\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [50/250 (20%)]\tLoss: 0.923478\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [60/250 (24%)]\tLoss: 0.909616\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [70/250 (28%)]\tLoss: 1.186895\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [80/250 (32%)]\tLoss: 0.902318\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [90/250 (36%)]\tLoss: 0.820242\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [100/250 (40%)]\tLoss: 1.328440\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [110/250 (44%)]\tLoss: 1.528290\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [120/250 (48%)]\tLoss: 1.114748\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [130/250 (52%)]\tLoss: 1.259966\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [140/250 (56%)]\tLoss: 1.139670\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [150/250 (60%)]\tLoss: 1.097920\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [160/250 (64%)]\tLoss: 0.966767\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [170/250 (68%)]\tLoss: 0.954089\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [180/250 (72%)]\tLoss: 0.636872\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [190/250 (76%)]\tLoss: 0.666890\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [200/250 (80%)]\tLoss: 0.652275\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [210/250 (84%)]\tLoss: 0.637438\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [220/250 (88%)]\tLoss: 0.622764\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [230/250 (92%)]\tLoss: 0.608276\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "Train Epoch: 0 [240/250 (96%)]\tLoss: 0.594021\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n",
      "train-output shape:  torch.Size([7, 1, 513, 41]) torch.Size([513, 41])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-807088bdab04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# ------------------------------------------- CALL NETWORK ---------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;31m# for epoch in range(1, n_epochs + 1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m#     train(epoch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-67f6bb90d186>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;31m#             torch.save(network.state_dict(), './results/model.pth')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m#             torch.save(optimizer.state_dict(), './results/optimizer.pth')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------- LOAD AUDIO FILE AND CREATE MIX------------------------------\n",
    "male_filename = '../recordings/male1/arctic_a0002.wav'\n",
    "female_filename = '../recordings/female1/arctic_a0001.wav'\n",
    "\n",
    "male, sr = librosa.load(male_filename, sr=16000) \n",
    "female, sr = librosa.load(female_filename, sr=16000) \n",
    "\n",
    "# pad smaller array with zeros, so both audio files have the same length\n",
    "female, male = my_utils.make_wav_files_same_size(female, male)\n",
    "\n",
    "# load the mixed audio \n",
    "mix = female + male\n",
    "\n",
    "male = np.array(male)\n",
    "female = np.array(female)\n",
    "mix = np.array(mix)\n",
    "\n",
    "\n",
    "# -------------------------------- CREATE TRAIN_SET_INPUT & TRAIN_SET_TARGET FOR ONE AUDIO FILE\n",
    "\n",
    "# train set for only one audio\n",
    "mix_audio_frames = np.array([mix[i:i + samples_per_frame] for i in range(0, len(mix), samples_per_frame)])\n",
    "train_set_input = torch.from_numpy(mix_audio_frames)\n",
    "\n",
    "# target contains the calculated masks for each PF from mix\n",
    "mask_frames = get_input_target_masks_for_voices(voice1, voice2)\n",
    "train_set_target = torch.from_numpy(mask_frames)\n",
    "\n",
    "# 250 ferestre a cate 320 valori fiecare / 250 PF-uri\n",
    "# print(\"Train set input shape: \", train_set_input.shape)\n",
    "\n",
    "# targetul are aceeasi dim la shape[0] ca si inputul -> acelasi nr de frame-uri\n",
    "# print(\"Train set target shape: \", train_set_target.shape)\n",
    "\n",
    "# train_set_input need to be bounded, so each PF from it needs to be associated with PF from voice1 + voice2\n",
    "# keys will be train_set_input\n",
    "# values will be train_set_target = each position of train_set_target will have a coresp mask\n",
    "\n",
    "train_set = dict(zip(train_set_input, train_set_target))\n",
    "# print(\"train set PFs: \", len(train_set))\n",
    "\n",
    "# ------------------------------------------- CALL NETWORK ---------------------------------------------\n",
    "mask = train(0)\n",
    "# for epoch in range(1, n_epochs + 1):\n",
    "#     train(epoch)\n",
    "#     test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9f0a4e316d42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mask' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
